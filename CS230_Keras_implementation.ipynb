{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS230_Keras_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VasuPatel001/Electricity_Climate_Change/blob/main/CS230_Keras_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0hBTyMQfYmA"
      },
      "source": [
        "CS 230 Project work: \n",
        "This project analyzes the delta increase in electricity consumption from the extreme weather data in California. \n",
        "\n",
        "We will begin by importing our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDuxYNl4Wvzb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkiR1nbSMLeq",
        "outputId": "91e0fae0-c4d7-4097-85a9-cabef4aeb6b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iogCdyX1EBt"
      },
      "source": [
        "dataset = pd.read_csv('/content/drive/Shareddrives/CS_229_project/Electricity_Consumption_data/dataset_final.csv', usecols = ['AWND','DAPR','DASF','EVAP','MDPR','MDSF','MNPN','MXPN','PGTM','PRCP','PSUN','SN33','SN35','SNOW','SNWD','SX32','SX33','TAVG','TMAX','TMIN','TOBS','TSUN','WDF2','WDF5','WDFG','WDMV','WESD','WESF','WSF2','WSF5','WSFG','WSFI','CLASS_2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xI2hU8t1SvH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "493aea5e-adda-4fa5-8a50-ac7a9c18d3c9"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AWND</th>\n",
              "      <th>DAPR</th>\n",
              "      <th>DASF</th>\n",
              "      <th>EVAP</th>\n",
              "      <th>MDPR</th>\n",
              "      <th>MDSF</th>\n",
              "      <th>MNPN</th>\n",
              "      <th>MXPN</th>\n",
              "      <th>PGTM</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>PSUN</th>\n",
              "      <th>SN33</th>\n",
              "      <th>SN35</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>SX32</th>\n",
              "      <th>SX33</th>\n",
              "      <th>TAVG</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>TOBS</th>\n",
              "      <th>TSUN</th>\n",
              "      <th>WDF2</th>\n",
              "      <th>WDF5</th>\n",
              "      <th>WDFG</th>\n",
              "      <th>WDMV</th>\n",
              "      <th>WESD</th>\n",
              "      <th>WESF</th>\n",
              "      <th>WSF2</th>\n",
              "      <th>WSF5</th>\n",
              "      <th>WSFG</th>\n",
              "      <th>WSFI</th>\n",
              "      <th>CLASS_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.161111</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.433077</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>62.833333</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>1022.785714</td>\n",
              "      <td>0.017936</td>\n",
              "      <td>NaN</td>\n",
              "      <td>87.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>77.408228</td>\n",
              "      <td>92.219313</td>\n",
              "      <td>65.963816</td>\n",
              "      <td>74.140271</td>\n",
              "      <td>NaN</td>\n",
              "      <td>222.051282</td>\n",
              "      <td>212.105263</td>\n",
              "      <td>266.5</td>\n",
              "      <td>63.016667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.770513</td>\n",
              "      <td>23.710526</td>\n",
              "      <td>32.45</td>\n",
              "      <td>14.766667</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.901000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.399231</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.909091</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>1566.384615</td>\n",
              "      <td>0.022638</td>\n",
              "      <td>NaN</td>\n",
              "      <td>88.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>76.123028</td>\n",
              "      <td>90.745098</td>\n",
              "      <td>65.422512</td>\n",
              "      <td>73.821429</td>\n",
              "      <td>NaN</td>\n",
              "      <td>229.610390</td>\n",
              "      <td>228.701299</td>\n",
              "      <td>261.5</td>\n",
              "      <td>107.658333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17.828571</td>\n",
              "      <td>23.131169</td>\n",
              "      <td>21.05</td>\n",
              "      <td>13.966667</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.962222</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.386923</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.181818</td>\n",
              "      <td>96.454545</td>\n",
              "      <td>1355.571429</td>\n",
              "      <td>0.019913</td>\n",
              "      <td>NaN</td>\n",
              "      <td>88.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>74.880126</td>\n",
              "      <td>89.543372</td>\n",
              "      <td>63.893617</td>\n",
              "      <td>73.205607</td>\n",
              "      <td>NaN</td>\n",
              "      <td>246.282051</td>\n",
              "      <td>245.131579</td>\n",
              "      <td>266.5</td>\n",
              "      <td>91.963636</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16.565385</td>\n",
              "      <td>21.165789</td>\n",
              "      <td>21.90</td>\n",
              "      <td>12.733333</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.224198</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.385385</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>69.454545</td>\n",
              "      <td>99.363636</td>\n",
              "      <td>1531.800000</td>\n",
              "      <td>0.023578</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>99.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>73.870662</td>\n",
              "      <td>88.651888</td>\n",
              "      <td>63.059211</td>\n",
              "      <td>71.646512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>241.666667</td>\n",
              "      <td>233.376623</td>\n",
              "      <td>187.5</td>\n",
              "      <td>82.354545</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17.682051</td>\n",
              "      <td>22.812987</td>\n",
              "      <td>19.45</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.927195</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.422308</td>\n",
              "      <td>0.03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>69.272727</td>\n",
              "      <td>97.636364</td>\n",
              "      <td>1620.800000</td>\n",
              "      <td>0.020585</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>101.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>71.583596</td>\n",
              "      <td>86.363339</td>\n",
              "      <td>61.748768</td>\n",
              "      <td>71.204651</td>\n",
              "      <td>0.0</td>\n",
              "      <td>233.544304</td>\n",
              "      <td>231.139240</td>\n",
              "      <td>182.5</td>\n",
              "      <td>121.475000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.098734</td>\n",
              "      <td>23.559494</td>\n",
              "      <td>24.50</td>\n",
              "      <td>12.966667</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2331</th>\n",
              "      <td>3.629211</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1107.909091</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>49.119741</td>\n",
              "      <td>63.233333</td>\n",
              "      <td>41.594595</td>\n",
              "      <td>48.434109</td>\n",
              "      <td>NaN</td>\n",
              "      <td>202.162162</td>\n",
              "      <td>200.972222</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.827273</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.214865</td>\n",
              "      <td>15.180822</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.700000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2332</th>\n",
              "      <td>3.940133</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1667.909091</td>\n",
              "      <td>0.006898</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.200647</td>\n",
              "      <td>61.282105</td>\n",
              "      <td>42.654737</td>\n",
              "      <td>47.637795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>224.931507</td>\n",
              "      <td>219.583333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.842424</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.267123</td>\n",
              "      <td>16.265753</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29.850000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333</th>\n",
              "      <td>4.232400</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1270.727273</td>\n",
              "      <td>0.020514</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.930556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>49.592233</td>\n",
              "      <td>61.242616</td>\n",
              "      <td>43.205074</td>\n",
              "      <td>46.952000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>263.835616</td>\n",
              "      <td>246.197183</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.809091</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.458904</td>\n",
              "      <td>16.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32.650000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2334</th>\n",
              "      <td>4.330000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1399.727273</td>\n",
              "      <td>0.003252</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.029851</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>48.344156</td>\n",
              "      <td>61.827957</td>\n",
              "      <td>41.189247</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>240.958904</td>\n",
              "      <td>227.638889</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.760606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.546575</td>\n",
              "      <td>16.390411</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.200000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2335</th>\n",
              "      <td>6.084533</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1110.636364</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.942857</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.435065</td>\n",
              "      <td>65.510593</td>\n",
              "      <td>41.488372</td>\n",
              "      <td>49.077586</td>\n",
              "      <td>NaN</td>\n",
              "      <td>173.287671</td>\n",
              "      <td>154.647887</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16.128767</td>\n",
              "      <td>22.094444</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.100000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2336 rows Ã— 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          AWND  DAPR  DASF      EVAP  ...       WSF5   WSFG       WSFI  CLASS_2\n",
              "0     6.161111   NaN   NaN  0.433077  ...  23.710526  32.45  14.766667        3\n",
              "1     6.901000   NaN   NaN  0.399231  ...  23.131169  21.05  13.966667        3\n",
              "2     6.962222   NaN   NaN  0.386923  ...  21.165789  21.90  12.733333        2\n",
              "3     7.224198   NaN   NaN  0.385385  ...  22.812987  19.45   8.500000        2\n",
              "4     7.927195   2.0   NaN  0.422308  ...  23.559494  24.50  12.966667        2\n",
              "...        ...   ...   ...       ...  ...        ...    ...        ...      ...\n",
              "2331  3.629211   NaN   NaN       NaN  ...  15.180822    NaN  22.700000        1\n",
              "2332  3.940133   NaN   NaN       NaN  ...  16.265753    NaN  29.850000        1\n",
              "2333  4.232400   NaN   NaN       NaN  ...  16.500000    NaN  32.650000        1\n",
              "2334  4.330000   NaN   NaN       NaN  ...  16.390411    NaN  14.200000        0\n",
              "2335  6.084533   NaN   NaN       NaN  ...  22.094444    NaN  14.100000        0\n",
              "\n",
              "[2336 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M7HiXZNjR-j"
      },
      "source": [
        "#Applying Linear Discriminant Analysis\n",
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# clf = LinearDiscriminantAnalysis(solver='svd', shrinkage=None, tol = 0.0001, covariance_estimator=None)\n",
        "# fit_transform(x_train_norm, y_train)\n",
        "# clf.fit(x_train_norm, y_train)\n",
        "# LinearDiscriminantAnalysis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lqmh1Gfl0dG"
      },
      "source": [
        "We now have imported our dataset. We move into creating our layers and then working through our layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB7AEXV91gGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665b7576-45e7-4366-ff55-f441290bdf78"
      },
      "source": [
        "#split train & test dataset\n",
        "train = dataset.sample(frac=0.8) #random_state=0\n",
        "test = dataset.drop(train.index)\n",
        "print(train.shape, test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1869, 33) (467, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_d8fi54MmUC"
      },
      "source": [
        "One hot encoding for Train & Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvWTdJ8N1hdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef5d56d-fc47-4360-8016-1a79dbddfdf3"
      },
      "source": [
        "#Create one-hot encoding for TRAIN dataset\n",
        "existing_vals = np.unique(train['CLASS_2'].values)\n",
        "mapping = {val: idx for idx, val in enumerate(existing_vals)}\n",
        "y_train = np.array([mapping[y] for y in train['CLASS_2'].values])\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "print(y_train.shape)\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1869, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br4Uxt-sMhUR",
        "outputId": "ec689667-f667-497d-ccc7-f77c733a7180"
      },
      "source": [
        "#Create one-hot encoding for TEST dataset\n",
        "existing_vals = np.unique(test['CLASS_2'].values)\n",
        "mapping = {val: idx for idx, val in enumerate(existing_vals)}\n",
        "y_test = np.array([mapping[x] for x in test['CLASS_2'].values])\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "print(y_test.shape)\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(467, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qvf-GkB3-A1"
      },
      "source": [
        "#sns.pairplot(train[['CLASS_2', 'AWND', 'PRCP', 'SNWD', 'TAVG', 'TMIN', 'TMAX']], diag_kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ujKmDhR0420y",
        "outputId": "a3ce5653-63b6-448f-eb32-62dca662b293"
      },
      "source": [
        "train.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AWND</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>6.336616</td>\n",
              "      <td>1.697368</td>\n",
              "      <td>2.851139</td>\n",
              "      <td>5.100658</td>\n",
              "      <td>6.266267</td>\n",
              "      <td>7.352597</td>\n",
              "      <td>13.221194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DAPR</th>\n",
              "      <td>890.0</td>\n",
              "      <td>3.437594</td>\n",
              "      <td>2.337339</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.666667</td>\n",
              "      <td>31.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DASF</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EVAP</th>\n",
              "      <td>1855.0</td>\n",
              "      <td>0.219079</td>\n",
              "      <td>0.231385</td>\n",
              "      <td>-1.033333</td>\n",
              "      <td>0.088333</td>\n",
              "      <td>0.206923</td>\n",
              "      <td>0.325385</td>\n",
              "      <td>4.439091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MDPR</th>\n",
              "      <td>886.0</td>\n",
              "      <td>0.426484</td>\n",
              "      <td>0.955709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.497500</td>\n",
              "      <td>14.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MDSF</th>\n",
              "      <td>2.0</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>32.512770</td>\n",
              "      <td>12.010000</td>\n",
              "      <td>23.505000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>46.495000</td>\n",
              "      <td>57.990000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MNPN</th>\n",
              "      <td>1855.0</td>\n",
              "      <td>51.972030</td>\n",
              "      <td>9.185035</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>44.190909</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>60.045455</td>\n",
              "      <td>71.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MXPN</th>\n",
              "      <td>1855.0</td>\n",
              "      <td>75.089538</td>\n",
              "      <td>15.335797</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>61.140909</td>\n",
              "      <td>76.200000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>168.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PGTM</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>1342.403746</td>\n",
              "      <td>231.432468</td>\n",
              "      <td>291.545455</td>\n",
              "      <td>1205.545455</td>\n",
              "      <td>1374.900000</td>\n",
              "      <td>1504.200000</td>\n",
              "      <td>2044.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRCP</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>0.061440</td>\n",
              "      <td>0.154533</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.004069</td>\n",
              "      <td>0.036254</td>\n",
              "      <td>1.465031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PSUN</th>\n",
              "      <td>32.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SN33</th>\n",
              "      <td>1789.0</td>\n",
              "      <td>66.877585</td>\n",
              "      <td>14.164269</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>92.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SN35</th>\n",
              "      <td>1789.0</td>\n",
              "      <td>68.523756</td>\n",
              "      <td>13.155205</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SNOW</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>0.082606</td>\n",
              "      <td>0.319053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008850</td>\n",
              "      <td>3.891089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SNWD</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>5.081285</td>\n",
              "      <td>6.920365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.468750</td>\n",
              "      <td>7.709924</td>\n",
              "      <td>30.232877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SX32</th>\n",
              "      <td>1789.0</td>\n",
              "      <td>73.097261</td>\n",
              "      <td>16.650760</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>102.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SX33</th>\n",
              "      <td>1785.0</td>\n",
              "      <td>70.972549</td>\n",
              "      <td>15.365423</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>97.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TAVG</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>56.874163</td>\n",
              "      <td>12.591391</td>\n",
              "      <td>29.545151</td>\n",
              "      <td>46.525316</td>\n",
              "      <td>56.012903</td>\n",
              "      <td>68.396166</td>\n",
              "      <td>83.643312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TMAX</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>71.540392</td>\n",
              "      <td>13.897530</td>\n",
              "      <td>43.012545</td>\n",
              "      <td>59.715736</td>\n",
              "      <td>71.462457</td>\n",
              "      <td>84.591525</td>\n",
              "      <td>101.299639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TMIN</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>47.448855</td>\n",
              "      <td>9.952117</td>\n",
              "      <td>22.400673</td>\n",
              "      <td>39.634064</td>\n",
              "      <td>46.706868</td>\n",
              "      <td>56.400729</td>\n",
              "      <td>69.508137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TOBS</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>56.059809</td>\n",
              "      <td>11.118078</td>\n",
              "      <td>30.957447</td>\n",
              "      <td>46.686636</td>\n",
              "      <td>55.201878</td>\n",
              "      <td>66.500000</td>\n",
              "      <td>80.150602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TSUN</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDF2</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>236.105896</td>\n",
              "      <td>24.797551</td>\n",
              "      <td>127.972973</td>\n",
              "      <td>224.109589</td>\n",
              "      <td>242.531646</td>\n",
              "      <td>251.923077</td>\n",
              "      <td>291.948052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDF5</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>232.320007</td>\n",
              "      <td>25.291092</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>219.729730</td>\n",
              "      <td>237.866667</td>\n",
              "      <td>249.154930</td>\n",
              "      <td>297.922078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDFG</th>\n",
              "      <td>1799.0</td>\n",
              "      <td>229.590884</td>\n",
              "      <td>84.300122</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>220.000000</td>\n",
              "      <td>240.000000</td>\n",
              "      <td>277.500000</td>\n",
              "      <td>360.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDMV</th>\n",
              "      <td>1855.0</td>\n",
              "      <td>45.465037</td>\n",
              "      <td>24.098539</td>\n",
              "      <td>14.290000</td>\n",
              "      <td>34.920000</td>\n",
              "      <td>41.866667</td>\n",
              "      <td>49.937500</td>\n",
              "      <td>738.560000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WESD</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>7.446948</td>\n",
              "      <td>10.809320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.365625</td>\n",
              "      <td>11.490909</td>\n",
              "      <td>44.772727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WESF</th>\n",
              "      <td>312.0</td>\n",
              "      <td>0.270513</td>\n",
              "      <td>0.422379</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>2.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WSF2</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>16.445513</td>\n",
              "      <td>3.057164</td>\n",
              "      <td>9.532468</td>\n",
              "      <td>14.571622</td>\n",
              "      <td>16.294805</td>\n",
              "      <td>17.900000</td>\n",
              "      <td>31.777632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WSF5</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>21.420572</td>\n",
              "      <td>4.126299</td>\n",
              "      <td>11.803896</td>\n",
              "      <td>18.906757</td>\n",
              "      <td>21.251948</td>\n",
              "      <td>23.391892</td>\n",
              "      <td>41.984211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WSFG</th>\n",
              "      <td>1801.0</td>\n",
              "      <td>24.805997</td>\n",
              "      <td>9.604550</td>\n",
              "      <td>2.900000</td>\n",
              "      <td>18.450000</td>\n",
              "      <td>23.900000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>182.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WSFI</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>18.752105</td>\n",
              "      <td>8.063202</td>\n",
              "      <td>4.233333</td>\n",
              "      <td>12.800000</td>\n",
              "      <td>16.800000</td>\n",
              "      <td>23.350000</td>\n",
              "      <td>52.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CLASS_2</th>\n",
              "      <td>1869.0</td>\n",
              "      <td>1.288925</td>\n",
              "      <td>0.931250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          count         mean         std  ...          50%          75%          max\n",
              "AWND     1869.0     6.336616    1.697368  ...     6.266267     7.352597    13.221194\n",
              "DAPR      890.0     3.437594    2.337339  ...     3.000000     3.666667    31.000000\n",
              "DASF        2.0     4.000000    0.000000  ...     4.000000     4.000000     4.000000\n",
              "EVAP     1855.0     0.219079    0.231385  ...     0.206923     0.325385     4.439091\n",
              "MDPR      886.0     0.426484    0.955709  ...     0.040000     0.497500    14.830000\n",
              "MDSF        2.0    35.000000   32.512770  ...    35.000000    46.495000    57.990000\n",
              "MNPN     1855.0    51.972030    9.185035  ...    52.000000    60.045455    71.800000\n",
              "MXPN     1855.0    75.089538   15.335797  ...    76.200000    89.000000   168.500000\n",
              "PGTM     1869.0  1342.403746  231.432468  ...  1374.900000  1504.200000  2044.875000\n",
              "PRCP     1869.0     0.061440    0.154533  ...     0.004069     0.036254     1.465031\n",
              "PSUN       32.0     0.000000    0.000000  ...     0.000000     0.000000     0.000000\n",
              "SN33     1789.0    66.877585   14.164269  ...    67.000000    80.000000    92.000000\n",
              "SN35     1789.0    68.523756   13.155205  ...    69.000000    81.000000    90.000000\n",
              "SNOW     1869.0     0.082606    0.319053  ...     0.000000     0.008850     3.891089\n",
              "SNWD     1869.0     5.081285    6.920365  ...     1.468750     7.709924    30.232877\n",
              "SX32     1789.0    73.097261   16.650760  ...    74.000000    89.000000   102.000000\n",
              "SX33     1785.0    70.972549   15.365423  ...    71.000000    85.000000    97.000000\n",
              "TAVG     1869.0    56.874163   12.591391  ...    56.012903    68.396166    83.643312\n",
              "TMAX     1869.0    71.540392   13.897530  ...    71.462457    84.591525   101.299639\n",
              "TMIN     1869.0    47.448855    9.952117  ...    46.706868    56.400729    69.508137\n",
              "TOBS     1869.0    56.059809   11.118078  ...    55.201878    66.500000    80.150602\n",
              "TSUN       29.0     0.000000    0.000000  ...     0.000000     0.000000     0.000000\n",
              "WDF2     1869.0   236.105896   24.797551  ...   242.531646   251.923077   291.948052\n",
              "WDF5     1869.0   232.320007   25.291092  ...   237.866667   249.154930   297.922078\n",
              "WDFG     1799.0   229.590884   84.300122  ...   240.000000   277.500000   360.000000\n",
              "WDMV     1855.0    45.465037   24.098539  ...    41.866667    49.937500   738.560000\n",
              "WESD     1869.0     7.446948   10.809320  ...     2.365625    11.490909    44.772727\n",
              "WESF      312.0     0.270513    0.422379  ...     0.100000     0.400000     2.700000\n",
              "WSF2     1869.0    16.445513    3.057164  ...    16.294805    17.900000    31.777632\n",
              "WSF5     1869.0    21.420572    4.126299  ...    21.251948    23.391892    41.984211\n",
              "WSFG     1801.0    24.805997    9.604550  ...    23.900000    30.000000   182.400000\n",
              "WSFI     1869.0    18.752105    8.063202  ...    16.800000    23.350000    52.200000\n",
              "CLASS_2  1869.0     1.288925    0.931250  ...     1.000000     2.000000     4.000000\n",
              "\n",
              "[33 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p6tu0rv5STap",
        "outputId": "5ca09cb1-a9ed-4476-e433-fc37f4742e38"
      },
      "source": [
        "test.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AWND</th>\n",
              "      <td>467.0</td>\n",
              "      <td>6.271174</td>\n",
              "      <td>1.774428</td>\n",
              "      <td>2.968272</td>\n",
              "      <td>4.930953</td>\n",
              "      <td>6.220411</td>\n",
              "      <td>7.274054</td>\n",
              "      <td>12.976750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DAPR</th>\n",
              "      <td>212.0</td>\n",
              "      <td>3.492020</td>\n",
              "      <td>2.174179</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.635417</td>\n",
              "      <td>16.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DASF</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EVAP</th>\n",
              "      <td>460.0</td>\n",
              "      <td>0.247895</td>\n",
              "      <td>0.762395</td>\n",
              "      <td>-0.130833</td>\n",
              "      <td>0.099583</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.319375</td>\n",
              "      <td>16.210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MDPR</th>\n",
              "      <td>210.0</td>\n",
              "      <td>0.451189</td>\n",
              "      <td>0.944598</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.455000</td>\n",
              "      <td>7.052500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MDSF</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MNPN</th>\n",
              "      <td>460.0</td>\n",
              "      <td>52.000709</td>\n",
              "      <td>9.301772</td>\n",
              "      <td>29.666667</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>52.261111</td>\n",
              "      <td>59.937500</td>\n",
              "      <td>70.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MXPN</th>\n",
              "      <td>460.0</td>\n",
              "      <td>75.073969</td>\n",
              "      <td>15.946215</td>\n",
              "      <td>41.666667</td>\n",
              "      <td>61.888889</td>\n",
              "      <td>76.738636</td>\n",
              "      <td>87.956818</td>\n",
              "      <td>178.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PGTM</th>\n",
              "      <td>467.0</td>\n",
              "      <td>1341.274763</td>\n",
              "      <td>241.063144</td>\n",
              "      <td>477.200000</td>\n",
              "      <td>1230.212121</td>\n",
              "      <td>1376.769231</td>\n",
              "      <td>1503.025000</td>\n",
              "      <td>1997.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRCP</th>\n",
              "      <td>467.0</td>\n",
              "      <td>0.056689</td>\n",
              "      <td>0.140803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000699</td>\n",
              "      <td>0.004075</td>\n",
              "      <td>0.035125</td>\n",
              "      <td>1.017445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PSUN</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SN33</th>\n",
              "      <td>442.0</td>\n",
              "      <td>66.798643</td>\n",
              "      <td>14.064081</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>68.500000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SN35</th>\n",
              "      <td>442.0</td>\n",
              "      <td>68.540724</td>\n",
              "      <td>13.072081</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>89.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SNOW</th>\n",
              "      <td>467.0</td>\n",
              "      <td>0.110165</td>\n",
              "      <td>0.406616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>4.256098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SNWD</th>\n",
              "      <td>467.0</td>\n",
              "      <td>5.176561</td>\n",
              "      <td>7.303276</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.341667</td>\n",
              "      <td>7.672410</td>\n",
              "      <td>31.092715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SX32</th>\n",
              "      <td>441.0</td>\n",
              "      <td>73.011338</td>\n",
              "      <td>16.467250</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>103.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SX33</th>\n",
              "      <td>441.0</td>\n",
              "      <td>70.959184</td>\n",
              "      <td>15.211185</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>97.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TAVG</th>\n",
              "      <td>467.0</td>\n",
              "      <td>56.623483</td>\n",
              "      <td>12.334462</td>\n",
              "      <td>31.625806</td>\n",
              "      <td>46.411263</td>\n",
              "      <td>56.109677</td>\n",
              "      <td>67.730512</td>\n",
              "      <td>80.996795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TMAX</th>\n",
              "      <td>467.0</td>\n",
              "      <td>71.306834</td>\n",
              "      <td>13.605263</td>\n",
              "      <td>44.314136</td>\n",
              "      <td>60.174724</td>\n",
              "      <td>71.309609</td>\n",
              "      <td>83.733665</td>\n",
              "      <td>97.129730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TMIN</th>\n",
              "      <td>467.0</td>\n",
              "      <td>47.357695</td>\n",
              "      <td>9.875831</td>\n",
              "      <td>26.165812</td>\n",
              "      <td>39.468353</td>\n",
              "      <td>47.127731</td>\n",
              "      <td>56.014269</td>\n",
              "      <td>69.547920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TOBS</th>\n",
              "      <td>467.0</td>\n",
              "      <td>55.890117</td>\n",
              "      <td>10.936743</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>46.534211</td>\n",
              "      <td>55.899038</td>\n",
              "      <td>65.952494</td>\n",
              "      <td>77.834320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TSUN</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDF2</th>\n",
              "      <td>467.0</td>\n",
              "      <td>236.184469</td>\n",
              "      <td>24.789143</td>\n",
              "      <td>135.194805</td>\n",
              "      <td>226.169294</td>\n",
              "      <td>241.621622</td>\n",
              "      <td>253.108108</td>\n",
              "      <td>288.289474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDF5</th>\n",
              "      <td>467.0</td>\n",
              "      <td>232.318515</td>\n",
              "      <td>24.941300</td>\n",
              "      <td>137.368421</td>\n",
              "      <td>222.452615</td>\n",
              "      <td>238.441558</td>\n",
              "      <td>248.379602</td>\n",
              "      <td>280.128205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDFG</th>\n",
              "      <td>444.0</td>\n",
              "      <td>234.279279</td>\n",
              "      <td>81.469286</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>221.500000</td>\n",
              "      <td>240.000000</td>\n",
              "      <td>277.500000</td>\n",
              "      <td>360.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDMV</th>\n",
              "      <td>460.0</td>\n",
              "      <td>49.705432</td>\n",
              "      <td>48.400069</td>\n",
              "      <td>16.580000</td>\n",
              "      <td>35.726667</td>\n",
              "      <td>42.260000</td>\n",
              "      <td>51.745000</td>\n",
              "      <td>874.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WESD</th>\n",
              "      <td>467.0</td>\n",
              "      <td>7.271634</td>\n",
              "      <td>10.923082</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.545455</td>\n",
              "      <td>10.989394</td>\n",
              "      <td>44.721212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WESF</th>\n",
              "      <td>80.0</td>\n",
              "      <td>0.358750</td>\n",
              "      <td>0.518565</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>3.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WSF2</th>\n",
              "      <td>467.0</td>\n",
              "      <td>16.423817</td>\n",
              "      <td>3.205340</td>\n",
              "      <td>10.027848</td>\n",
              "      <td>14.346560</td>\n",
              "      <td>16.150685</td>\n",
              "      <td>17.934848</td>\n",
              "      <td>29.474667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WSF5</th>\n",
              "      <td>467.0</td>\n",
              "      <td>21.375626</td>\n",
              "      <td>4.354865</td>\n",
              "      <td>12.546835</td>\n",
              "      <td>18.620679</td>\n",
              "      <td>21.077922</td>\n",
              "      <td>23.446663</td>\n",
              "      <td>39.818667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WSFG</th>\n",
              "      <td>444.0</td>\n",
              "      <td>25.233221</td>\n",
              "      <td>11.655278</td>\n",
              "      <td>4.900000</td>\n",
              "      <td>17.975000</td>\n",
              "      <td>24.225000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>178.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WSFI</th>\n",
              "      <td>467.0</td>\n",
              "      <td>18.544468</td>\n",
              "      <td>8.137728</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>12.541667</td>\n",
              "      <td>16.500000</td>\n",
              "      <td>22.525000</td>\n",
              "      <td>49.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CLASS_2</th>\n",
              "      <td>467.0</td>\n",
              "      <td>1.297645</td>\n",
              "      <td>0.928365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         count         mean         std  ...          50%          75%          max\n",
              "AWND     467.0     6.271174    1.774428  ...     6.220411     7.274054    12.976750\n",
              "DAPR     212.0     3.492020    2.174179  ...     3.000000     3.635417    16.400000\n",
              "DASF       0.0          NaN         NaN  ...          NaN          NaN          NaN\n",
              "EVAP     460.0     0.247895    0.762395  ...     0.200000     0.319375    16.210000\n",
              "MDPR     210.0     0.451189    0.944598  ...     0.033333     0.455000     7.052500\n",
              "MDSF       0.0          NaN         NaN  ...          NaN          NaN          NaN\n",
              "MNPN     460.0    52.000709    9.301772  ...    52.261111    59.937500    70.800000\n",
              "MXPN     460.0    75.073969   15.946215  ...    76.738636    87.956818   178.000000\n",
              "PGTM     467.0  1341.274763  241.063144  ...  1376.769231  1503.025000  1997.400000\n",
              "PRCP     467.0     0.056689    0.140803  ...     0.004075     0.035125     1.017445\n",
              "PSUN       3.0     0.000000    0.000000  ...     0.000000     0.000000     0.000000\n",
              "SN33     442.0    66.798643   14.064081  ...    68.500000    80.000000    90.000000\n",
              "SN35     442.0    68.540724   13.072081  ...    69.000000    81.000000    89.000000\n",
              "SNOW     467.0     0.110165    0.406616  ...     0.000000     0.003135     4.256098\n",
              "SNWD     467.0     5.176561    7.303276  ...     1.341667     7.672410    31.092715\n",
              "SX32     441.0    73.011338   16.467250  ...    74.000000    88.000000   103.000000\n",
              "SX33     441.0    70.959184   15.211185  ...    72.000000    85.000000    97.000000\n",
              "TAVG     467.0    56.623483   12.334462  ...    56.109677    67.730512    80.996795\n",
              "TMAX     467.0    71.306834   13.605263  ...    71.309609    83.733665    97.129730\n",
              "TMIN     467.0    47.357695    9.875831  ...    47.127731    56.014269    69.547920\n",
              "TOBS     467.0    55.890117   10.936743  ...    55.899038    65.952494    77.834320\n",
              "TSUN       3.0     0.000000    0.000000  ...     0.000000     0.000000     0.000000\n",
              "WDF2     467.0   236.184469   24.789143  ...   241.621622   253.108108   288.289474\n",
              "WDF5     467.0   232.318515   24.941300  ...   238.441558   248.379602   280.128205\n",
              "WDFG     444.0   234.279279   81.469286  ...   240.000000   277.500000   360.000000\n",
              "WDMV     460.0    49.705432   48.400069  ...    42.260000    51.745000   874.520000\n",
              "WESD     467.0     7.271634   10.923082  ...     1.545455    10.989394    44.721212\n",
              "WESF      80.0     0.358750    0.518565  ...     0.100000     0.600000     3.100000\n",
              "WSF2     467.0    16.423817    3.205340  ...    16.150685    17.934848    29.474667\n",
              "WSF5     467.0    21.375626    4.354865  ...    21.077922    23.446663    39.818667\n",
              "WSFG     444.0    25.233221   11.655278  ...    24.225000    30.000000   178.100000\n",
              "WSFI     467.0    18.544468    8.137728  ...    16.500000    22.525000    49.650000\n",
              "CLASS_2  467.0     1.297645    0.928365  ...     1.000000     2.000000     4.000000\n",
              "\n",
              "[33 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaDtLACtRq5L"
      },
      "source": [
        "Train and test data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBRZsslm6YC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be93c7df-90eb-4702-994e-4e737134c9d0"
      },
      "source": [
        "#train data normalization\n",
        "train_features = train.copy()\n",
        "train_labels = train_features.pop('CLASS_2')\n",
        "train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "261     1\n",
              "1348    4\n",
              "1500    2\n",
              "1851    2\n",
              "1426    0\n",
              "       ..\n",
              "1258    1\n",
              "2087    1\n",
              "377     2\n",
              "201     1\n",
              "1548    2\n",
              "Name: CLASS_2, Length: 1869, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYiNAmceOHpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ccd5d5-a279-4567-f8a7-391c73e05536"
      },
      "source": [
        "#test data normalization\n",
        "test_features = test.copy()\n",
        "test_labels = test_features.pop('CLASS_2')\n",
        "test_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2       2\n",
              "14      2\n",
              "15      2\n",
              "17      2\n",
              "39      1\n",
              "       ..\n",
              "2318    1\n",
              "2321    0\n",
              "2326    1\n",
              "2328    0\n",
              "2332    1\n",
              "Name: CLASS_2, Length: 467, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7Fp7QJocycz",
        "outputId": "e973f4fe-15be-4dd1-95b8-b1244f2a0629"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnrfNh186YIm"
      },
      "source": [
        "# train_features.shape\n",
        "x_train_clean = train_features.fillna(0).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "QKt5fMp-Rpyo",
        "outputId": "7de9db7f-9480-4fab-d62d-8a5328dfb536"
      },
      "source": [
        "#test feature cleaning \n",
        "x_test_clean = test_features.fillna(0).dropna()\n",
        "x_test_clean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AWND</th>\n",
              "      <th>DAPR</th>\n",
              "      <th>DASF</th>\n",
              "      <th>EVAP</th>\n",
              "      <th>MDPR</th>\n",
              "      <th>MDSF</th>\n",
              "      <th>MNPN</th>\n",
              "      <th>MXPN</th>\n",
              "      <th>PGTM</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>PSUN</th>\n",
              "      <th>SN33</th>\n",
              "      <th>SN35</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>SX32</th>\n",
              "      <th>SX33</th>\n",
              "      <th>TAVG</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>TOBS</th>\n",
              "      <th>TSUN</th>\n",
              "      <th>WDF2</th>\n",
              "      <th>WDF5</th>\n",
              "      <th>WDFG</th>\n",
              "      <th>WDMV</th>\n",
              "      <th>WESD</th>\n",
              "      <th>WESF</th>\n",
              "      <th>WSF2</th>\n",
              "      <th>WSF5</th>\n",
              "      <th>WSFG</th>\n",
              "      <th>WSFI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.962222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.386923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.181818</td>\n",
              "      <td>96.454545</td>\n",
              "      <td>1355.571429</td>\n",
              "      <td>0.019913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>74.880126</td>\n",
              "      <td>89.543372</td>\n",
              "      <td>63.893617</td>\n",
              "      <td>73.205607</td>\n",
              "      <td>0.0</td>\n",
              "      <td>246.282051</td>\n",
              "      <td>245.131579</td>\n",
              "      <td>266.5</td>\n",
              "      <td>91.963636</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.565385</td>\n",
              "      <td>21.165789</td>\n",
              "      <td>21.90</td>\n",
              "      <td>12.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>6.912651</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.370769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>88.500000</td>\n",
              "      <td>1587.625000</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.116719</td>\n",
              "      <td>85.723127</td>\n",
              "      <td>57.398026</td>\n",
              "      <td>68.640909</td>\n",
              "      <td>0.0</td>\n",
              "      <td>255.500000</td>\n",
              "      <td>244.875000</td>\n",
              "      <td>169.0</td>\n",
              "      <td>54.783333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.568750</td>\n",
              "      <td>21.641250</td>\n",
              "      <td>28.50</td>\n",
              "      <td>11.566667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6.778916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.344167</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.181818</td>\n",
              "      <td>87.909091</td>\n",
              "      <td>1529.375000</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>72.018927</td>\n",
              "      <td>87.387622</td>\n",
              "      <td>59.026187</td>\n",
              "      <td>69.796380</td>\n",
              "      <td>0.0</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>240.500000</td>\n",
              "      <td>182.5</td>\n",
              "      <td>102.016667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.146250</td>\n",
              "      <td>22.132500</td>\n",
              "      <td>27.95</td>\n",
              "      <td>13.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6.293373</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.392308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.454545</td>\n",
              "      <td>96.545455</td>\n",
              "      <td>1161.250000</td>\n",
              "      <td>0.097582</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>99.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>69.396825</td>\n",
              "      <td>84.896839</td>\n",
              "      <td>59.483278</td>\n",
              "      <td>68.086957</td>\n",
              "      <td>0.0</td>\n",
              "      <td>203.250000</td>\n",
              "      <td>195.128205</td>\n",
              "      <td>269.0</td>\n",
              "      <td>54.381818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.185000</td>\n",
              "      <td>24.582051</td>\n",
              "      <td>25.95</td>\n",
              "      <td>14.233333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>6.106625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.727273</td>\n",
              "      <td>92.090909</td>\n",
              "      <td>1561.307692</td>\n",
              "      <td>0.001851</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>94.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>69.082278</td>\n",
              "      <td>85.179104</td>\n",
              "      <td>56.599665</td>\n",
              "      <td>68.061905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>236.103896</td>\n",
              "      <td>228.961039</td>\n",
              "      <td>289.0</td>\n",
              "      <td>66.318182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.463636</td>\n",
              "      <td>21.490909</td>\n",
              "      <td>18.00</td>\n",
              "      <td>14.233333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2318</th>\n",
              "      <td>5.919474</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1279.000000</td>\n",
              "      <td>0.063095</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.969231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.819936</td>\n",
              "      <td>67.127119</td>\n",
              "      <td>47.072187</td>\n",
              "      <td>51.966667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>245.945946</td>\n",
              "      <td>243.287671</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.745455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.451351</td>\n",
              "      <td>20.002703</td>\n",
              "      <td>0.00</td>\n",
              "      <td>27.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2321</th>\n",
              "      <td>5.280000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1342.416667</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>0.953846</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.321543</td>\n",
              "      <td>60.312236</td>\n",
              "      <td>40.529536</td>\n",
              "      <td>45.818966</td>\n",
              "      <td>0.0</td>\n",
              "      <td>241.621622</td>\n",
              "      <td>242.191781</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.672727</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.964865</td>\n",
              "      <td>17.875676</td>\n",
              "      <td>0.00</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2326</th>\n",
              "      <td>4.212078</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1323.636364</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.461538</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58.267742</td>\n",
              "      <td>72.414163</td>\n",
              "      <td>49.578495</td>\n",
              "      <td>52.336207</td>\n",
              "      <td>0.0</td>\n",
              "      <td>226.533333</td>\n",
              "      <td>203.150685</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.084848</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.512000</td>\n",
              "      <td>16.490667</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2328</th>\n",
              "      <td>3.355584</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1356.666667</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.117647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58.741935</td>\n",
              "      <td>73.417012</td>\n",
              "      <td>49.336100</td>\n",
              "      <td>54.125984</td>\n",
              "      <td>0.0</td>\n",
              "      <td>234.133333</td>\n",
              "      <td>220.410959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.957576</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.117333</td>\n",
              "      <td>12.894595</td>\n",
              "      <td>0.00</td>\n",
              "      <td>12.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2332</th>\n",
              "      <td>3.940133</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1667.909091</td>\n",
              "      <td>0.006898</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.200647</td>\n",
              "      <td>61.282105</td>\n",
              "      <td>42.654737</td>\n",
              "      <td>47.637795</td>\n",
              "      <td>0.0</td>\n",
              "      <td>224.931507</td>\n",
              "      <td>219.583333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.842424</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.267123</td>\n",
              "      <td>16.265753</td>\n",
              "      <td>0.00</td>\n",
              "      <td>29.850000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>467 rows Ã— 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          AWND  DAPR  DASF      EVAP  ...       WSF2       WSF5   WSFG       WSFI\n",
              "2     6.962222   0.0   0.0  0.386923  ...  16.565385  21.165789  21.90  12.733333\n",
              "14    6.912651   3.0   0.0  0.370769  ...  16.568750  21.641250  28.50  11.566667\n",
              "15    6.778916   0.0   0.0  0.344167  ...  17.146250  22.132500  27.95  13.500000\n",
              "17    6.293373   0.0   0.0  0.392308  ...  19.185000  24.582051  25.95  14.233333\n",
              "39    6.106625   0.0   0.0  0.317692  ...  16.463636  21.490909  18.00  14.233333\n",
              "...        ...   ...   ...       ...  ...        ...        ...    ...        ...\n",
              "2318  5.919474   0.0   0.0  0.000000  ...  15.451351  20.002703   0.00  27.050000\n",
              "2321  5.280000   0.0   0.0  0.000000  ...  13.964865  17.875676   0.00  21.000000\n",
              "2326  4.212078   0.0   0.0  0.000000  ...  12.512000  16.490667   0.00  10.400000\n",
              "2328  3.355584   0.0   0.0  0.000000  ...  10.117333  12.894595   0.00  12.200000\n",
              "2332  3.940133   0.0   0.0  0.000000  ...  12.267123  16.265753   0.00  29.850000\n",
              "\n",
              "[467 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6zgMZnSH81"
      },
      "source": [
        "Normalizing our train & test clean data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8XIfk1H6YPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993d418a-4f3d-43a6-c022-8ef2c8915e02"
      },
      "source": [
        "#normalize x_train_clean \n",
        "normalizer = tf.keras.layers.Normalization()\n",
        "normalizer.adapt(x_train_clean)\n",
        "x_train_norm = normalizer(x_train_clean)\n",
        "x_train_norm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1869, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM-JR3vBSlkn",
        "outputId": "ae61ebda-9a72-4771-e6f8-dfaacc587ab2"
      },
      "source": [
        "#normalize x_test_clean\n",
        "normalizer = tf.keras.layers.Normalization()\n",
        "normalizer.adapt(x_test_clean)\n",
        "x_test_norm = normalizer(x_test_clean)\n",
        "x_test_norm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([467, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpj3CHXPPnr5"
      },
      "source": [
        "Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzPD35aNX20J"
      },
      "source": [
        "#Import CLASS_2es\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "\n",
        "#Model Definition\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(32,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(5, activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbpGwJ8yZmkj"
      },
      "source": [
        "#Configure model training\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy']) # "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKJT5w7DPsc1"
      },
      "source": [
        "Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83Z7a2wiZ1qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20b9ac7-f281-4c1c-e271-af815f05a7a8"
      },
      "source": [
        "#train model\n",
        "from keras import backend as K\n",
        "K.set_value(model.optimizer.learning_rate, 0.0005)\n",
        "history = model.fit(x_train_norm, y_train, validation_split=0.1, epochs=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "53/53 [==============================] - 1s 6ms/step - loss: 1.4405 - accuracy: 0.4512 - val_loss: 1.2154 - val_accuracy: 0.5401\n",
            "Epoch 2/500\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 1.2404 - accuracy: 0.4976 - val_loss: 1.0816 - val_accuracy: 0.5401\n",
            "Epoch 3/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 1.1292 - accuracy: 0.5220 - val_loss: 0.9808 - val_accuracy: 0.6203\n",
            "Epoch 4/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 1.0399 - accuracy: 0.5779 - val_loss: 0.9030 - val_accuracy: 0.6364\n",
            "Epoch 5/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.9498 - accuracy: 0.5981 - val_loss: 0.8668 - val_accuracy: 0.6364\n",
            "Epoch 6/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.9369 - accuracy: 0.5844 - val_loss: 0.8550 - val_accuracy: 0.6257\n",
            "Epoch 7/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.9282 - accuracy: 0.6171 - val_loss: 0.8471 - val_accuracy: 0.6471\n",
            "Epoch 8/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8901 - accuracy: 0.6064 - val_loss: 0.8202 - val_accuracy: 0.6578\n",
            "Epoch 9/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8656 - accuracy: 0.6195 - val_loss: 0.8119 - val_accuracy: 0.6684\n",
            "Epoch 10/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8468 - accuracy: 0.6266 - val_loss: 0.7928 - val_accuracy: 0.7112\n",
            "Epoch 11/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8456 - accuracy: 0.6243 - val_loss: 0.7856 - val_accuracy: 0.7166\n",
            "Epoch 12/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8239 - accuracy: 0.6296 - val_loss: 0.7722 - val_accuracy: 0.6845\n",
            "Epoch 13/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8298 - accuracy: 0.6231 - val_loss: 0.7714 - val_accuracy: 0.7059\n",
            "Epoch 14/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8317 - accuracy: 0.6391 - val_loss: 0.7593 - val_accuracy: 0.6952\n",
            "Epoch 15/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8145 - accuracy: 0.6427 - val_loss: 0.7654 - val_accuracy: 0.7112\n",
            "Epoch 16/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7989 - accuracy: 0.6373 - val_loss: 0.7498 - val_accuracy: 0.6898\n",
            "Epoch 17/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7947 - accuracy: 0.6379 - val_loss: 0.7377 - val_accuracy: 0.7166\n",
            "Epoch 18/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7838 - accuracy: 0.6510 - val_loss: 0.7561 - val_accuracy: 0.6952\n",
            "Epoch 19/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.8014 - accuracy: 0.6433 - val_loss: 0.7530 - val_accuracy: 0.7112\n",
            "Epoch 20/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7604 - accuracy: 0.6451 - val_loss: 0.7497 - val_accuracy: 0.6845\n",
            "Epoch 21/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7478 - accuracy: 0.6403 - val_loss: 0.7263 - val_accuracy: 0.7166\n",
            "Epoch 22/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7442 - accuracy: 0.6647 - val_loss: 0.7345 - val_accuracy: 0.6845\n",
            "Epoch 23/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7371 - accuracy: 0.6718 - val_loss: 0.7428 - val_accuracy: 0.6738\n",
            "Epoch 24/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7631 - accuracy: 0.6546 - val_loss: 0.7290 - val_accuracy: 0.7005\n",
            "Epoch 25/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7310 - accuracy: 0.6754 - val_loss: 0.7391 - val_accuracy: 0.6738\n",
            "Epoch 26/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7415 - accuracy: 0.6522 - val_loss: 0.7515 - val_accuracy: 0.6898\n",
            "Epoch 27/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7423 - accuracy: 0.6611 - val_loss: 0.7391 - val_accuracy: 0.7059\n",
            "Epoch 28/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7388 - accuracy: 0.6587 - val_loss: 0.7237 - val_accuracy: 0.7166\n",
            "Epoch 29/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7319 - accuracy: 0.6605 - val_loss: 0.7282 - val_accuracy: 0.7219\n",
            "Epoch 30/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7510 - accuracy: 0.6677 - val_loss: 0.7376 - val_accuracy: 0.6791\n",
            "Epoch 31/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7241 - accuracy: 0.6671 - val_loss: 0.7319 - val_accuracy: 0.6898\n",
            "Epoch 32/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7116 - accuracy: 0.6748 - val_loss: 0.7262 - val_accuracy: 0.7112\n",
            "Epoch 33/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7076 - accuracy: 0.6748 - val_loss: 0.7349 - val_accuracy: 0.7005\n",
            "Epoch 34/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7068 - accuracy: 0.6700 - val_loss: 0.7171 - val_accuracy: 0.7219\n",
            "Epoch 35/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.7044 - accuracy: 0.6766 - val_loss: 0.7503 - val_accuracy: 0.6898\n",
            "Epoch 36/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.6825 - val_loss: 0.7338 - val_accuracy: 0.6952\n",
            "Epoch 37/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.6861 - val_loss: 0.7121 - val_accuracy: 0.7166\n",
            "Epoch 38/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6949 - accuracy: 0.6891 - val_loss: 0.7427 - val_accuracy: 0.6684\n",
            "Epoch 39/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.6879 - val_loss: 0.7376 - val_accuracy: 0.6738\n",
            "Epoch 40/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6724 - accuracy: 0.6867 - val_loss: 0.7139 - val_accuracy: 0.7059\n",
            "Epoch 41/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.6694 - val_loss: 0.7263 - val_accuracy: 0.6898\n",
            "Epoch 42/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6787 - accuracy: 0.6825 - val_loss: 0.7225 - val_accuracy: 0.7059\n",
            "Epoch 43/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6849 - accuracy: 0.6807 - val_loss: 0.7222 - val_accuracy: 0.7112\n",
            "Epoch 44/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.6885 - val_loss: 0.7242 - val_accuracy: 0.6845\n",
            "Epoch 45/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6816 - accuracy: 0.6849 - val_loss: 0.7029 - val_accuracy: 0.7166\n",
            "Epoch 46/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6791 - accuracy: 0.6849 - val_loss: 0.7176 - val_accuracy: 0.6845\n",
            "Epoch 47/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6817 - accuracy: 0.6748 - val_loss: 0.6972 - val_accuracy: 0.7219\n",
            "Epoch 48/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.6932 - val_loss: 0.7100 - val_accuracy: 0.7112\n",
            "Epoch 49/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.6867 - val_loss: 0.7292 - val_accuracy: 0.6845\n",
            "Epoch 50/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6448 - accuracy: 0.7045 - val_loss: 0.7431 - val_accuracy: 0.6791\n",
            "Epoch 51/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6592 - accuracy: 0.7004 - val_loss: 0.7576 - val_accuracy: 0.6791\n",
            "Epoch 52/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6552 - accuracy: 0.6950 - val_loss: 0.7321 - val_accuracy: 0.6684\n",
            "Epoch 53/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6406 - accuracy: 0.7099 - val_loss: 0.7308 - val_accuracy: 0.6631\n",
            "Epoch 54/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6477 - accuracy: 0.6956 - val_loss: 0.7315 - val_accuracy: 0.6684\n",
            "Epoch 55/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6569 - accuracy: 0.6897 - val_loss: 0.7387 - val_accuracy: 0.6791\n",
            "Epoch 56/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6494 - accuracy: 0.7010 - val_loss: 0.7525 - val_accuracy: 0.6684\n",
            "Epoch 57/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6654 - accuracy: 0.6879 - val_loss: 0.7380 - val_accuracy: 0.6738\n",
            "Epoch 58/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6452 - accuracy: 0.7117 - val_loss: 0.7543 - val_accuracy: 0.6791\n",
            "Epoch 59/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6473 - accuracy: 0.6974 - val_loss: 0.7509 - val_accuracy: 0.6684\n",
            "Epoch 60/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6302 - accuracy: 0.7063 - val_loss: 0.7466 - val_accuracy: 0.6845\n",
            "Epoch 61/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6458 - accuracy: 0.7134 - val_loss: 0.7506 - val_accuracy: 0.6738\n",
            "Epoch 62/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6282 - accuracy: 0.7105 - val_loss: 0.7601 - val_accuracy: 0.6791\n",
            "Epoch 63/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6414 - accuracy: 0.7087 - val_loss: 0.7439 - val_accuracy: 0.6578\n",
            "Epoch 64/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6311 - accuracy: 0.7057 - val_loss: 0.7308 - val_accuracy: 0.6845\n",
            "Epoch 65/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6267 - accuracy: 0.7229 - val_loss: 0.7519 - val_accuracy: 0.6791\n",
            "Epoch 66/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6490 - accuracy: 0.7105 - val_loss: 0.7425 - val_accuracy: 0.6738\n",
            "Epoch 67/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6310 - accuracy: 0.7140 - val_loss: 0.7435 - val_accuracy: 0.6898\n",
            "Epoch 68/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6180 - accuracy: 0.7134 - val_loss: 0.7764 - val_accuracy: 0.6791\n",
            "Epoch 69/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6227 - accuracy: 0.7170 - val_loss: 0.7535 - val_accuracy: 0.6684\n",
            "Epoch 70/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6368 - accuracy: 0.7111 - val_loss: 0.7503 - val_accuracy: 0.6684\n",
            "Epoch 71/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6136 - accuracy: 0.7122 - val_loss: 0.7303 - val_accuracy: 0.6898\n",
            "Epoch 72/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6229 - accuracy: 0.7146 - val_loss: 0.7376 - val_accuracy: 0.7005\n",
            "Epoch 73/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6302 - accuracy: 0.7158 - val_loss: 0.7344 - val_accuracy: 0.6791\n",
            "Epoch 74/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6083 - accuracy: 0.7152 - val_loss: 0.7218 - val_accuracy: 0.6952\n",
            "Epoch 75/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6179 - accuracy: 0.7218 - val_loss: 0.7213 - val_accuracy: 0.6845\n",
            "Epoch 76/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6202 - accuracy: 0.7277 - val_loss: 0.7309 - val_accuracy: 0.6845\n",
            "Epoch 77/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5970 - accuracy: 0.7372 - val_loss: 0.7411 - val_accuracy: 0.7005\n",
            "Epoch 78/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6053 - accuracy: 0.7283 - val_loss: 0.7605 - val_accuracy: 0.6952\n",
            "Epoch 79/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6074 - accuracy: 0.7128 - val_loss: 0.7725 - val_accuracy: 0.6898\n",
            "Epoch 80/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.7247 - val_loss: 0.7332 - val_accuracy: 0.7005\n",
            "Epoch 81/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5972 - accuracy: 0.7200 - val_loss: 0.7568 - val_accuracy: 0.6898\n",
            "Epoch 82/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6137 - accuracy: 0.7206 - val_loss: 0.7398 - val_accuracy: 0.6952\n",
            "Epoch 83/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.7313 - val_loss: 0.7451 - val_accuracy: 0.6898\n",
            "Epoch 84/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6029 - accuracy: 0.7348 - val_loss: 0.7611 - val_accuracy: 0.6845\n",
            "Epoch 85/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6084 - accuracy: 0.7247 - val_loss: 0.7504 - val_accuracy: 0.7005\n",
            "Epoch 86/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6074 - accuracy: 0.7265 - val_loss: 0.7429 - val_accuracy: 0.7112\n",
            "Epoch 87/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5942 - accuracy: 0.7289 - val_loss: 0.7338 - val_accuracy: 0.7059\n",
            "Epoch 88/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5787 - accuracy: 0.7271 - val_loss: 0.7434 - val_accuracy: 0.7059\n",
            "Epoch 89/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.7402 - val_loss: 0.7475 - val_accuracy: 0.7005\n",
            "Epoch 90/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6034 - accuracy: 0.7342 - val_loss: 0.7726 - val_accuracy: 0.6845\n",
            "Epoch 91/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6071 - accuracy: 0.7247 - val_loss: 0.7634 - val_accuracy: 0.6898\n",
            "Epoch 92/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.6111 - accuracy: 0.7319 - val_loss: 0.7294 - val_accuracy: 0.6845\n",
            "Epoch 93/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5976 - accuracy: 0.7325 - val_loss: 0.7331 - val_accuracy: 0.6898\n",
            "Epoch 94/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5908 - accuracy: 0.7360 - val_loss: 0.7599 - val_accuracy: 0.6952\n",
            "Epoch 95/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.7247 - val_loss: 0.7383 - val_accuracy: 0.6898\n",
            "Epoch 96/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5923 - accuracy: 0.7224 - val_loss: 0.7214 - val_accuracy: 0.6898\n",
            "Epoch 97/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5785 - accuracy: 0.7378 - val_loss: 0.7135 - val_accuracy: 0.7059\n",
            "Epoch 98/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5737 - accuracy: 0.7354 - val_loss: 0.7435 - val_accuracy: 0.6898\n",
            "Epoch 99/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5785 - accuracy: 0.7438 - val_loss: 0.7790 - val_accuracy: 0.6952\n",
            "Epoch 100/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5803 - accuracy: 0.7426 - val_loss: 0.7044 - val_accuracy: 0.7112\n",
            "Epoch 101/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5947 - accuracy: 0.7295 - val_loss: 0.7514 - val_accuracy: 0.7112\n",
            "Epoch 102/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5947 - accuracy: 0.7319 - val_loss: 0.7701 - val_accuracy: 0.7005\n",
            "Epoch 103/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5752 - accuracy: 0.7426 - val_loss: 0.7237 - val_accuracy: 0.7112\n",
            "Epoch 104/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5796 - accuracy: 0.7396 - val_loss: 0.7409 - val_accuracy: 0.7112\n",
            "Epoch 105/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5834 - accuracy: 0.7366 - val_loss: 0.7741 - val_accuracy: 0.7005\n",
            "Epoch 106/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5771 - accuracy: 0.7372 - val_loss: 0.7214 - val_accuracy: 0.7059\n",
            "Epoch 107/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5781 - accuracy: 0.7473 - val_loss: 0.7109 - val_accuracy: 0.6952\n",
            "Epoch 108/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5685 - accuracy: 0.7360 - val_loss: 0.7619 - val_accuracy: 0.6952\n",
            "Epoch 109/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5667 - accuracy: 0.7479 - val_loss: 0.7704 - val_accuracy: 0.6898\n",
            "Epoch 110/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5679 - accuracy: 0.7360 - val_loss: 0.7381 - val_accuracy: 0.7112\n",
            "Epoch 111/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5842 - accuracy: 0.7325 - val_loss: 0.7172 - val_accuracy: 0.7059\n",
            "Epoch 112/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5691 - accuracy: 0.7539 - val_loss: 0.7342 - val_accuracy: 0.7059\n",
            "Epoch 113/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5548 - accuracy: 0.7479 - val_loss: 0.7249 - val_accuracy: 0.7219\n",
            "Epoch 114/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5697 - accuracy: 0.7473 - val_loss: 0.7592 - val_accuracy: 0.7059\n",
            "Epoch 115/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5717 - accuracy: 0.7420 - val_loss: 0.7609 - val_accuracy: 0.6898\n",
            "Epoch 116/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5470 - accuracy: 0.7485 - val_loss: 0.7475 - val_accuracy: 0.7005\n",
            "Epoch 117/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5589 - accuracy: 0.7497 - val_loss: 0.7312 - val_accuracy: 0.6952\n",
            "Epoch 118/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5759 - accuracy: 0.7337 - val_loss: 0.7292 - val_accuracy: 0.7059\n",
            "Epoch 119/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.7449 - val_loss: 0.7270 - val_accuracy: 0.7059\n",
            "Epoch 120/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5488 - accuracy: 0.7556 - val_loss: 0.7476 - val_accuracy: 0.7005\n",
            "Epoch 121/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5672 - accuracy: 0.7366 - val_loss: 0.7463 - val_accuracy: 0.6952\n",
            "Epoch 122/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5522 - accuracy: 0.7521 - val_loss: 0.7449 - val_accuracy: 0.6738\n",
            "Epoch 123/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5417 - accuracy: 0.7539 - val_loss: 0.7704 - val_accuracy: 0.6898\n",
            "Epoch 124/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5612 - accuracy: 0.7444 - val_loss: 0.7905 - val_accuracy: 0.7059\n",
            "Epoch 125/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5493 - accuracy: 0.7503 - val_loss: 0.7823 - val_accuracy: 0.7112\n",
            "Epoch 126/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.7479 - val_loss: 0.7757 - val_accuracy: 0.6845\n",
            "Epoch 127/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5732 - accuracy: 0.7509 - val_loss: 0.7818 - val_accuracy: 0.7059\n",
            "Epoch 128/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5530 - accuracy: 0.7527 - val_loss: 0.7683 - val_accuracy: 0.6845\n",
            "Epoch 129/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5460 - accuracy: 0.7562 - val_loss: 0.7935 - val_accuracy: 0.6845\n",
            "Epoch 130/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5645 - accuracy: 0.7449 - val_loss: 0.7592 - val_accuracy: 0.6952\n",
            "Epoch 131/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5314 - accuracy: 0.7723 - val_loss: 0.7595 - val_accuracy: 0.6898\n",
            "Epoch 132/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5472 - accuracy: 0.7646 - val_loss: 0.8022 - val_accuracy: 0.7059\n",
            "Epoch 133/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7515 - val_loss: 0.7956 - val_accuracy: 0.7059\n",
            "Epoch 134/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7681 - val_loss: 0.7757 - val_accuracy: 0.6952\n",
            "Epoch 135/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5501 - accuracy: 0.7562 - val_loss: 0.7533 - val_accuracy: 0.6952\n",
            "Epoch 136/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5396 - accuracy: 0.7598 - val_loss: 0.8264 - val_accuracy: 0.6898\n",
            "Epoch 137/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5288 - accuracy: 0.7497 - val_loss: 0.7749 - val_accuracy: 0.7005\n",
            "Epoch 138/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5557 - accuracy: 0.7568 - val_loss: 0.7950 - val_accuracy: 0.7112\n",
            "Epoch 139/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5431 - accuracy: 0.7592 - val_loss: 0.7423 - val_accuracy: 0.7112\n",
            "Epoch 140/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5534 - accuracy: 0.7574 - val_loss: 0.7672 - val_accuracy: 0.6898\n",
            "Epoch 141/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5586 - accuracy: 0.7562 - val_loss: 0.8050 - val_accuracy: 0.6952\n",
            "Epoch 142/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5403 - accuracy: 0.7765 - val_loss: 0.8107 - val_accuracy: 0.6791\n",
            "Epoch 143/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7663 - val_loss: 0.8193 - val_accuracy: 0.7059\n",
            "Epoch 144/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7669 - val_loss: 0.7733 - val_accuracy: 0.7005\n",
            "Epoch 145/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5550 - accuracy: 0.7491 - val_loss: 0.7618 - val_accuracy: 0.7005\n",
            "Epoch 146/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5381 - accuracy: 0.7497 - val_loss: 0.8091 - val_accuracy: 0.7166\n",
            "Epoch 147/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5226 - accuracy: 0.7723 - val_loss: 0.8480 - val_accuracy: 0.6791\n",
            "Epoch 148/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.7705 - val_loss: 0.8110 - val_accuracy: 0.6845\n",
            "Epoch 149/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7652 - val_loss: 0.8098 - val_accuracy: 0.7059\n",
            "Epoch 150/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5304 - accuracy: 0.7568 - val_loss: 0.7811 - val_accuracy: 0.6952\n",
            "Epoch 151/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7759 - val_loss: 0.7841 - val_accuracy: 0.6845\n",
            "Epoch 152/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5278 - accuracy: 0.7681 - val_loss: 0.8054 - val_accuracy: 0.6898\n",
            "Epoch 153/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.7782 - val_loss: 0.7693 - val_accuracy: 0.7005\n",
            "Epoch 154/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5279 - accuracy: 0.7610 - val_loss: 0.8385 - val_accuracy: 0.7005\n",
            "Epoch 155/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7652 - val_loss: 0.8259 - val_accuracy: 0.6791\n",
            "Epoch 156/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7711 - val_loss: 0.8111 - val_accuracy: 0.7059\n",
            "Epoch 157/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5511 - accuracy: 0.7586 - val_loss: 0.7704 - val_accuracy: 0.6952\n",
            "Epoch 158/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5226 - accuracy: 0.7675 - val_loss: 0.8062 - val_accuracy: 0.7059\n",
            "Epoch 159/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5364 - accuracy: 0.7658 - val_loss: 0.8126 - val_accuracy: 0.7059\n",
            "Epoch 160/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5181 - accuracy: 0.7663 - val_loss: 0.8086 - val_accuracy: 0.7059\n",
            "Epoch 161/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4998 - accuracy: 0.7729 - val_loss: 0.8170 - val_accuracy: 0.6684\n",
            "Epoch 162/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5519 - accuracy: 0.7610 - val_loss: 0.8066 - val_accuracy: 0.6791\n",
            "Epoch 163/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5196 - accuracy: 0.7687 - val_loss: 0.8265 - val_accuracy: 0.6845\n",
            "Epoch 164/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5534 - accuracy: 0.7586 - val_loss: 0.7844 - val_accuracy: 0.7112\n",
            "Epoch 165/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5149 - accuracy: 0.7741 - val_loss: 0.7834 - val_accuracy: 0.7059\n",
            "Epoch 166/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7646 - val_loss: 0.7954 - val_accuracy: 0.6898\n",
            "Epoch 167/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5238 - accuracy: 0.7663 - val_loss: 0.8023 - val_accuracy: 0.7059\n",
            "Epoch 168/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5144 - accuracy: 0.7812 - val_loss: 0.8126 - val_accuracy: 0.7112\n",
            "Epoch 169/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5123 - accuracy: 0.7741 - val_loss: 0.8707 - val_accuracy: 0.6738\n",
            "Epoch 170/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5235 - accuracy: 0.7699 - val_loss: 0.8481 - val_accuracy: 0.7059\n",
            "Epoch 171/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5277 - accuracy: 0.7747 - val_loss: 0.8430 - val_accuracy: 0.6952\n",
            "Epoch 172/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5119 - accuracy: 0.7663 - val_loss: 0.8290 - val_accuracy: 0.7005\n",
            "Epoch 173/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7669 - val_loss: 0.8537 - val_accuracy: 0.6791\n",
            "Epoch 174/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.7842 - val_loss: 0.8314 - val_accuracy: 0.7005\n",
            "Epoch 175/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7794 - val_loss: 0.8268 - val_accuracy: 0.6898\n",
            "Epoch 176/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5261 - accuracy: 0.7699 - val_loss: 0.7582 - val_accuracy: 0.6791\n",
            "Epoch 177/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5210 - accuracy: 0.7699 - val_loss: 0.8094 - val_accuracy: 0.7112\n",
            "Epoch 178/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4983 - accuracy: 0.7806 - val_loss: 0.8241 - val_accuracy: 0.6791\n",
            "Epoch 179/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5108 - accuracy: 0.7693 - val_loss: 0.8330 - val_accuracy: 0.6738\n",
            "Epoch 180/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4877 - accuracy: 0.7901 - val_loss: 0.8270 - val_accuracy: 0.6898\n",
            "Epoch 181/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7824 - val_loss: 0.8604 - val_accuracy: 0.6898\n",
            "Epoch 182/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5213 - accuracy: 0.7705 - val_loss: 0.8100 - val_accuracy: 0.6952\n",
            "Epoch 183/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4898 - accuracy: 0.7901 - val_loss: 0.8679 - val_accuracy: 0.6684\n",
            "Epoch 184/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5104 - accuracy: 0.7765 - val_loss: 0.7925 - val_accuracy: 0.6898\n",
            "Epoch 185/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5231 - accuracy: 0.7717 - val_loss: 0.7715 - val_accuracy: 0.7326\n",
            "Epoch 186/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.7824 - val_loss: 0.8314 - val_accuracy: 0.7005\n",
            "Epoch 187/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4947 - accuracy: 0.7842 - val_loss: 0.8319 - val_accuracy: 0.7112\n",
            "Epoch 188/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.7812 - val_loss: 0.8200 - val_accuracy: 0.6898\n",
            "Epoch 189/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7794 - val_loss: 0.7970 - val_accuracy: 0.7059\n",
            "Epoch 190/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4993 - accuracy: 0.7836 - val_loss: 0.7963 - val_accuracy: 0.6845\n",
            "Epoch 191/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.7943 - val_loss: 0.8971 - val_accuracy: 0.6898\n",
            "Epoch 192/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5235 - accuracy: 0.7747 - val_loss: 0.8186 - val_accuracy: 0.7166\n",
            "Epoch 193/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.7693 - val_loss: 0.7891 - val_accuracy: 0.7273\n",
            "Epoch 194/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4946 - accuracy: 0.7854 - val_loss: 0.8634 - val_accuracy: 0.6791\n",
            "Epoch 195/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5002 - accuracy: 0.7883 - val_loss: 0.8215 - val_accuracy: 0.6898\n",
            "Epoch 196/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4986 - accuracy: 0.7842 - val_loss: 0.8097 - val_accuracy: 0.6952\n",
            "Epoch 197/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4883 - accuracy: 0.7931 - val_loss: 0.8460 - val_accuracy: 0.6898\n",
            "Epoch 198/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4821 - accuracy: 0.7973 - val_loss: 0.8701 - val_accuracy: 0.7112\n",
            "Epoch 199/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4870 - accuracy: 0.7889 - val_loss: 0.8704 - val_accuracy: 0.6898\n",
            "Epoch 200/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4855 - accuracy: 0.7919 - val_loss: 0.8497 - val_accuracy: 0.7005\n",
            "Epoch 201/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5085 - accuracy: 0.7800 - val_loss: 0.8585 - val_accuracy: 0.6845\n",
            "Epoch 202/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4898 - accuracy: 0.7925 - val_loss: 0.8227 - val_accuracy: 0.7112\n",
            "Epoch 203/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4707 - accuracy: 0.8032 - val_loss: 0.8598 - val_accuracy: 0.7005\n",
            "Epoch 204/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.7967 - val_loss: 0.8487 - val_accuracy: 0.7059\n",
            "Epoch 205/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4937 - accuracy: 0.7949 - val_loss: 0.8356 - val_accuracy: 0.7059\n",
            "Epoch 206/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4728 - accuracy: 0.7996 - val_loss: 0.7963 - val_accuracy: 0.6952\n",
            "Epoch 207/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.7919 - val_loss: 0.8360 - val_accuracy: 0.7112\n",
            "Epoch 208/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.7925 - val_loss: 0.8465 - val_accuracy: 0.6952\n",
            "Epoch 209/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4886 - accuracy: 0.7800 - val_loss: 0.8593 - val_accuracy: 0.7005\n",
            "Epoch 210/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5004 - accuracy: 0.7842 - val_loss: 0.8187 - val_accuracy: 0.6952\n",
            "Epoch 211/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7901 - val_loss: 0.8222 - val_accuracy: 0.6845\n",
            "Epoch 212/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7836 - val_loss: 0.8595 - val_accuracy: 0.6791\n",
            "Epoch 213/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.8050 - val_loss: 0.8661 - val_accuracy: 0.6631\n",
            "Epoch 214/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4640 - accuracy: 0.7949 - val_loss: 0.9378 - val_accuracy: 0.6578\n",
            "Epoch 215/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.7883 - val_loss: 0.8874 - val_accuracy: 0.6631\n",
            "Epoch 216/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.7878 - val_loss: 0.9087 - val_accuracy: 0.6524\n",
            "Epoch 217/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.7973 - val_loss: 0.8249 - val_accuracy: 0.6845\n",
            "Epoch 218/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.7800 - val_loss: 0.8765 - val_accuracy: 0.6845\n",
            "Epoch 219/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4781 - accuracy: 0.8008 - val_loss: 0.8153 - val_accuracy: 0.6791\n",
            "Epoch 220/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4835 - accuracy: 0.7824 - val_loss: 0.8618 - val_accuracy: 0.6845\n",
            "Epoch 221/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.7985 - val_loss: 0.9100 - val_accuracy: 0.6684\n",
            "Epoch 222/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4835 - accuracy: 0.7878 - val_loss: 0.8543 - val_accuracy: 0.7005\n",
            "Epoch 223/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4951 - accuracy: 0.7985 - val_loss: 0.8524 - val_accuracy: 0.7112\n",
            "Epoch 224/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4962 - accuracy: 0.7883 - val_loss: 0.8717 - val_accuracy: 0.7005\n",
            "Epoch 225/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4919 - accuracy: 0.7979 - val_loss: 0.8881 - val_accuracy: 0.6845\n",
            "Epoch 226/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4701 - accuracy: 0.8074 - val_loss: 0.8878 - val_accuracy: 0.6898\n",
            "Epoch 227/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.8103 - val_loss: 0.9173 - val_accuracy: 0.6684\n",
            "Epoch 228/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4794 - accuracy: 0.7812 - val_loss: 0.8628 - val_accuracy: 0.6791\n",
            "Epoch 229/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.8092 - val_loss: 0.8537 - val_accuracy: 0.6631\n",
            "Epoch 230/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.8032 - val_loss: 0.8722 - val_accuracy: 0.6898\n",
            "Epoch 231/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.8038 - val_loss: 0.8667 - val_accuracy: 0.6738\n",
            "Epoch 232/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4778 - accuracy: 0.7931 - val_loss: 0.8992 - val_accuracy: 0.7005\n",
            "Epoch 233/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4721 - accuracy: 0.7949 - val_loss: 0.9304 - val_accuracy: 0.6631\n",
            "Epoch 234/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4699 - accuracy: 0.7985 - val_loss: 0.9328 - val_accuracy: 0.6845\n",
            "Epoch 235/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4717 - accuracy: 0.8038 - val_loss: 0.9242 - val_accuracy: 0.6952\n",
            "Epoch 236/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4765 - accuracy: 0.7943 - val_loss: 0.8863 - val_accuracy: 0.6524\n",
            "Epoch 237/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4679 - accuracy: 0.8068 - val_loss: 0.9172 - val_accuracy: 0.6684\n",
            "Epoch 238/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4535 - accuracy: 0.8068 - val_loss: 0.9675 - val_accuracy: 0.6845\n",
            "Epoch 239/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.7967 - val_loss: 0.9834 - val_accuracy: 0.6738\n",
            "Epoch 240/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7961 - val_loss: 0.9380 - val_accuracy: 0.6471\n",
            "Epoch 241/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.8210 - val_loss: 0.9489 - val_accuracy: 0.6684\n",
            "Epoch 242/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4510 - accuracy: 0.8139 - val_loss: 0.9169 - val_accuracy: 0.6684\n",
            "Epoch 243/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.8062 - val_loss: 0.8938 - val_accuracy: 0.6738\n",
            "Epoch 244/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4445 - accuracy: 0.8086 - val_loss: 0.8592 - val_accuracy: 0.6791\n",
            "Epoch 245/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.7967 - val_loss: 0.8739 - val_accuracy: 0.6791\n",
            "Epoch 246/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4682 - accuracy: 0.8062 - val_loss: 0.8644 - val_accuracy: 0.6898\n",
            "Epoch 247/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.8050 - val_loss: 0.8332 - val_accuracy: 0.6845\n",
            "Epoch 248/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4639 - accuracy: 0.8068 - val_loss: 0.9194 - val_accuracy: 0.6898\n",
            "Epoch 249/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.8026 - val_loss: 0.9159 - val_accuracy: 0.6738\n",
            "Epoch 250/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.8210 - val_loss: 0.9080 - val_accuracy: 0.6791\n",
            "Epoch 251/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.8103 - val_loss: 0.9195 - val_accuracy: 0.6898\n",
            "Epoch 252/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4728 - accuracy: 0.8008 - val_loss: 0.8538 - val_accuracy: 0.7005\n",
            "Epoch 253/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4726 - accuracy: 0.8056 - val_loss: 0.9019 - val_accuracy: 0.7005\n",
            "Epoch 254/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4875 - accuracy: 0.7854 - val_loss: 0.8861 - val_accuracy: 0.6684\n",
            "Epoch 255/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4673 - accuracy: 0.8133 - val_loss: 0.9285 - val_accuracy: 0.6898\n",
            "Epoch 256/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4462 - accuracy: 0.8086 - val_loss: 0.8530 - val_accuracy: 0.6845\n",
            "Epoch 257/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.8056 - val_loss: 0.9414 - val_accuracy: 0.6898\n",
            "Epoch 258/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.8050 - val_loss: 0.8672 - val_accuracy: 0.6845\n",
            "Epoch 259/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.7990 - val_loss: 0.9069 - val_accuracy: 0.6952\n",
            "Epoch 260/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.8020 - val_loss: 0.8702 - val_accuracy: 0.6845\n",
            "Epoch 261/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4484 - accuracy: 0.8068 - val_loss: 0.9838 - val_accuracy: 0.6898\n",
            "Epoch 262/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8145 - val_loss: 0.9137 - val_accuracy: 0.6845\n",
            "Epoch 263/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.7967 - val_loss: 0.8928 - val_accuracy: 0.6738\n",
            "Epoch 264/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4413 - accuracy: 0.8115 - val_loss: 1.0000 - val_accuracy: 0.6791\n",
            "Epoch 265/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.8080 - val_loss: 0.9577 - val_accuracy: 0.6845\n",
            "Epoch 266/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.8074 - val_loss: 0.9792 - val_accuracy: 0.6631\n",
            "Epoch 267/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.8199 - val_loss: 0.9792 - val_accuracy: 0.6845\n",
            "Epoch 268/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.7996 - val_loss: 0.9403 - val_accuracy: 0.6845\n",
            "Epoch 269/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.8317 - val_loss: 0.9360 - val_accuracy: 0.6684\n",
            "Epoch 270/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.8109 - val_loss: 0.9534 - val_accuracy: 0.6845\n",
            "Epoch 271/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4391 - accuracy: 0.8080 - val_loss: 0.8815 - val_accuracy: 0.6578\n",
            "Epoch 272/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8139 - val_loss: 0.9208 - val_accuracy: 0.6578\n",
            "Epoch 273/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8228 - val_loss: 0.9559 - val_accuracy: 0.6631\n",
            "Epoch 274/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.8127 - val_loss: 0.9603 - val_accuracy: 0.6791\n",
            "Epoch 275/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.8020 - val_loss: 0.9587 - val_accuracy: 0.6578\n",
            "Epoch 276/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.8175 - val_loss: 0.8294 - val_accuracy: 0.6898\n",
            "Epoch 277/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.8282 - val_loss: 0.9428 - val_accuracy: 0.6791\n",
            "Epoch 278/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4590 - accuracy: 0.8020 - val_loss: 0.8905 - val_accuracy: 0.6738\n",
            "Epoch 279/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4467 - accuracy: 0.8157 - val_loss: 0.9933 - val_accuracy: 0.6791\n",
            "Epoch 280/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4402 - accuracy: 0.8210 - val_loss: 0.9604 - val_accuracy: 0.6738\n",
            "Epoch 281/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4431 - accuracy: 0.8234 - val_loss: 0.9310 - val_accuracy: 0.6684\n",
            "Epoch 282/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4350 - accuracy: 0.8228 - val_loss: 0.9437 - val_accuracy: 0.6845\n",
            "Epoch 283/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.8216 - val_loss: 0.9677 - val_accuracy: 0.6738\n",
            "Epoch 284/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.8014 - val_loss: 0.9723 - val_accuracy: 0.6738\n",
            "Epoch 285/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4395 - accuracy: 0.8151 - val_loss: 0.9425 - val_accuracy: 0.6631\n",
            "Epoch 286/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8199 - val_loss: 0.9721 - val_accuracy: 0.6738\n",
            "Epoch 287/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.8228 - val_loss: 1.0481 - val_accuracy: 0.6578\n",
            "Epoch 288/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4174 - accuracy: 0.8270 - val_loss: 1.0107 - val_accuracy: 0.6578\n",
            "Epoch 289/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4432 - accuracy: 0.8163 - val_loss: 1.0249 - val_accuracy: 0.6578\n",
            "Epoch 290/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4195 - accuracy: 0.8210 - val_loss: 1.0090 - val_accuracy: 0.6845\n",
            "Epoch 291/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.8199 - val_loss: 0.9653 - val_accuracy: 0.6631\n",
            "Epoch 292/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4337 - accuracy: 0.8216 - val_loss: 0.9718 - val_accuracy: 0.6738\n",
            "Epoch 293/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4461 - accuracy: 0.8115 - val_loss: 1.0022 - val_accuracy: 0.6738\n",
            "Epoch 294/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4484 - accuracy: 0.8145 - val_loss: 0.9613 - val_accuracy: 0.6631\n",
            "Epoch 295/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4316 - accuracy: 0.8264 - val_loss: 0.9794 - val_accuracy: 0.6631\n",
            "Epoch 296/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8210 - val_loss: 1.0148 - val_accuracy: 0.6524\n",
            "Epoch 297/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4400 - accuracy: 0.8288 - val_loss: 0.9199 - val_accuracy: 0.6738\n",
            "Epoch 298/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4282 - accuracy: 0.8228 - val_loss: 0.9685 - val_accuracy: 0.6684\n",
            "Epoch 299/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8282 - val_loss: 0.9825 - val_accuracy: 0.6417\n",
            "Epoch 300/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8246 - val_loss: 0.9777 - val_accuracy: 0.6524\n",
            "Epoch 301/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4279 - accuracy: 0.8282 - val_loss: 0.9517 - val_accuracy: 0.6791\n",
            "Epoch 302/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4116 - accuracy: 0.8276 - val_loss: 0.9858 - val_accuracy: 0.6578\n",
            "Epoch 303/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.8234 - val_loss: 1.0595 - val_accuracy: 0.6578\n",
            "Epoch 304/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4213 - accuracy: 0.8175 - val_loss: 1.0661 - val_accuracy: 0.6578\n",
            "Epoch 305/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4274 - accuracy: 0.8246 - val_loss: 1.0113 - val_accuracy: 0.6471\n",
            "Epoch 306/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4190 - accuracy: 0.8181 - val_loss: 1.0779 - val_accuracy: 0.6364\n",
            "Epoch 307/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.8157 - val_loss: 1.0002 - val_accuracy: 0.6417\n",
            "Epoch 308/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4189 - accuracy: 0.8353 - val_loss: 1.0157 - val_accuracy: 0.6631\n",
            "Epoch 309/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4265 - accuracy: 0.8193 - val_loss: 0.9502 - val_accuracy: 0.6578\n",
            "Epoch 310/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4179 - accuracy: 0.8246 - val_loss: 0.9896 - val_accuracy: 0.6631\n",
            "Epoch 311/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4116 - accuracy: 0.8294 - val_loss: 1.0137 - val_accuracy: 0.6471\n",
            "Epoch 312/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4136 - accuracy: 0.8347 - val_loss: 1.0488 - val_accuracy: 0.6471\n",
            "Epoch 313/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3985 - accuracy: 0.8258 - val_loss: 1.0152 - val_accuracy: 0.6791\n",
            "Epoch 314/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4225 - accuracy: 0.8306 - val_loss: 0.9900 - val_accuracy: 0.6310\n",
            "Epoch 315/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8210 - val_loss: 1.0129 - val_accuracy: 0.6631\n",
            "Epoch 316/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8270 - val_loss: 1.0308 - val_accuracy: 0.6471\n",
            "Epoch 317/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4285 - accuracy: 0.8252 - val_loss: 0.9928 - val_accuracy: 0.6150\n",
            "Epoch 318/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4167 - accuracy: 0.8258 - val_loss: 1.0455 - val_accuracy: 0.6684\n",
            "Epoch 319/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4216 - accuracy: 0.8306 - val_loss: 0.9911 - val_accuracy: 0.6631\n",
            "Epoch 320/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8234 - val_loss: 0.9693 - val_accuracy: 0.6684\n",
            "Epoch 321/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4240 - accuracy: 0.8282 - val_loss: 0.9546 - val_accuracy: 0.6524\n",
            "Epoch 322/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4093 - accuracy: 0.8276 - val_loss: 1.0415 - val_accuracy: 0.6578\n",
            "Epoch 323/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4055 - accuracy: 0.8312 - val_loss: 1.0425 - val_accuracy: 0.6631\n",
            "Epoch 324/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.8193 - val_loss: 1.0177 - val_accuracy: 0.6471\n",
            "Epoch 325/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4191 - accuracy: 0.8276 - val_loss: 0.9573 - val_accuracy: 0.6417\n",
            "Epoch 326/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8448 - val_loss: 1.0533 - val_accuracy: 0.6791\n",
            "Epoch 327/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4392 - accuracy: 0.8163 - val_loss: 0.9872 - val_accuracy: 0.6738\n",
            "Epoch 328/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8288 - val_loss: 0.9872 - val_accuracy: 0.6310\n",
            "Epoch 329/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4066 - accuracy: 0.8335 - val_loss: 0.9961 - val_accuracy: 0.6684\n",
            "Epoch 330/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4159 - accuracy: 0.8389 - val_loss: 0.9994 - val_accuracy: 0.6791\n",
            "Epoch 331/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8317 - val_loss: 0.9697 - val_accuracy: 0.6791\n",
            "Epoch 332/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4147 - accuracy: 0.8264 - val_loss: 1.0273 - val_accuracy: 0.6738\n",
            "Epoch 333/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8300 - val_loss: 1.0263 - val_accuracy: 0.6738\n",
            "Epoch 334/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4110 - accuracy: 0.8323 - val_loss: 0.9518 - val_accuracy: 0.6631\n",
            "Epoch 335/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4147 - accuracy: 0.8294 - val_loss: 0.9274 - val_accuracy: 0.6471\n",
            "Epoch 336/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8359 - val_loss: 1.0055 - val_accuracy: 0.6631\n",
            "Epoch 337/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3957 - accuracy: 0.8401 - val_loss: 1.0921 - val_accuracy: 0.6738\n",
            "Epoch 338/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4037 - accuracy: 0.8329 - val_loss: 1.0504 - val_accuracy: 0.6791\n",
            "Epoch 339/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4047 - accuracy: 0.8329 - val_loss: 1.0935 - val_accuracy: 0.6684\n",
            "Epoch 340/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4125 - accuracy: 0.8323 - val_loss: 0.9947 - val_accuracy: 0.6845\n",
            "Epoch 341/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.8270 - val_loss: 1.0319 - val_accuracy: 0.6578\n",
            "Epoch 342/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4075 - accuracy: 0.8359 - val_loss: 1.0673 - val_accuracy: 0.6364\n",
            "Epoch 343/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3973 - accuracy: 0.8484 - val_loss: 1.0647 - val_accuracy: 0.6684\n",
            "Epoch 344/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3983 - accuracy: 0.8294 - val_loss: 0.9888 - val_accuracy: 0.6791\n",
            "Epoch 345/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8335 - val_loss: 0.9676 - val_accuracy: 0.6578\n",
            "Epoch 346/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3983 - accuracy: 0.8282 - val_loss: 1.0296 - val_accuracy: 0.6524\n",
            "Epoch 347/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4200 - accuracy: 0.8353 - val_loss: 0.9819 - val_accuracy: 0.6524\n",
            "Epoch 348/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8335 - val_loss: 0.9587 - val_accuracy: 0.6738\n",
            "Epoch 349/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4160 - accuracy: 0.8371 - val_loss: 1.0016 - val_accuracy: 0.6524\n",
            "Epoch 350/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3974 - accuracy: 0.8270 - val_loss: 1.0129 - val_accuracy: 0.6524\n",
            "Epoch 351/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4053 - accuracy: 0.8323 - val_loss: 1.0351 - val_accuracy: 0.6471\n",
            "Epoch 352/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4046 - accuracy: 0.8407 - val_loss: 1.0237 - val_accuracy: 0.6364\n",
            "Epoch 353/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4166 - accuracy: 0.8246 - val_loss: 0.9852 - val_accuracy: 0.6417\n",
            "Epoch 354/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8383 - val_loss: 1.0575 - val_accuracy: 0.6578\n",
            "Epoch 355/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8454 - val_loss: 1.0478 - val_accuracy: 0.6738\n",
            "Epoch 356/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4094 - accuracy: 0.8276 - val_loss: 1.0328 - val_accuracy: 0.6738\n",
            "Epoch 357/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8288 - val_loss: 1.0953 - val_accuracy: 0.6578\n",
            "Epoch 358/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.8371 - val_loss: 0.9370 - val_accuracy: 0.6631\n",
            "Epoch 359/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4106 - accuracy: 0.8240 - val_loss: 0.9733 - val_accuracy: 0.6364\n",
            "Epoch 360/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8520 - val_loss: 1.0287 - val_accuracy: 0.6524\n",
            "Epoch 361/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3814 - accuracy: 0.8472 - val_loss: 1.0782 - val_accuracy: 0.6203\n",
            "Epoch 362/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8460 - val_loss: 1.0151 - val_accuracy: 0.6524\n",
            "Epoch 363/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.8424 - val_loss: 1.0896 - val_accuracy: 0.6150\n",
            "Epoch 364/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8383 - val_loss: 1.0858 - val_accuracy: 0.6364\n",
            "Epoch 365/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.8430 - val_loss: 1.0441 - val_accuracy: 0.6578\n",
            "Epoch 366/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3810 - accuracy: 0.8436 - val_loss: 1.0830 - val_accuracy: 0.6631\n",
            "Epoch 367/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3967 - accuracy: 0.8317 - val_loss: 0.9799 - val_accuracy: 0.6364\n",
            "Epoch 368/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8413 - val_loss: 1.0811 - val_accuracy: 0.6631\n",
            "Epoch 369/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3873 - accuracy: 0.8306 - val_loss: 1.0454 - val_accuracy: 0.6364\n",
            "Epoch 370/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3910 - accuracy: 0.8496 - val_loss: 0.9980 - val_accuracy: 0.6471\n",
            "Epoch 371/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4071 - accuracy: 0.8317 - val_loss: 1.0622 - val_accuracy: 0.6631\n",
            "Epoch 372/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8329 - val_loss: 1.0346 - val_accuracy: 0.6738\n",
            "Epoch 373/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4074 - accuracy: 0.8258 - val_loss: 1.0761 - val_accuracy: 0.6524\n",
            "Epoch 374/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.8419 - val_loss: 1.0618 - val_accuracy: 0.6738\n",
            "Epoch 375/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3842 - accuracy: 0.8424 - val_loss: 1.0505 - val_accuracy: 0.6845\n",
            "Epoch 376/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3688 - accuracy: 0.8573 - val_loss: 1.1198 - val_accuracy: 0.6791\n",
            "Epoch 377/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4022 - accuracy: 0.8371 - val_loss: 1.0504 - val_accuracy: 0.6684\n",
            "Epoch 378/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4078 - accuracy: 0.8312 - val_loss: 1.0549 - val_accuracy: 0.6845\n",
            "Epoch 379/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4033 - accuracy: 0.8413 - val_loss: 1.0541 - val_accuracy: 0.6578\n",
            "Epoch 380/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3803 - accuracy: 0.8430 - val_loss: 1.0428 - val_accuracy: 0.6631\n",
            "Epoch 381/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3848 - accuracy: 0.8413 - val_loss: 1.0715 - val_accuracy: 0.6364\n",
            "Epoch 382/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3923 - accuracy: 0.8377 - val_loss: 1.0020 - val_accuracy: 0.6578\n",
            "Epoch 383/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4038 - accuracy: 0.8454 - val_loss: 1.0057 - val_accuracy: 0.6417\n",
            "Epoch 384/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4101 - accuracy: 0.8395 - val_loss: 0.9932 - val_accuracy: 0.6417\n",
            "Epoch 385/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3796 - accuracy: 0.8514 - val_loss: 1.0568 - val_accuracy: 0.6524\n",
            "Epoch 386/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3882 - accuracy: 0.8389 - val_loss: 1.1176 - val_accuracy: 0.6578\n",
            "Epoch 387/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4071 - accuracy: 0.8389 - val_loss: 1.0656 - val_accuracy: 0.6845\n",
            "Epoch 388/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8395 - val_loss: 1.0462 - val_accuracy: 0.6150\n",
            "Epoch 389/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3876 - accuracy: 0.8419 - val_loss: 1.0934 - val_accuracy: 0.6150\n",
            "Epoch 390/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8365 - val_loss: 1.0079 - val_accuracy: 0.6364\n",
            "Epoch 391/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3783 - accuracy: 0.8472 - val_loss: 1.0540 - val_accuracy: 0.6310\n",
            "Epoch 392/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8543 - val_loss: 1.0971 - val_accuracy: 0.6150\n",
            "Epoch 393/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3739 - accuracy: 0.8549 - val_loss: 1.0334 - val_accuracy: 0.6738\n",
            "Epoch 394/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3808 - accuracy: 0.8353 - val_loss: 1.0785 - val_accuracy: 0.6364\n",
            "Epoch 395/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.8323 - val_loss: 1.0774 - val_accuracy: 0.6043\n",
            "Epoch 396/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4092 - accuracy: 0.8371 - val_loss: 1.0930 - val_accuracy: 0.6364\n",
            "Epoch 397/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3882 - accuracy: 0.8419 - val_loss: 1.0450 - val_accuracy: 0.6471\n",
            "Epoch 398/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8502 - val_loss: 1.0710 - val_accuracy: 0.6417\n",
            "Epoch 399/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3777 - accuracy: 0.8442 - val_loss: 1.1085 - val_accuracy: 0.6524\n",
            "Epoch 400/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.8520 - val_loss: 1.1102 - val_accuracy: 0.6524\n",
            "Epoch 401/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8472 - val_loss: 1.1116 - val_accuracy: 0.6471\n",
            "Epoch 402/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3735 - accuracy: 0.8466 - val_loss: 1.1234 - val_accuracy: 0.6524\n",
            "Epoch 403/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3615 - accuracy: 0.8537 - val_loss: 1.1451 - val_accuracy: 0.6524\n",
            "Epoch 404/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3943 - accuracy: 0.8371 - val_loss: 1.0536 - val_accuracy: 0.6310\n",
            "Epoch 405/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8442 - val_loss: 1.0563 - val_accuracy: 0.6203\n",
            "Epoch 406/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8496 - val_loss: 1.0786 - val_accuracy: 0.6631\n",
            "Epoch 407/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4043 - accuracy: 0.8413 - val_loss: 0.9955 - val_accuracy: 0.6578\n",
            "Epoch 408/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3730 - accuracy: 0.8472 - val_loss: 1.0382 - val_accuracy: 0.6684\n",
            "Epoch 409/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8508 - val_loss: 1.1114 - val_accuracy: 0.6578\n",
            "Epoch 410/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3833 - accuracy: 0.8484 - val_loss: 1.0791 - val_accuracy: 0.6524\n",
            "Epoch 411/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3813 - accuracy: 0.8502 - val_loss: 1.0148 - val_accuracy: 0.6631\n",
            "Epoch 412/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3858 - accuracy: 0.8424 - val_loss: 1.0119 - val_accuracy: 0.6417\n",
            "Epoch 413/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8329 - val_loss: 0.9680 - val_accuracy: 0.6364\n",
            "Epoch 414/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3835 - accuracy: 0.8401 - val_loss: 1.0351 - val_accuracy: 0.6524\n",
            "Epoch 415/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3822 - accuracy: 0.8371 - val_loss: 1.0177 - val_accuracy: 0.6417\n",
            "Epoch 416/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3513 - accuracy: 0.8561 - val_loss: 1.0391 - val_accuracy: 0.6417\n",
            "Epoch 417/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8448 - val_loss: 1.0484 - val_accuracy: 0.6845\n",
            "Epoch 418/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3516 - accuracy: 0.8627 - val_loss: 1.1616 - val_accuracy: 0.6310\n",
            "Epoch 419/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8490 - val_loss: 1.1526 - val_accuracy: 0.6257\n",
            "Epoch 420/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8496 - val_loss: 1.0800 - val_accuracy: 0.6578\n",
            "Epoch 421/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4111 - accuracy: 0.8436 - val_loss: 1.0716 - val_accuracy: 0.6257\n",
            "Epoch 422/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4102 - accuracy: 0.8383 - val_loss: 1.0533 - val_accuracy: 0.6578\n",
            "Epoch 423/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3582 - accuracy: 0.8514 - val_loss: 1.0659 - val_accuracy: 0.6845\n",
            "Epoch 424/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8490 - val_loss: 1.1421 - val_accuracy: 0.6471\n",
            "Epoch 425/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8472 - val_loss: 0.9625 - val_accuracy: 0.6738\n",
            "Epoch 426/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8567 - val_loss: 0.9874 - val_accuracy: 0.6738\n",
            "Epoch 427/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3584 - accuracy: 0.8543 - val_loss: 1.0769 - val_accuracy: 0.6684\n",
            "Epoch 428/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8496 - val_loss: 1.0047 - val_accuracy: 0.6471\n",
            "Epoch 429/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3660 - accuracy: 0.8537 - val_loss: 1.0515 - val_accuracy: 0.6471\n",
            "Epoch 430/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.8460 - val_loss: 1.0323 - val_accuracy: 0.6471\n",
            "Epoch 431/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3639 - accuracy: 0.8520 - val_loss: 1.1619 - val_accuracy: 0.6310\n",
            "Epoch 432/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3699 - accuracy: 0.8633 - val_loss: 1.0098 - val_accuracy: 0.6684\n",
            "Epoch 433/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8639 - val_loss: 1.0105 - val_accuracy: 0.6684\n",
            "Epoch 434/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3981 - accuracy: 0.8490 - val_loss: 0.9675 - val_accuracy: 0.6417\n",
            "Epoch 435/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8484 - val_loss: 0.9686 - val_accuracy: 0.6364\n",
            "Epoch 436/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8377 - val_loss: 0.8891 - val_accuracy: 0.6738\n",
            "Epoch 437/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3665 - accuracy: 0.8526 - val_loss: 0.9424 - val_accuracy: 0.6417\n",
            "Epoch 438/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3641 - accuracy: 0.8496 - val_loss: 0.9584 - val_accuracy: 0.6578\n",
            "Epoch 439/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3599 - accuracy: 0.8543 - val_loss: 1.0359 - val_accuracy: 0.6631\n",
            "Epoch 440/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3670 - accuracy: 0.8484 - val_loss: 1.0970 - val_accuracy: 0.6203\n",
            "Epoch 441/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3621 - accuracy: 0.8633 - val_loss: 1.0602 - val_accuracy: 0.6471\n",
            "Epoch 442/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3804 - accuracy: 0.8395 - val_loss: 1.0376 - val_accuracy: 0.6631\n",
            "Epoch 443/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3663 - accuracy: 0.8496 - val_loss: 1.0643 - val_accuracy: 0.6471\n",
            "Epoch 444/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8454 - val_loss: 1.1102 - val_accuracy: 0.6471\n",
            "Epoch 445/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3827 - accuracy: 0.8371 - val_loss: 1.0798 - val_accuracy: 0.6364\n",
            "Epoch 446/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3582 - accuracy: 0.8579 - val_loss: 1.0251 - val_accuracy: 0.6471\n",
            "Epoch 447/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3805 - accuracy: 0.8508 - val_loss: 0.9932 - val_accuracy: 0.6203\n",
            "Epoch 448/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8508 - val_loss: 0.9922 - val_accuracy: 0.6578\n",
            "Epoch 449/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3747 - accuracy: 0.8526 - val_loss: 1.0368 - val_accuracy: 0.6524\n",
            "Epoch 450/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8579 - val_loss: 1.0032 - val_accuracy: 0.6417\n",
            "Epoch 451/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3599 - accuracy: 0.8555 - val_loss: 1.0658 - val_accuracy: 0.6524\n",
            "Epoch 452/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8555 - val_loss: 1.0524 - val_accuracy: 0.6257\n",
            "Epoch 453/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3503 - accuracy: 0.8573 - val_loss: 1.0964 - val_accuracy: 0.6791\n",
            "Epoch 454/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3633 - accuracy: 0.8579 - val_loss: 1.0385 - val_accuracy: 0.6364\n",
            "Epoch 455/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3737 - accuracy: 0.8490 - val_loss: 1.0710 - val_accuracy: 0.6578\n",
            "Epoch 456/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3626 - accuracy: 0.8514 - val_loss: 0.9788 - val_accuracy: 0.6310\n",
            "Epoch 457/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8549 - val_loss: 1.0700 - val_accuracy: 0.6257\n",
            "Epoch 458/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8514 - val_loss: 1.0996 - val_accuracy: 0.6578\n",
            "Epoch 459/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3683 - accuracy: 0.8478 - val_loss: 1.0681 - val_accuracy: 0.6631\n",
            "Epoch 460/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3601 - accuracy: 0.8508 - val_loss: 1.0278 - val_accuracy: 0.6631\n",
            "Epoch 461/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3631 - accuracy: 0.8639 - val_loss: 1.0658 - val_accuracy: 0.7059\n",
            "Epoch 462/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8407 - val_loss: 1.0025 - val_accuracy: 0.6845\n",
            "Epoch 463/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3573 - accuracy: 0.8579 - val_loss: 1.1041 - val_accuracy: 0.6684\n",
            "Epoch 464/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3746 - accuracy: 0.8466 - val_loss: 1.0342 - val_accuracy: 0.6791\n",
            "Epoch 465/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3550 - accuracy: 0.8573 - val_loss: 0.9993 - val_accuracy: 0.6845\n",
            "Epoch 466/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.8508 - val_loss: 1.1238 - val_accuracy: 0.6738\n",
            "Epoch 467/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8692 - val_loss: 1.0438 - val_accuracy: 0.6684\n",
            "Epoch 468/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8579 - val_loss: 1.1575 - val_accuracy: 0.6791\n",
            "Epoch 469/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3433 - accuracy: 0.8621 - val_loss: 0.9936 - val_accuracy: 0.6684\n",
            "Epoch 470/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3536 - accuracy: 0.8561 - val_loss: 1.0204 - val_accuracy: 0.6364\n",
            "Epoch 471/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3508 - accuracy: 0.8615 - val_loss: 1.1152 - val_accuracy: 0.6257\n",
            "Epoch 472/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3487 - accuracy: 0.8597 - val_loss: 1.0741 - val_accuracy: 0.6738\n",
            "Epoch 473/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3872 - accuracy: 0.8430 - val_loss: 1.0717 - val_accuracy: 0.6578\n",
            "Epoch 474/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3598 - accuracy: 0.8644 - val_loss: 1.0363 - val_accuracy: 0.6524\n",
            "Epoch 475/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3574 - accuracy: 0.8621 - val_loss: 1.0370 - val_accuracy: 0.6738\n",
            "Epoch 476/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8436 - val_loss: 1.0687 - val_accuracy: 0.6578\n",
            "Epoch 477/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8478 - val_loss: 1.0315 - val_accuracy: 0.6578\n",
            "Epoch 478/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8537 - val_loss: 1.0586 - val_accuracy: 0.6257\n",
            "Epoch 479/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8728 - val_loss: 1.0308 - val_accuracy: 0.6631\n",
            "Epoch 480/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3511 - accuracy: 0.8656 - val_loss: 1.0508 - val_accuracy: 0.6631\n",
            "Epoch 481/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8419 - val_loss: 1.0126 - val_accuracy: 0.6417\n",
            "Epoch 482/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3569 - accuracy: 0.8692 - val_loss: 1.0085 - val_accuracy: 0.6684\n",
            "Epoch 483/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8514 - val_loss: 1.0115 - val_accuracy: 0.6578\n",
            "Epoch 484/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8609 - val_loss: 1.1150 - val_accuracy: 0.6578\n",
            "Epoch 485/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3470 - accuracy: 0.8656 - val_loss: 1.1132 - val_accuracy: 0.6631\n",
            "Epoch 486/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3396 - accuracy: 0.8644 - val_loss: 1.1851 - val_accuracy: 0.6631\n",
            "Epoch 487/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3598 - accuracy: 0.8532 - val_loss: 1.0902 - val_accuracy: 0.6738\n",
            "Epoch 488/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8543 - val_loss: 1.0492 - val_accuracy: 0.6524\n",
            "Epoch 489/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3478 - accuracy: 0.8656 - val_loss: 1.0795 - val_accuracy: 0.6738\n",
            "Epoch 490/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3437 - accuracy: 0.8627 - val_loss: 1.0887 - val_accuracy: 0.6631\n",
            "Epoch 491/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3496 - accuracy: 0.8591 - val_loss: 1.0718 - val_accuracy: 0.6684\n",
            "Epoch 492/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.8621 - val_loss: 1.2103 - val_accuracy: 0.6631\n",
            "Epoch 493/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8621 - val_loss: 1.1462 - val_accuracy: 0.6578\n",
            "Epoch 494/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8573 - val_loss: 1.1214 - val_accuracy: 0.6631\n",
            "Epoch 495/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.8627 - val_loss: 1.0865 - val_accuracy: 0.6684\n",
            "Epoch 496/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3277 - accuracy: 0.8668 - val_loss: 1.1313 - val_accuracy: 0.6471\n",
            "Epoch 497/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3345 - accuracy: 0.8686 - val_loss: 1.1390 - val_accuracy: 0.6631\n",
            "Epoch 498/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 0.8543 - val_loss: 1.0802 - val_accuracy: 0.6738\n",
            "Epoch 499/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8520 - val_loss: 1.1365 - val_accuracy: 0.6845\n",
            "Epoch 500/500\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3479 - accuracy: 0.8609 - val_loss: 1.0914 - val_accuracy: 0.6791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxCntbXXOYpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74af2723-1bba-49c3-966d-236abfacd7dc"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYwpH13bIvzz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508bd921-3574-4655-9fcd-b3082c6c1898"
      },
      "source": [
        "#plot training accuracy\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO-4XeLAMXVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fbdddc-1c28-4cf9-9f11-5651e7eeecae"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjsmz2HZQo64"
      },
      "source": [
        "Model predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1xo_ePjGvoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e7b8a1-26b3-4133-d01e-1a9c13feb12c"
      },
      "source": [
        "#predict on testing dataset\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "K.set_value(model.optimizer.learning_rate, 0.0008)\n",
        "y_pred = model.predict(x_test_norm)\n",
        "y_pred\n",
        "#print('y_pred',y_pred)\n",
        "y_pred.shape, y_test.shape\n",
        "y_test = y_test.argmax(axis=-1)\n",
        "print('y_test', y_test)\n",
        "y_pred = y_pred.argmax(axis=-1)\n",
        "print('y_pred',y_pred)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test [2 2 2 2 1 2 3 3 3 3 4 2 2 3 2 2 3 3 2 2 2 3 3 2 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 2 2 1 2 1 1 1 2 3 3 2 2 4 3 3 2 2 3 2 1 2 2 1 3 2 1 1 1 1 2 1 0 1 2 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 2 1 3 4 3 3 2 3 3 4 3 3 3 2 2 2 2 2 2 3 3 3 2 3 2 1 1 2 2 1 1 1 1 2 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 2 1 2 2 2 3 3 4 3 3 3 3 4 3 2 4 3 3 2 2 2 1 2 2 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 3 1 1 2 2 3 3 2 3 3 3 2 2 3 3 1 2 2 3 2 2 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 2 2 1 2 1 1 1 1 1 1 1 2 2\n",
            " 2 1 2 2 2 2 3 3 3 3 2 1 2 2 1 2 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 2 3 2 2 2 2 2\n",
            " 2 3 3 3 3 1 2 2 3 2 1 1 0 1 1 1 1 1 1 0 1 0 1]\n",
            "y_pred [3 2 3 2 2 2 3 2 4 3 3 2 2 3 3 1 2 3 2 1 2 2 2 2 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 3 2 1 1 1 2 2 1 2 4 3 3 2 2 2 2 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 2\n",
            " 2 1 2 0 3 4 3 3 2 3 3 4 3 3 3 3 2 3 2 2 2 4 4 2 2 2 2 2 1 2 1 1 1 1 1 2 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1\n",
            " 1 2 1 2 2 3 3 3 4 4 3 3 4 3 3 3 3 3 3 2 2 3 2 2 2 1 1 1 1 1 1 2 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1\n",
            " 1 3 1 2 2 2 1 3 3 3 3 3 2 2 2 3 2 1 1 2 1 1 2 0 1 1 1 0 2 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 2 1 1 2 2 1 2 1 1 1 1 1 1 2 2 2\n",
            " 2 1 2 2 2 2 4 3 3 3 2 2 2 2 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 2 1 1 2 2 2 2 2 2 3\n",
            " 3 2 3 3 3 2 2 2 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAg-ssPAO4hN",
        "outputId": "06c0e38f-fda8-48c8-9a1e-b1036bba0ba0"
      },
      "source": [
        "#Compute Recall, precision, f1_score\n",
        "# from sklearn.metrics import precision_score , recall_score\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.32      0.37        76\n",
            "           1       0.75      0.82      0.78       245\n",
            "           2       0.65      0.67      0.66        84\n",
            "           3       0.67      0.60      0.63        55\n",
            "           4       0.36      0.57      0.44         7\n",
            "\n",
            "    accuracy                           0.68       467\n",
            "   macro avg       0.58      0.59      0.58       467\n",
            "weighted avg       0.67      0.68      0.67       467\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxaq8MZKPjPg",
        "outputId": "f00563da-92d2-4b22-c6ed-14fbdcb429da"
      },
      "source": [
        "#Compute confusion matrix using accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 24,  51,   1,   0,   0],\n",
              "       [ 29, 201,  15,   0,   0],\n",
              "       [  0,  15,  56,  13,   0],\n",
              "       [  0,   1,  14,  33,   7],\n",
              "       [  0,   0,   0,   3,   4]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb0VpRt_d32E",
        "outputId": "39bb70f3-0b3c-4596-b182-f255ac335220"
      },
      "source": [
        "#SMOTE Implementation \n",
        "#Link: https://machinelearningmastery.com/multi-class-imbalanced-classification/ \n",
        "\n",
        "# example of oversampling a multi-class classification dataset\n",
        "from pandas import read_csv\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# define the dataset location\n",
        "url = '/content/drive/Shareddrives/CS_229_project/Electricity_Consumption_data/dataset_final.csv'\n",
        "# load the csv file as a data frame\n",
        "df = read_csv(url, usecols=['AWND','DAPR','DASF','EVAP','MDPR','MDSF','MNPN','MXPN','PGTM','PRCP','PSUN','SN33','SN35','SNOW','SNWD','SX32','SX33','TAVG','TMAX','TMIN','TOBS','TSUN','WDF2','WDF5','WDFG','WDMV','WESD','WESF','WSF2','WSF5','WSFG','WSFI','CLASS_2'])\n",
        "df = df.fillna(0).dropna()\n",
        "data = df.values\n",
        "\n",
        "# split into input and output elements\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "# label encode the target variable\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "# transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)\n",
        "# summarize distribution\n",
        "counter = Counter(y)\n",
        "for k,v in counter.items():\n",
        "\tper = v / len(y) * 100\n",
        "\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
        "# plot the distribution\n",
        "pyplot.bar(counter.keys(), counter.values())\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class=3, n=1203 (20.000%)\n",
            "Class=2, n=1203 (20.000%)\n",
            "Class=1, n=1203 (20.000%)\n",
            "Class=4, n=1203 (20.000%)\n",
            "Class=0, n=1203 (20.000%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRD-2FPebJj2",
        "outputId": "178a005f-f97e-4256-d03d-8a858b6ba9e0"
      },
      "source": [
        "# summarize distribution\n",
        "counter = Counter(train_labels)\n",
        "for k,v in counter.items():\n",
        "\tper = v / len(y) * 100\n",
        "\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
        "# plot the distribution\n",
        "pyplot.bar(counter.keys(), counter.values())\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class=1, n=958 (15.927%)\n",
            "Class=4, n=32 (0.532%)\n",
            "Class=2, n=357 (5.935%)\n",
            "Class=0, n=319 (5.303%)\n",
            "Class=3, n=203 (3.375%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0okPL_7miuD",
        "outputId": "592a9b10-72a8-4772-f528-642ae76144e7"
      },
      "source": [
        "#Latest X & y values\n",
        "print('X',X.shape)\n",
        "print('y', y.shape)\n",
        "#print('X:',X)\n",
        "#print('y:',y)\n",
        "#type(X)\n",
        "type(y)\n",
        "n = X.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X (6015, 32)\n",
            "y (6015,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmBSXbkfzv8V",
        "outputId": "4bacbfbe-e393-40f2-eb3f-c3f29aba0da4"
      },
      "source": [
        "#Concetanate X & y\n",
        "y = np.reshape(y, (n,1))\n",
        "print('y',y)\n",
        "data = np.concatenate((X, y), axis=1)\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y [[3]\n",
            " [3]\n",
            " [2]\n",
            " ...\n",
            " [4]\n",
            " [4]\n",
            " [4]]\n",
            "[[ 6.16111111  0.          0.         ... 32.45       14.76666667\n",
            "   3.        ]\n",
            " [ 6.901       0.          0.         ... 21.05       13.96666667\n",
            "   3.        ]\n",
            " [ 6.96222222  0.          0.         ... 21.9        12.73333333\n",
            "   2.        ]\n",
            " ...\n",
            " [ 6.17738283  2.          0.         ... 23.41912743 10.32465455\n",
            "   4.        ]\n",
            " [ 5.24708586  0.47733955  0.         ... 28.27261144 16.98413769\n",
            "   4.        ]\n",
            " [ 5.64744085  0.          0.         ... 25.28731821 15.66266722\n",
            "   4.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma5Eh3Yl1inr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3654d192-3437-4ed2-b6f4-5da9062a2596"
      },
      "source": [
        "#Convert 'data' to pandas dataframe\n",
        "dataset_s = pd.DataFrame(data, columns=['AWND','DAPR','DASF','EVAP','MDPR','MDSF','MNPN','MXPN','PGTM','PRCP','PSUN','SN33','SN35','SNOW','SNWD','SX32','SX33','TAVG','TMAX','TMIN','TOBS','TSUN','WDF2','WDF5','WDFG','WDMV','WESD','WESF','WSF2','WSF5','WSFG','WSFI','CLASS_2'])\n",
        "dataset_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AWND</th>\n",
              "      <th>DAPR</th>\n",
              "      <th>DASF</th>\n",
              "      <th>EVAP</th>\n",
              "      <th>MDPR</th>\n",
              "      <th>MDSF</th>\n",
              "      <th>MNPN</th>\n",
              "      <th>MXPN</th>\n",
              "      <th>PGTM</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>PSUN</th>\n",
              "      <th>SN33</th>\n",
              "      <th>SN35</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>SX32</th>\n",
              "      <th>SX33</th>\n",
              "      <th>TAVG</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>TOBS</th>\n",
              "      <th>TSUN</th>\n",
              "      <th>WDF2</th>\n",
              "      <th>WDF5</th>\n",
              "      <th>WDFG</th>\n",
              "      <th>WDMV</th>\n",
              "      <th>WESD</th>\n",
              "      <th>WESF</th>\n",
              "      <th>WSF2</th>\n",
              "      <th>WSF5</th>\n",
              "      <th>WSFG</th>\n",
              "      <th>WSFI</th>\n",
              "      <th>CLASS_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.161111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.433077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.833333</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>1022.785714</td>\n",
              "      <td>0.017936</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>77.408228</td>\n",
              "      <td>92.219313</td>\n",
              "      <td>65.963816</td>\n",
              "      <td>74.140271</td>\n",
              "      <td>0.0</td>\n",
              "      <td>222.051282</td>\n",
              "      <td>212.105263</td>\n",
              "      <td>266.500000</td>\n",
              "      <td>63.016667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.770513</td>\n",
              "      <td>23.710526</td>\n",
              "      <td>32.450000</td>\n",
              "      <td>14.766667</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.901000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.399231</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.909091</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>1566.384615</td>\n",
              "      <td>0.022638</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>76.123028</td>\n",
              "      <td>90.745098</td>\n",
              "      <td>65.422512</td>\n",
              "      <td>73.821429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.610390</td>\n",
              "      <td>228.701299</td>\n",
              "      <td>261.500000</td>\n",
              "      <td>107.658333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.828571</td>\n",
              "      <td>23.131169</td>\n",
              "      <td>21.050000</td>\n",
              "      <td>13.966667</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.962222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.386923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.181818</td>\n",
              "      <td>96.454545</td>\n",
              "      <td>1355.571429</td>\n",
              "      <td>0.019913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>74.880126</td>\n",
              "      <td>89.543372</td>\n",
              "      <td>63.893617</td>\n",
              "      <td>73.205607</td>\n",
              "      <td>0.0</td>\n",
              "      <td>246.282051</td>\n",
              "      <td>245.131579</td>\n",
              "      <td>266.500000</td>\n",
              "      <td>91.963636</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.565385</td>\n",
              "      <td>21.165789</td>\n",
              "      <td>21.900000</td>\n",
              "      <td>12.733333</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.224198</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.385385</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.454545</td>\n",
              "      <td>99.363636</td>\n",
              "      <td>1531.800000</td>\n",
              "      <td>0.023578</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>73.870662</td>\n",
              "      <td>88.651888</td>\n",
              "      <td>63.059211</td>\n",
              "      <td>71.646512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>241.666667</td>\n",
              "      <td>233.376623</td>\n",
              "      <td>187.500000</td>\n",
              "      <td>82.354545</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.682051</td>\n",
              "      <td>22.812987</td>\n",
              "      <td>19.450000</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.927195</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.422308</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.272727</td>\n",
              "      <td>97.636364</td>\n",
              "      <td>1620.800000</td>\n",
              "      <td>0.020585</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>101.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>71.583596</td>\n",
              "      <td>86.363339</td>\n",
              "      <td>61.748768</td>\n",
              "      <td>71.204651</td>\n",
              "      <td>0.0</td>\n",
              "      <td>233.544304</td>\n",
              "      <td>231.139240</td>\n",
              "      <td>182.500000</td>\n",
              "      <td>121.475000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.098734</td>\n",
              "      <td>23.559494</td>\n",
              "      <td>24.500000</td>\n",
              "      <td>12.966667</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6010</th>\n",
              "      <td>5.789527</td>\n",
              "      <td>0.825699</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.363144</td>\n",
              "      <td>0.474777</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.653209</td>\n",
              "      <td>93.170646</td>\n",
              "      <td>1354.659224</td>\n",
              "      <td>0.003614</td>\n",
              "      <td>0.0</td>\n",
              "      <td>83.587150</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.587150</td>\n",
              "      <td>88.587150</td>\n",
              "      <td>76.320842</td>\n",
              "      <td>93.394428</td>\n",
              "      <td>63.372934</td>\n",
              "      <td>72.786065</td>\n",
              "      <td>0.0</td>\n",
              "      <td>223.607617</td>\n",
              "      <td>213.052225</td>\n",
              "      <td>240.000000</td>\n",
              "      <td>39.638671</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.301729</td>\n",
              "      <td>21.372142</td>\n",
              "      <td>22.702736</td>\n",
              "      <td>21.829772</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6011</th>\n",
              "      <td>6.859866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.417548</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.688144</td>\n",
              "      <td>97.505798</td>\n",
              "      <td>1570.102811</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>81.176547</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.938858</td>\n",
              "      <td>97.470359</td>\n",
              "      <td>91.823453</td>\n",
              "      <td>78.376976</td>\n",
              "      <td>94.616619</td>\n",
              "      <td>64.765051</td>\n",
              "      <td>75.492420</td>\n",
              "      <td>0.0</td>\n",
              "      <td>258.861459</td>\n",
              "      <td>243.892701</td>\n",
              "      <td>230.000000</td>\n",
              "      <td>39.749060</td>\n",
              "      <td>5.159325</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.289140</td>\n",
              "      <td>21.373759</td>\n",
              "      <td>13.477067</td>\n",
              "      <td>14.585504</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6012</th>\n",
              "      <td>6.177383</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.125576</td>\n",
              "      <td>92.467295</td>\n",
              "      <td>1414.538459</td>\n",
              "      <td>0.001904</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.964779</td>\n",
              "      <td>82.964779</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>89.929558</td>\n",
              "      <td>86.964779</td>\n",
              "      <td>76.974445</td>\n",
              "      <td>91.789553</td>\n",
              "      <td>64.593099</td>\n",
              "      <td>73.689989</td>\n",
              "      <td>0.0</td>\n",
              "      <td>251.643380</td>\n",
              "      <td>242.625233</td>\n",
              "      <td>210.352208</td>\n",
              "      <td>47.176809</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.709867</td>\n",
              "      <td>20.210882</td>\n",
              "      <td>23.419127</td>\n",
              "      <td>10.324655</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6013</th>\n",
              "      <td>5.247086</td>\n",
              "      <td>0.477340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.395570</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.769968</td>\n",
              "      <td>94.598405</td>\n",
              "      <td>1331.152677</td>\n",
              "      <td>0.002913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>86.363547</td>\n",
              "      <td>85.886208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.351885</td>\n",
              "      <td>96.159113</td>\n",
              "      <td>91.681774</td>\n",
              "      <td>80.545454</td>\n",
              "      <td>96.978473</td>\n",
              "      <td>66.036589</td>\n",
              "      <td>77.549635</td>\n",
              "      <td>0.0</td>\n",
              "      <td>249.615557</td>\n",
              "      <td>240.541722</td>\n",
              "      <td>230.000000</td>\n",
              "      <td>39.939673</td>\n",
              "      <td>0.911767</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.714821</td>\n",
              "      <td>19.052847</td>\n",
              "      <td>28.272611</td>\n",
              "      <td>16.984138</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6014</th>\n",
              "      <td>5.647441</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.374896</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.384288</td>\n",
              "      <td>96.298324</td>\n",
              "      <td>1569.148334</td>\n",
              "      <td>0.004851</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.126818</td>\n",
              "      <td>86.063409</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>95.360966</td>\n",
              "      <td>20.370855</td>\n",
              "      <td>82.175250</td>\n",
              "      <td>98.321965</td>\n",
              "      <td>68.188884</td>\n",
              "      <td>79.173147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>217.818714</td>\n",
              "      <td>225.202438</td>\n",
              "      <td>240.000000</td>\n",
              "      <td>39.726885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.670543</td>\n",
              "      <td>21.393130</td>\n",
              "      <td>25.287318</td>\n",
              "      <td>15.662667</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6015 rows Ã— 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          AWND      DAPR  DASF  ...       WSFG       WSFI  CLASS_2\n",
              "0     6.161111  0.000000   0.0  ...  32.450000  14.766667      3.0\n",
              "1     6.901000  0.000000   0.0  ...  21.050000  13.966667      3.0\n",
              "2     6.962222  0.000000   0.0  ...  21.900000  12.733333      2.0\n",
              "3     7.224198  0.000000   0.0  ...  19.450000   8.500000      2.0\n",
              "4     7.927195  2.000000   0.0  ...  24.500000  12.966667      2.0\n",
              "...        ...       ...   ...  ...        ...        ...      ...\n",
              "6010  5.789527  0.825699   0.0  ...  22.702736  21.829772      4.0\n",
              "6011  6.859866  0.000000   0.0  ...  13.477067  14.585504      4.0\n",
              "6012  6.177383  2.000000   0.0  ...  23.419127  10.324655      4.0\n",
              "6013  5.247086  0.477340   0.0  ...  28.272611  16.984138      4.0\n",
              "6014  5.647441  0.000000   0.0  ...  25.287318  15.662667      4.0\n",
              "\n",
              "[6015 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTYD43zn3Dl9",
        "outputId": "58029ebc-3d2a-4e6c-a865-105c45448b20"
      },
      "source": [
        "#split train & test dataset\n",
        "train_s = dataset_s.sample(frac=0.9) #random_state=0\n",
        "test_s = dataset_s.drop(train_s.index)\n",
        "print(train_s.shape, test_s.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5414, 33) (601, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx9uD8h93DoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f73c744-3f6a-417a-9ffa-2e1d4bcda7e9"
      },
      "source": [
        "#Create one-hot encoding for TRAIN dataset\n",
        "existing_vals = np.unique(train_s['CLASS_2'].values)\n",
        "mapping = {val: idx for idx, val in enumerate(existing_vals)}\n",
        "y_train_s = np.array([mapping[y] for y in train_s['CLASS_2'].values])\n",
        "y_train_s = np_utils.to_categorical(y_train_s)\n",
        "print(y_train_s.shape)\n",
        "y_train_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5414, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ETFAkIL3Dqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545ba7af-4505-4907-c344-645f1f5b2479"
      },
      "source": [
        "#Create one-hot encoding for TEST dataset\n",
        "existing_vals = np.unique(test_s['CLASS_2'].values)\n",
        "mapping = {val: idx for idx, val in enumerate(existing_vals)}\n",
        "y_test_s = np.array([mapping[x] for x in test_s['CLASS_2'].values])\n",
        "y_test_s = np_utils.to_categorical(y_test_s)\n",
        "\n",
        "print(y_test_s.shape)\n",
        "y_test_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(601, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg3oJMbD3Dss",
        "outputId": "0d357733-4ef2-4b18-ad42-526ca5727b02"
      },
      "source": [
        "#train data normalization\n",
        "train_features_s = train_s.copy()\n",
        "train_labels_s = train_features_s.pop('CLASS_2')\n",
        "train_labels_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5227    4.0\n",
              "380     3.0\n",
              "2172    1.0\n",
              "5695    4.0\n",
              "1690    0.0\n",
              "       ... \n",
              "2323    1.0\n",
              "3918    3.0\n",
              "2655    0.0\n",
              "1385    0.0\n",
              "5293    4.0\n",
              "Name: CLASS_2, Length: 5414, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJHEg_-x3Duy",
        "outputId": "a9893dfe-5154-48a9-f6e5-e5d6833327d9"
      },
      "source": [
        "#test data normalization\n",
        "test_features_s = test_s.copy()\n",
        "test_labels_s = test_features_s.pop('CLASS_2')\n",
        "test_labels_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2       2.0\n",
              "3       2.0\n",
              "6       2.0\n",
              "26      2.0\n",
              "53      2.0\n",
              "       ... \n",
              "5955    4.0\n",
              "5976    4.0\n",
              "5990    4.0\n",
              "5997    4.0\n",
              "6005    4.0\n",
              "Name: CLASS_2, Length: 601, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmG-rZHt3Dw3",
        "outputId": "4d123bca-6d1c-41b7-f2ba-d418e0161885"
      },
      "source": [
        "#Clean x_train_s & x_test_s\n",
        "x_train_clean_s = train_features_s.fillna(0).dropna()\n",
        "x_test_clean_s = test_features_s.fillna(0).dropna()\n",
        "print(x_train_clean_s)\n",
        "print(x_test_clean_s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          AWND      DAPR  DASF  ...       WSF5       WSFG       WSFI\n",
            "5227  6.088502  0.000000   0.0  ...  20.889814  34.755388  17.282868\n",
            "380   7.313210  0.000000   0.0  ...  22.316883  15.000000  14.933333\n",
            "2172  7.471688  6.000000   0.0  ...  22.907576  30.000000  19.000000\n",
            "5695  5.521240  1.804255   0.0  ...  20.220528  33.148082  11.347447\n",
            "1690  4.145065  8.000000   0.0  ...  15.800000  17.000000  21.450000\n",
            "...        ...       ...   ...  ...        ...        ...        ...\n",
            "2323  6.848947  0.000000   0.0  ...  24.871233   0.000000  34.350000\n",
            "3918  6.694095  0.000000   0.0  ...  21.884414   1.346782  17.453416\n",
            "2655  4.156169  3.959614   0.0  ...  16.330960  26.565847  13.650000\n",
            "1385  7.837848  2.500000   0.0  ...  23.749351  25.100000  24.800000\n",
            "5293  6.235022  5.702035   0.0  ...  21.240748  28.499104  10.891194\n",
            "\n",
            "[5414 rows x 32 columns]\n",
            "          AWND      DAPR  DASF  ...       WSF5       WSFG       WSFI\n",
            "2     6.962222  0.000000   0.0  ...  21.165789  21.900000  12.733333\n",
            "3     7.224198  0.000000   0.0  ...  22.812987  19.450000   8.500000\n",
            "6     7.796341  0.000000   0.0  ...  24.106329  21.500000  13.200000\n",
            "26    7.214000  3.500000   0.0  ...  23.248000  24.950000  10.466667\n",
            "53    6.225122  0.000000   0.0  ...  20.293421  21.450000  16.866667\n",
            "...        ...       ...   ...  ...        ...        ...        ...\n",
            "5955  6.171233  3.000000   0.0  ...  20.887499  26.642991  18.661735\n",
            "5976  6.105762  1.881069   0.0  ...  21.844598  25.093526  16.903764\n",
            "5990  6.168039  3.000000   0.0  ...  20.779500  25.825780  18.688032\n",
            "5997  6.772350  2.851305   0.0  ...  22.100391  32.059132  23.224175\n",
            "6005  4.448004  0.000000   0.0  ...  16.585161  27.934828  12.208743\n",
            "\n",
            "[601 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TjYfWIc3DzM",
        "outputId": "f56a7e2a-9a30-41f0-d486-07f018ace5ca"
      },
      "source": [
        "#normalize x_train_clean \n",
        "normalizer = tf.keras.layers.Normalization()\n",
        "normalizer.adapt(x_train_clean_s)\n",
        "x_train_norm_s = normalizer(x_train_clean_s)\n",
        "x_train_norm_s.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5414, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5MEuPld3D0w",
        "outputId": "de2d4b5e-b82e-4391-cb3d-b2a700485135"
      },
      "source": [
        "#normalize x_test_clean\n",
        "normalizer = tf.keras.layers.Normalization()\n",
        "normalizer.adapt(x_test_clean_s)\n",
        "x_test_norm_s = normalizer(x_test_clean_s)\n",
        "x_test_norm_s.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([601, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2O19wwN8UGR"
      },
      "source": [
        "#Import CLASS_2es\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "#Model Definition\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(32,)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtCSm8ti8UPH"
      },
      "source": [
        "#Configure model training\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36_dfk0s8USO",
        "outputId": "f272e346-76c9-4db2-976d-5ecdf95213da"
      },
      "source": [
        "#train model\n",
        "from keras import backend as K\n",
        "K.set_value(model.optimizer.learning_rate, 0.0009)\n",
        "history = model.fit(x_train_norm_s, y_train_s, validation_split=0.2, epochs=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 1.2741 - accuracy: 0.3974 - val_loss: 0.9447 - val_accuracy: 0.5522\n",
            "Epoch 2/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.9537 - accuracy: 0.5507 - val_loss: 0.7240 - val_accuracy: 0.6999\n",
            "Epoch 3/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.8020 - accuracy: 0.6253 - val_loss: 0.6435 - val_accuracy: 0.7018\n",
            "Epoch 4/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.7406 - accuracy: 0.6523 - val_loss: 0.5923 - val_accuracy: 0.7202\n",
            "Epoch 5/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.7086 - accuracy: 0.6749 - val_loss: 0.5818 - val_accuracy: 0.7452\n",
            "Epoch 6/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.6682 - accuracy: 0.6989 - val_loss: 0.5315 - val_accuracy: 0.7608\n",
            "Epoch 7/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.6217 - accuracy: 0.7250 - val_loss: 0.5187 - val_accuracy: 0.7599\n",
            "Epoch 8/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.6133 - accuracy: 0.7250 - val_loss: 0.5150 - val_accuracy: 0.7562\n",
            "Epoch 9/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5864 - accuracy: 0.7370 - val_loss: 0.5068 - val_accuracy: 0.7599\n",
            "Epoch 10/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5792 - accuracy: 0.7449 - val_loss: 0.5166 - val_accuracy: 0.7544\n",
            "Epoch 11/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5764 - accuracy: 0.7446 - val_loss: 0.5058 - val_accuracy: 0.7590\n",
            "Epoch 12/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7548 - val_loss: 0.4831 - val_accuracy: 0.7673\n",
            "Epoch 13/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7663 - val_loss: 0.4793 - val_accuracy: 0.7719\n",
            "Epoch 14/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5453 - accuracy: 0.7643 - val_loss: 0.4916 - val_accuracy: 0.7793\n",
            "Epoch 15/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5231 - accuracy: 0.7735 - val_loss: 0.4871 - val_accuracy: 0.7701\n",
            "Epoch 16/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5205 - accuracy: 0.7756 - val_loss: 0.4576 - val_accuracy: 0.7876\n",
            "Epoch 17/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5103 - accuracy: 0.7813 - val_loss: 0.4495 - val_accuracy: 0.7886\n",
            "Epoch 18/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5105 - accuracy: 0.7749 - val_loss: 0.4516 - val_accuracy: 0.7886\n",
            "Epoch 19/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7832 - val_loss: 0.4472 - val_accuracy: 0.7904\n",
            "Epoch 20/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4838 - accuracy: 0.7940 - val_loss: 0.4448 - val_accuracy: 0.7978\n",
            "Epoch 21/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4977 - accuracy: 0.7899 - val_loss: 0.4579 - val_accuracy: 0.7978\n",
            "Epoch 22/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4854 - accuracy: 0.7938 - val_loss: 0.4317 - val_accuracy: 0.8042\n",
            "Epoch 23/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4752 - accuracy: 0.7931 - val_loss: 0.4180 - val_accuracy: 0.8135\n",
            "Epoch 24/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.8012 - val_loss: 0.4214 - val_accuracy: 0.8070\n",
            "Epoch 25/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.8035 - val_loss: 0.4100 - val_accuracy: 0.8172\n",
            "Epoch 26/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.8093 - val_loss: 0.4216 - val_accuracy: 0.8116\n",
            "Epoch 27/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.8033 - val_loss: 0.4065 - val_accuracy: 0.8163\n",
            "Epoch 28/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4405 - accuracy: 0.8118 - val_loss: 0.4364 - val_accuracy: 0.8070\n",
            "Epoch 29/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4488 - accuracy: 0.8164 - val_loss: 0.4273 - val_accuracy: 0.8006\n",
            "Epoch 30/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.8139 - val_loss: 0.4076 - val_accuracy: 0.8089\n",
            "Epoch 31/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4164 - accuracy: 0.8280 - val_loss: 0.4425 - val_accuracy: 0.8061\n",
            "Epoch 32/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4413 - accuracy: 0.8187 - val_loss: 0.4140 - val_accuracy: 0.8283\n",
            "Epoch 33/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4406 - accuracy: 0.8079 - val_loss: 0.3975 - val_accuracy: 0.8264\n",
            "Epoch 34/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4286 - accuracy: 0.8164 - val_loss: 0.3971 - val_accuracy: 0.8163\n",
            "Epoch 35/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4129 - accuracy: 0.8248 - val_loss: 0.4115 - val_accuracy: 0.8061\n",
            "Epoch 36/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4183 - accuracy: 0.8211 - val_loss: 0.4015 - val_accuracy: 0.8209\n",
            "Epoch 37/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4110 - accuracy: 0.8254 - val_loss: 0.4131 - val_accuracy: 0.8070\n",
            "Epoch 38/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4189 - accuracy: 0.8160 - val_loss: 0.3875 - val_accuracy: 0.8236\n",
            "Epoch 39/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8271 - val_loss: 0.3837 - val_accuracy: 0.8199\n",
            "Epoch 40/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4050 - accuracy: 0.8310 - val_loss: 0.3956 - val_accuracy: 0.8135\n",
            "Epoch 41/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4128 - accuracy: 0.8224 - val_loss: 0.3977 - val_accuracy: 0.8227\n",
            "Epoch 42/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4140 - accuracy: 0.8268 - val_loss: 0.4067 - val_accuracy: 0.8126\n",
            "Epoch 43/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4089 - accuracy: 0.8287 - val_loss: 0.3866 - val_accuracy: 0.8264\n",
            "Epoch 44/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8370 - val_loss: 0.3906 - val_accuracy: 0.8236\n",
            "Epoch 45/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.4042 - accuracy: 0.8296 - val_loss: 0.3923 - val_accuracy: 0.8199\n",
            "Epoch 46/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3997 - accuracy: 0.8271 - val_loss: 0.3943 - val_accuracy: 0.8255\n",
            "Epoch 47/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8365 - val_loss: 0.3914 - val_accuracy: 0.8163\n",
            "Epoch 48/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.8370 - val_loss: 0.3728 - val_accuracy: 0.8246\n",
            "Epoch 49/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8377 - val_loss: 0.3885 - val_accuracy: 0.8209\n",
            "Epoch 50/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8358 - val_loss: 0.3828 - val_accuracy: 0.8319\n",
            "Epoch 51/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3896 - accuracy: 0.8372 - val_loss: 0.3909 - val_accuracy: 0.8218\n",
            "Epoch 52/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3847 - accuracy: 0.8351 - val_loss: 0.3728 - val_accuracy: 0.8246\n",
            "Epoch 53/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 0.8384 - val_loss: 0.3741 - val_accuracy: 0.8338\n",
            "Epoch 54/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8469 - val_loss: 0.3639 - val_accuracy: 0.8356\n",
            "Epoch 55/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8407 - val_loss: 0.3715 - val_accuracy: 0.8292\n",
            "Epoch 56/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8386 - val_loss: 0.3795 - val_accuracy: 0.8384\n",
            "Epoch 57/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8502 - val_loss: 0.3643 - val_accuracy: 0.8273\n",
            "Epoch 58/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3636 - accuracy: 0.8490 - val_loss: 0.3684 - val_accuracy: 0.8421\n",
            "Epoch 59/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3598 - accuracy: 0.8471 - val_loss: 0.3680 - val_accuracy: 0.8338\n",
            "Epoch 60/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3651 - accuracy: 0.8467 - val_loss: 0.3706 - val_accuracy: 0.8319\n",
            "Epoch 61/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3625 - accuracy: 0.8497 - val_loss: 0.3819 - val_accuracy: 0.8329\n",
            "Epoch 62/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3600 - accuracy: 0.8490 - val_loss: 0.3967 - val_accuracy: 0.8209\n",
            "Epoch 63/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.8520 - val_loss: 0.3772 - val_accuracy: 0.8264\n",
            "Epoch 64/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8508 - val_loss: 0.3950 - val_accuracy: 0.8264\n",
            "Epoch 65/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.8483 - val_loss: 0.3909 - val_accuracy: 0.8283\n",
            "Epoch 66/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3795 - accuracy: 0.8439 - val_loss: 0.3793 - val_accuracy: 0.8273\n",
            "Epoch 67/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3555 - accuracy: 0.8497 - val_loss: 0.3791 - val_accuracy: 0.8412\n",
            "Epoch 68/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3586 - accuracy: 0.8515 - val_loss: 0.3726 - val_accuracy: 0.8430\n",
            "Epoch 69/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8527 - val_loss: 0.3836 - val_accuracy: 0.8329\n",
            "Epoch 70/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3514 - accuracy: 0.8515 - val_loss: 0.3655 - val_accuracy: 0.8292\n",
            "Epoch 71/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8534 - val_loss: 0.3639 - val_accuracy: 0.8366\n",
            "Epoch 72/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.8562 - val_loss: 0.3759 - val_accuracy: 0.8384\n",
            "Epoch 73/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8589 - val_loss: 0.3795 - val_accuracy: 0.8301\n",
            "Epoch 74/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3515 - accuracy: 0.8541 - val_loss: 0.3871 - val_accuracy: 0.8255\n",
            "Epoch 75/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.8619 - val_loss: 0.3459 - val_accuracy: 0.8393\n",
            "Epoch 76/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3427 - accuracy: 0.8562 - val_loss: 0.3545 - val_accuracy: 0.8440\n",
            "Epoch 77/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3374 - accuracy: 0.8624 - val_loss: 0.3464 - val_accuracy: 0.8393\n",
            "Epoch 78/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8659 - val_loss: 0.3864 - val_accuracy: 0.8356\n",
            "Epoch 79/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8592 - val_loss: 0.3612 - val_accuracy: 0.8366\n",
            "Epoch 80/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3316 - accuracy: 0.8672 - val_loss: 0.3612 - val_accuracy: 0.8440\n",
            "Epoch 81/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8649 - val_loss: 0.3674 - val_accuracy: 0.8375\n",
            "Epoch 82/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3404 - accuracy: 0.8605 - val_loss: 0.3500 - val_accuracy: 0.8523\n",
            "Epoch 83/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3306 - accuracy: 0.8645 - val_loss: 0.3628 - val_accuracy: 0.8467\n",
            "Epoch 84/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8626 - val_loss: 0.3609 - val_accuracy: 0.8476\n",
            "Epoch 85/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3146 - accuracy: 0.8686 - val_loss: 0.3556 - val_accuracy: 0.8532\n",
            "Epoch 86/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8573 - val_loss: 0.3624 - val_accuracy: 0.8430\n",
            "Epoch 87/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3275 - accuracy: 0.8645 - val_loss: 0.3683 - val_accuracy: 0.8440\n",
            "Epoch 88/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8670 - val_loss: 0.3605 - val_accuracy: 0.8356\n",
            "Epoch 89/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8638 - val_loss: 0.3610 - val_accuracy: 0.8412\n",
            "Epoch 90/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3121 - accuracy: 0.8672 - val_loss: 0.3595 - val_accuracy: 0.8356\n",
            "Epoch 91/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8659 - val_loss: 0.3521 - val_accuracy: 0.8366\n",
            "Epoch 92/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8700 - val_loss: 0.3647 - val_accuracy: 0.8338\n",
            "Epoch 93/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8619 - val_loss: 0.3395 - val_accuracy: 0.8449\n",
            "Epoch 94/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3159 - accuracy: 0.8702 - val_loss: 0.3530 - val_accuracy: 0.8375\n",
            "Epoch 95/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.8675 - val_loss: 0.3664 - val_accuracy: 0.8467\n",
            "Epoch 96/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3047 - accuracy: 0.8649 - val_loss: 0.3440 - val_accuracy: 0.8541\n",
            "Epoch 97/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8705 - val_loss: 0.3745 - val_accuracy: 0.8513\n",
            "Epoch 98/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8684 - val_loss: 0.3583 - val_accuracy: 0.8412\n",
            "Epoch 99/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.8695 - val_loss: 0.3664 - val_accuracy: 0.8458\n",
            "Epoch 100/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3022 - accuracy: 0.8725 - val_loss: 0.3627 - val_accuracy: 0.8449\n",
            "Epoch 101/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8596 - val_loss: 0.3698 - val_accuracy: 0.8440\n",
            "Epoch 102/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3045 - accuracy: 0.8663 - val_loss: 0.3455 - val_accuracy: 0.8486\n",
            "Epoch 103/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8762 - val_loss: 0.3475 - val_accuracy: 0.8523\n",
            "Epoch 104/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8689 - val_loss: 0.3582 - val_accuracy: 0.8569\n",
            "Epoch 105/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3168 - accuracy: 0.8698 - val_loss: 0.3664 - val_accuracy: 0.8366\n",
            "Epoch 106/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8737 - val_loss: 0.3295 - val_accuracy: 0.8513\n",
            "Epoch 107/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3009 - accuracy: 0.8721 - val_loss: 0.3616 - val_accuracy: 0.8523\n",
            "Epoch 108/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8698 - val_loss: 0.3521 - val_accuracy: 0.8486\n",
            "Epoch 109/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8659 - val_loss: 0.3825 - val_accuracy: 0.8356\n",
            "Epoch 110/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3004 - accuracy: 0.8758 - val_loss: 0.3562 - val_accuracy: 0.8430\n",
            "Epoch 111/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2994 - accuracy: 0.8749 - val_loss: 0.3744 - val_accuracy: 0.8384\n",
            "Epoch 112/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2935 - accuracy: 0.8781 - val_loss: 0.3637 - val_accuracy: 0.8421\n",
            "Epoch 113/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8779 - val_loss: 0.3477 - val_accuracy: 0.8449\n",
            "Epoch 114/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8762 - val_loss: 0.3773 - val_accuracy: 0.8440\n",
            "Epoch 115/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8809 - val_loss: 0.3772 - val_accuracy: 0.8393\n",
            "Epoch 116/500\n",
            "136/136 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8836 - val_loss: 0.3863 - val_accuracy: 0.8375\n",
            "Epoch 117/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.8811 - val_loss: 0.3850 - val_accuracy: 0.8458\n",
            "Epoch 118/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.8788 - val_loss: 0.3786 - val_accuracy: 0.8440\n",
            "Epoch 119/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3057 - accuracy: 0.8804 - val_loss: 0.3534 - val_accuracy: 0.8458\n",
            "Epoch 120/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8799 - val_loss: 0.3682 - val_accuracy: 0.8467\n",
            "Epoch 121/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2930 - accuracy: 0.8811 - val_loss: 0.3511 - val_accuracy: 0.8476\n",
            "Epoch 122/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2709 - accuracy: 0.8848 - val_loss: 0.3495 - val_accuracy: 0.8513\n",
            "Epoch 123/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3000 - accuracy: 0.8841 - val_loss: 0.3754 - val_accuracy: 0.8403\n",
            "Epoch 124/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8792 - val_loss: 0.3850 - val_accuracy: 0.8393\n",
            "Epoch 125/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.8813 - val_loss: 0.3540 - val_accuracy: 0.8476\n",
            "Epoch 126/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8871 - val_loss: 0.3496 - val_accuracy: 0.8476\n",
            "Epoch 127/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.8899 - val_loss: 0.3731 - val_accuracy: 0.8495\n",
            "Epoch 128/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8873 - val_loss: 0.3390 - val_accuracy: 0.8513\n",
            "Epoch 129/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.8882 - val_loss: 0.3706 - val_accuracy: 0.8366\n",
            "Epoch 130/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8871 - val_loss: 0.3697 - val_accuracy: 0.8430\n",
            "Epoch 131/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2739 - accuracy: 0.8929 - val_loss: 0.3812 - val_accuracy: 0.8513\n",
            "Epoch 132/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8841 - val_loss: 0.3721 - val_accuracy: 0.8393\n",
            "Epoch 133/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2810 - accuracy: 0.8871 - val_loss: 0.3749 - val_accuracy: 0.8440\n",
            "Epoch 134/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.8834 - val_loss: 0.3480 - val_accuracy: 0.8560\n",
            "Epoch 135/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8908 - val_loss: 0.3852 - val_accuracy: 0.8476\n",
            "Epoch 136/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.8903 - val_loss: 0.3716 - val_accuracy: 0.8476\n",
            "Epoch 137/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8792 - val_loss: 0.3576 - val_accuracy: 0.8513\n",
            "Epoch 138/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.8862 - val_loss: 0.3712 - val_accuracy: 0.8430\n",
            "Epoch 139/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.8862 - val_loss: 0.3530 - val_accuracy: 0.8633\n",
            "Epoch 140/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8896 - val_loss: 0.3307 - val_accuracy: 0.8587\n",
            "Epoch 141/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8906 - val_loss: 0.3515 - val_accuracy: 0.8532\n",
            "Epoch 142/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.8894 - val_loss: 0.3581 - val_accuracy: 0.8467\n",
            "Epoch 143/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2652 - accuracy: 0.8933 - val_loss: 0.3585 - val_accuracy: 0.8476\n",
            "Epoch 144/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.8852 - val_loss: 0.3461 - val_accuracy: 0.8560\n",
            "Epoch 145/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8940 - val_loss: 0.3691 - val_accuracy: 0.8532\n",
            "Epoch 146/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8882 - val_loss: 0.3631 - val_accuracy: 0.8532\n",
            "Epoch 147/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8901 - val_loss: 0.3540 - val_accuracy: 0.8633\n",
            "Epoch 148/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2822 - accuracy: 0.8885 - val_loss: 0.3369 - val_accuracy: 0.8560\n",
            "Epoch 149/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2705 - accuracy: 0.8901 - val_loss: 0.3551 - val_accuracy: 0.8615\n",
            "Epoch 150/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8876 - val_loss: 0.3719 - val_accuracy: 0.8550\n",
            "Epoch 151/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2537 - accuracy: 0.8975 - val_loss: 0.3508 - val_accuracy: 0.8560\n",
            "Epoch 152/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8903 - val_loss: 0.3528 - val_accuracy: 0.8523\n",
            "Epoch 153/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.8940 - val_loss: 0.3633 - val_accuracy: 0.8596\n",
            "Epoch 154/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8813 - val_loss: 0.3403 - val_accuracy: 0.8633\n",
            "Epoch 155/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8871 - val_loss: 0.3521 - val_accuracy: 0.8569\n",
            "Epoch 156/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2667 - accuracy: 0.8829 - val_loss: 0.3759 - val_accuracy: 0.8495\n",
            "Epoch 157/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.8899 - val_loss: 0.3522 - val_accuracy: 0.8587\n",
            "Epoch 158/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2583 - accuracy: 0.8892 - val_loss: 0.3557 - val_accuracy: 0.8569\n",
            "Epoch 159/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8956 - val_loss: 0.3591 - val_accuracy: 0.8560\n",
            "Epoch 160/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.8963 - val_loss: 0.3584 - val_accuracy: 0.8652\n",
            "Epoch 161/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2532 - accuracy: 0.8936 - val_loss: 0.3819 - val_accuracy: 0.8587\n",
            "Epoch 162/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.8878 - val_loss: 0.3499 - val_accuracy: 0.8624\n",
            "Epoch 163/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.8929 - val_loss: 0.3482 - val_accuracy: 0.8661\n",
            "Epoch 164/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2674 - accuracy: 0.8947 - val_loss: 0.3533 - val_accuracy: 0.8596\n",
            "Epoch 165/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2729 - accuracy: 0.8873 - val_loss: 0.3220 - val_accuracy: 0.8587\n",
            "Epoch 166/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.8977 - val_loss: 0.3471 - val_accuracy: 0.8587\n",
            "Epoch 167/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.8880 - val_loss: 0.3507 - val_accuracy: 0.8596\n",
            "Epoch 168/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.9000 - val_loss: 0.3647 - val_accuracy: 0.8661\n",
            "Epoch 169/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.8986 - val_loss: 0.3524 - val_accuracy: 0.8587\n",
            "Epoch 170/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.8924 - val_loss: 0.3315 - val_accuracy: 0.8541\n",
            "Epoch 171/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.8991 - val_loss: 0.3586 - val_accuracy: 0.8615\n",
            "Epoch 172/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2678 - accuracy: 0.8975 - val_loss: 0.3501 - val_accuracy: 0.8560\n",
            "Epoch 173/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.8984 - val_loss: 0.3753 - val_accuracy: 0.8569\n",
            "Epoch 174/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.8984 - val_loss: 0.3569 - val_accuracy: 0.8569\n",
            "Epoch 175/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.8977 - val_loss: 0.3463 - val_accuracy: 0.8606\n",
            "Epoch 176/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2491 - accuracy: 0.8989 - val_loss: 0.3486 - val_accuracy: 0.8670\n",
            "Epoch 177/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2577 - accuracy: 0.8986 - val_loss: 0.3431 - val_accuracy: 0.8513\n",
            "Epoch 178/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8989 - val_loss: 0.3632 - val_accuracy: 0.8532\n",
            "Epoch 179/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8924 - val_loss: 0.3749 - val_accuracy: 0.8532\n",
            "Epoch 180/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8993 - val_loss: 0.3578 - val_accuracy: 0.8550\n",
            "Epoch 181/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.9049 - val_loss: 0.3741 - val_accuracy: 0.8578\n",
            "Epoch 182/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2522 - accuracy: 0.8986 - val_loss: 0.3419 - val_accuracy: 0.8550\n",
            "Epoch 183/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.9033 - val_loss: 0.3492 - val_accuracy: 0.8606\n",
            "Epoch 184/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.8989 - val_loss: 0.3624 - val_accuracy: 0.8560\n",
            "Epoch 185/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2425 - accuracy: 0.9044 - val_loss: 0.3453 - val_accuracy: 0.8532\n",
            "Epoch 186/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.9023 - val_loss: 0.3719 - val_accuracy: 0.8560\n",
            "Epoch 187/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2507 - accuracy: 0.8989 - val_loss: 0.3419 - val_accuracy: 0.8578\n",
            "Epoch 188/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8993 - val_loss: 0.3674 - val_accuracy: 0.8578\n",
            "Epoch 189/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2449 - accuracy: 0.9012 - val_loss: 0.3596 - val_accuracy: 0.8587\n",
            "Epoch 190/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2428 - accuracy: 0.8982 - val_loss: 0.3841 - val_accuracy: 0.8440\n",
            "Epoch 191/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.9005 - val_loss: 0.3591 - val_accuracy: 0.8532\n",
            "Epoch 192/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2483 - accuracy: 0.8963 - val_loss: 0.3568 - val_accuracy: 0.8541\n",
            "Epoch 193/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2393 - accuracy: 0.9026 - val_loss: 0.3781 - val_accuracy: 0.8578\n",
            "Epoch 194/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.8968 - val_loss: 0.3684 - val_accuracy: 0.8606\n",
            "Epoch 195/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.8986 - val_loss: 0.3515 - val_accuracy: 0.8578\n",
            "Epoch 196/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.9016 - val_loss: 0.3655 - val_accuracy: 0.8587\n",
            "Epoch 197/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.8991 - val_loss: 0.3497 - val_accuracy: 0.8633\n",
            "Epoch 198/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2484 - accuracy: 0.9026 - val_loss: 0.3336 - val_accuracy: 0.8643\n",
            "Epoch 199/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.9009 - val_loss: 0.3634 - val_accuracy: 0.8633\n",
            "Epoch 200/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2377 - accuracy: 0.9063 - val_loss: 0.3495 - val_accuracy: 0.8643\n",
            "Epoch 201/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2432 - accuracy: 0.9028 - val_loss: 0.3439 - val_accuracy: 0.8615\n",
            "Epoch 202/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2274 - accuracy: 0.9067 - val_loss: 0.3396 - val_accuracy: 0.8652\n",
            "Epoch 203/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.8966 - val_loss: 0.3766 - val_accuracy: 0.8495\n",
            "Epoch 204/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.9026 - val_loss: 0.3397 - val_accuracy: 0.8587\n",
            "Epoch 205/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.9009 - val_loss: 0.3474 - val_accuracy: 0.8587\n",
            "Epoch 206/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8993 - val_loss: 0.3496 - val_accuracy: 0.8596\n",
            "Epoch 207/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.9030 - val_loss: 0.3438 - val_accuracy: 0.8633\n",
            "Epoch 208/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.8993 - val_loss: 0.3677 - val_accuracy: 0.8569\n",
            "Epoch 209/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2380 - accuracy: 0.9051 - val_loss: 0.3509 - val_accuracy: 0.8652\n",
            "Epoch 210/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.9005 - val_loss: 0.3634 - val_accuracy: 0.8587\n",
            "Epoch 211/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2358 - accuracy: 0.9076 - val_loss: 0.3773 - val_accuracy: 0.8606\n",
            "Epoch 212/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.9035 - val_loss: 0.3588 - val_accuracy: 0.8717\n",
            "Epoch 213/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9093 - val_loss: 0.3940 - val_accuracy: 0.8615\n",
            "Epoch 214/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2332 - accuracy: 0.9069 - val_loss: 0.3479 - val_accuracy: 0.8587\n",
            "Epoch 215/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.9072 - val_loss: 0.3393 - val_accuracy: 0.8680\n",
            "Epoch 216/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.9012 - val_loss: 0.3657 - val_accuracy: 0.8569\n",
            "Epoch 217/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8963 - val_loss: 0.3625 - val_accuracy: 0.8689\n",
            "Epoch 218/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2398 - accuracy: 0.9069 - val_loss: 0.3417 - val_accuracy: 0.8661\n",
            "Epoch 219/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.9058 - val_loss: 0.3737 - val_accuracy: 0.8643\n",
            "Epoch 220/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2266 - accuracy: 0.9104 - val_loss: 0.3637 - val_accuracy: 0.8652\n",
            "Epoch 221/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2349 - accuracy: 0.9102 - val_loss: 0.3370 - val_accuracy: 0.8643\n",
            "Epoch 222/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9104 - val_loss: 0.3903 - val_accuracy: 0.8624\n",
            "Epoch 223/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.9063 - val_loss: 0.3905 - val_accuracy: 0.8560\n",
            "Epoch 224/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.9039 - val_loss: 0.3820 - val_accuracy: 0.8624\n",
            "Epoch 225/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2285 - accuracy: 0.9051 - val_loss: 0.3772 - val_accuracy: 0.8615\n",
            "Epoch 226/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.9060 - val_loss: 0.3769 - val_accuracy: 0.8596\n",
            "Epoch 227/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.9093 - val_loss: 0.3824 - val_accuracy: 0.8596\n",
            "Epoch 228/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.9042 - val_loss: 0.3816 - val_accuracy: 0.8652\n",
            "Epoch 229/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.9046 - val_loss: 0.3454 - val_accuracy: 0.8596\n",
            "Epoch 230/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.9081 - val_loss: 0.3690 - val_accuracy: 0.8569\n",
            "Epoch 231/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2192 - accuracy: 0.9106 - val_loss: 0.3523 - val_accuracy: 0.8661\n",
            "Epoch 232/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.9065 - val_loss: 0.3664 - val_accuracy: 0.8596\n",
            "Epoch 233/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.8998 - val_loss: 0.3710 - val_accuracy: 0.8513\n",
            "Epoch 234/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2330 - accuracy: 0.9058 - val_loss: 0.3570 - val_accuracy: 0.8652\n",
            "Epoch 235/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.9058 - val_loss: 0.3266 - val_accuracy: 0.8689\n",
            "Epoch 236/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.9067 - val_loss: 0.3512 - val_accuracy: 0.8541\n",
            "Epoch 237/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.9093 - val_loss: 0.3358 - val_accuracy: 0.8633\n",
            "Epoch 238/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9090 - val_loss: 0.3499 - val_accuracy: 0.8717\n",
            "Epoch 239/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9127 - val_loss: 0.3720 - val_accuracy: 0.8670\n",
            "Epoch 240/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.9102 - val_loss: 0.3402 - val_accuracy: 0.8606\n",
            "Epoch 241/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.9012 - val_loss: 0.3435 - val_accuracy: 0.8707\n",
            "Epoch 242/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.9076 - val_loss: 0.3881 - val_accuracy: 0.8680\n",
            "Epoch 243/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.9100 - val_loss: 0.3685 - val_accuracy: 0.8624\n",
            "Epoch 244/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9042 - val_loss: 0.3421 - val_accuracy: 0.8698\n",
            "Epoch 245/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2332 - accuracy: 0.9120 - val_loss: 0.3589 - val_accuracy: 0.8661\n",
            "Epoch 246/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2216 - accuracy: 0.9139 - val_loss: 0.3582 - val_accuracy: 0.8680\n",
            "Epoch 247/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.9095 - val_loss: 0.3749 - val_accuracy: 0.8643\n",
            "Epoch 248/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2261 - accuracy: 0.9079 - val_loss: 0.3681 - val_accuracy: 0.8652\n",
            "Epoch 249/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2197 - accuracy: 0.9102 - val_loss: 0.3346 - val_accuracy: 0.8772\n",
            "Epoch 250/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.9118 - val_loss: 0.3365 - val_accuracy: 0.8726\n",
            "Epoch 251/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.9023 - val_loss: 0.3789 - val_accuracy: 0.8652\n",
            "Epoch 252/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2329 - accuracy: 0.9030 - val_loss: 0.3881 - val_accuracy: 0.8652\n",
            "Epoch 253/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9106 - val_loss: 0.3620 - val_accuracy: 0.8643\n",
            "Epoch 254/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9153 - val_loss: 0.3482 - val_accuracy: 0.8670\n",
            "Epoch 255/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.9127 - val_loss: 0.3719 - val_accuracy: 0.8587\n",
            "Epoch 256/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2227 - accuracy: 0.9141 - val_loss: 0.3825 - val_accuracy: 0.8633\n",
            "Epoch 257/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9118 - val_loss: 0.3896 - val_accuracy: 0.8615\n",
            "Epoch 258/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9150 - val_loss: 0.3955 - val_accuracy: 0.8652\n",
            "Epoch 259/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9143 - val_loss: 0.3641 - val_accuracy: 0.8763\n",
            "Epoch 260/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9178 - val_loss: 0.3857 - val_accuracy: 0.8744\n",
            "Epoch 261/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9118 - val_loss: 0.3533 - val_accuracy: 0.8633\n",
            "Epoch 262/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.9063 - val_loss: 0.3397 - val_accuracy: 0.8717\n",
            "Epoch 263/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9148 - val_loss: 0.3639 - val_accuracy: 0.8680\n",
            "Epoch 264/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2137 - accuracy: 0.9162 - val_loss: 0.3411 - val_accuracy: 0.8606\n",
            "Epoch 265/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9166 - val_loss: 0.3820 - val_accuracy: 0.8587\n",
            "Epoch 266/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 0.9118 - val_loss: 0.3739 - val_accuracy: 0.8707\n",
            "Epoch 267/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9173 - val_loss: 0.3627 - val_accuracy: 0.8735\n",
            "Epoch 268/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.9127 - val_loss: 0.3755 - val_accuracy: 0.8726\n",
            "Epoch 269/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9164 - val_loss: 0.3624 - val_accuracy: 0.8726\n",
            "Epoch 270/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9146 - val_loss: 0.3927 - val_accuracy: 0.8587\n",
            "Epoch 271/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2285 - accuracy: 0.9120 - val_loss: 0.3463 - val_accuracy: 0.8652\n",
            "Epoch 272/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.2177 - accuracy: 0.9141 - val_loss: 0.3553 - val_accuracy: 0.8606\n",
            "Epoch 273/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9236 - val_loss: 0.3842 - val_accuracy: 0.8560\n",
            "Epoch 274/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2134 - accuracy: 0.9178 - val_loss: 0.3673 - val_accuracy: 0.8578\n",
            "Epoch 275/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2105 - accuracy: 0.9116 - val_loss: 0.3822 - val_accuracy: 0.8652\n",
            "Epoch 276/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.9125 - val_loss: 0.3513 - val_accuracy: 0.8615\n",
            "Epoch 277/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.2109 - accuracy: 0.9160 - val_loss: 0.3842 - val_accuracy: 0.8670\n",
            "Epoch 278/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9176 - val_loss: 0.3419 - val_accuracy: 0.8661\n",
            "Epoch 279/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9141 - val_loss: 0.3573 - val_accuracy: 0.8689\n",
            "Epoch 280/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.9130 - val_loss: 0.3807 - val_accuracy: 0.8652\n",
            "Epoch 281/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.9153 - val_loss: 0.3830 - val_accuracy: 0.8624\n",
            "Epoch 282/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.9166 - val_loss: 0.3933 - val_accuracy: 0.8430\n",
            "Epoch 283/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.9166 - val_loss: 0.3403 - val_accuracy: 0.8698\n",
            "Epoch 284/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9194 - val_loss: 0.3547 - val_accuracy: 0.8753\n",
            "Epoch 285/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9192 - val_loss: 0.3750 - val_accuracy: 0.8680\n",
            "Epoch 286/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9199 - val_loss: 0.3930 - val_accuracy: 0.8707\n",
            "Epoch 287/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9155 - val_loss: 0.3713 - val_accuracy: 0.8661\n",
            "Epoch 288/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 0.9139 - val_loss: 0.3452 - val_accuracy: 0.8652\n",
            "Epoch 289/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1973 - accuracy: 0.9238 - val_loss: 0.3797 - val_accuracy: 0.8698\n",
            "Epoch 290/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2193 - accuracy: 0.9111 - val_loss: 0.3333 - val_accuracy: 0.8689\n",
            "Epoch 291/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9166 - val_loss: 0.3603 - val_accuracy: 0.8717\n",
            "Epoch 292/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2197 - accuracy: 0.9171 - val_loss: 0.3544 - val_accuracy: 0.8680\n",
            "Epoch 293/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9220 - val_loss: 0.3570 - val_accuracy: 0.8744\n",
            "Epoch 294/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2004 - accuracy: 0.9196 - val_loss: 0.3371 - val_accuracy: 0.8772\n",
            "Epoch 295/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9203 - val_loss: 0.3468 - val_accuracy: 0.8781\n",
            "Epoch 296/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1941 - accuracy: 0.9180 - val_loss: 0.3500 - val_accuracy: 0.8818\n",
            "Epoch 297/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9238 - val_loss: 0.3957 - val_accuracy: 0.8523\n",
            "Epoch 298/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.2204 - accuracy: 0.9118 - val_loss: 0.3889 - val_accuracy: 0.8698\n",
            "Epoch 299/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9155 - val_loss: 0.3810 - val_accuracy: 0.8652\n",
            "Epoch 300/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2144 - accuracy: 0.9199 - val_loss: 0.3804 - val_accuracy: 0.8643\n",
            "Epoch 301/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.9141 - val_loss: 0.3934 - val_accuracy: 0.8643\n",
            "Epoch 302/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9240 - val_loss: 0.3916 - val_accuracy: 0.8606\n",
            "Epoch 303/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2020 - accuracy: 0.9215 - val_loss: 0.4343 - val_accuracy: 0.8578\n",
            "Epoch 304/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9097 - val_loss: 0.3691 - val_accuracy: 0.8707\n",
            "Epoch 305/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1951 - accuracy: 0.9233 - val_loss: 0.3849 - val_accuracy: 0.8633\n",
            "Epoch 306/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9194 - val_loss: 0.3964 - val_accuracy: 0.8661\n",
            "Epoch 307/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.9215 - val_loss: 0.3845 - val_accuracy: 0.8735\n",
            "Epoch 308/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9196 - val_loss: 0.3937 - val_accuracy: 0.8689\n",
            "Epoch 309/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9192 - val_loss: 0.3857 - val_accuracy: 0.8763\n",
            "Epoch 310/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1968 - accuracy: 0.9178 - val_loss: 0.3871 - val_accuracy: 0.8744\n",
            "Epoch 311/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2002 - accuracy: 0.9206 - val_loss: 0.3862 - val_accuracy: 0.8717\n",
            "Epoch 312/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9173 - val_loss: 0.3807 - val_accuracy: 0.8652\n",
            "Epoch 313/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1941 - accuracy: 0.9252 - val_loss: 0.3851 - val_accuracy: 0.8698\n",
            "Epoch 314/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9268 - val_loss: 0.3749 - val_accuracy: 0.8707\n",
            "Epoch 315/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9169 - val_loss: 0.4124 - val_accuracy: 0.8689\n",
            "Epoch 316/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9254 - val_loss: 0.3991 - val_accuracy: 0.8661\n",
            "Epoch 317/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9280 - val_loss: 0.3833 - val_accuracy: 0.8615\n",
            "Epoch 318/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9222 - val_loss: 0.3339 - val_accuracy: 0.8790\n",
            "Epoch 319/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9160 - val_loss: 0.3748 - val_accuracy: 0.8744\n",
            "Epoch 320/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9208 - val_loss: 0.3842 - val_accuracy: 0.8689\n",
            "Epoch 321/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1950 - accuracy: 0.9183 - val_loss: 0.3771 - val_accuracy: 0.8753\n",
            "Epoch 322/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9224 - val_loss: 0.4231 - val_accuracy: 0.8689\n",
            "Epoch 323/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9220 - val_loss: 0.3626 - val_accuracy: 0.8800\n",
            "Epoch 324/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1985 - accuracy: 0.9238 - val_loss: 0.3657 - val_accuracy: 0.8707\n",
            "Epoch 325/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9263 - val_loss: 0.3849 - val_accuracy: 0.8763\n",
            "Epoch 326/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1992 - accuracy: 0.9196 - val_loss: 0.3515 - val_accuracy: 0.8763\n",
            "Epoch 327/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9323 - val_loss: 0.3573 - val_accuracy: 0.8781\n",
            "Epoch 328/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9224 - val_loss: 0.3812 - val_accuracy: 0.8643\n",
            "Epoch 329/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1958 - accuracy: 0.9199 - val_loss: 0.3744 - val_accuracy: 0.8698\n",
            "Epoch 330/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.9231 - val_loss: 0.3323 - val_accuracy: 0.8707\n",
            "Epoch 331/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.9268 - val_loss: 0.3549 - val_accuracy: 0.8744\n",
            "Epoch 332/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9243 - val_loss: 0.3652 - val_accuracy: 0.8670\n",
            "Epoch 333/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1945 - accuracy: 0.9252 - val_loss: 0.3812 - val_accuracy: 0.8753\n",
            "Epoch 334/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9282 - val_loss: 0.3923 - val_accuracy: 0.8744\n",
            "Epoch 335/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.9247 - val_loss: 0.3871 - val_accuracy: 0.8670\n",
            "Epoch 336/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1931 - accuracy: 0.9284 - val_loss: 0.3905 - val_accuracy: 0.8735\n",
            "Epoch 337/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1890 - accuracy: 0.9266 - val_loss: 0.3874 - val_accuracy: 0.8763\n",
            "Epoch 338/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1883 - accuracy: 0.9284 - val_loss: 0.3644 - val_accuracy: 0.8726\n",
            "Epoch 339/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9173 - val_loss: 0.3708 - val_accuracy: 0.8735\n",
            "Epoch 340/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1966 - accuracy: 0.9236 - val_loss: 0.3618 - val_accuracy: 0.8790\n",
            "Epoch 341/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9296 - val_loss: 0.3996 - val_accuracy: 0.8800\n",
            "Epoch 342/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1980 - accuracy: 0.9210 - val_loss: 0.3824 - val_accuracy: 0.8763\n",
            "Epoch 343/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9222 - val_loss: 0.3431 - val_accuracy: 0.8800\n",
            "Epoch 344/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9261 - val_loss: 0.3553 - val_accuracy: 0.8753\n",
            "Epoch 345/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1948 - accuracy: 0.9252 - val_loss: 0.3811 - val_accuracy: 0.8753\n",
            "Epoch 346/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.2029 - accuracy: 0.9227 - val_loss: 0.3741 - val_accuracy: 0.8726\n",
            "Epoch 347/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9298 - val_loss: 0.3675 - val_accuracy: 0.8652\n",
            "Epoch 348/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1896 - accuracy: 0.9284 - val_loss: 0.3904 - val_accuracy: 0.8735\n",
            "Epoch 349/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2137 - accuracy: 0.9220 - val_loss: 0.3638 - val_accuracy: 0.8753\n",
            "Epoch 350/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.9282 - val_loss: 0.3906 - val_accuracy: 0.8680\n",
            "Epoch 351/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9233 - val_loss: 0.3985 - val_accuracy: 0.8809\n",
            "Epoch 352/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9247 - val_loss: 0.4123 - val_accuracy: 0.8772\n",
            "Epoch 353/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9266 - val_loss: 0.3801 - val_accuracy: 0.8781\n",
            "Epoch 354/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9236 - val_loss: 0.3759 - val_accuracy: 0.8809\n",
            "Epoch 355/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2018 - accuracy: 0.9254 - val_loss: 0.3942 - val_accuracy: 0.8772\n",
            "Epoch 356/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9217 - val_loss: 0.3718 - val_accuracy: 0.8735\n",
            "Epoch 357/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9270 - val_loss: 0.3710 - val_accuracy: 0.8800\n",
            "Epoch 358/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1872 - accuracy: 0.9273 - val_loss: 0.3642 - val_accuracy: 0.8827\n",
            "Epoch 359/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1931 - accuracy: 0.9305 - val_loss: 0.3789 - val_accuracy: 0.8873\n",
            "Epoch 360/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1740 - accuracy: 0.9330 - val_loss: 0.3792 - val_accuracy: 0.8846\n",
            "Epoch 361/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1802 - accuracy: 0.9312 - val_loss: 0.3898 - val_accuracy: 0.8753\n",
            "Epoch 362/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1903 - accuracy: 0.9261 - val_loss: 0.3746 - val_accuracy: 0.8707\n",
            "Epoch 363/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2003 - accuracy: 0.9190 - val_loss: 0.3770 - val_accuracy: 0.8680\n",
            "Epoch 364/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9238 - val_loss: 0.3533 - val_accuracy: 0.8790\n",
            "Epoch 365/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9277 - val_loss: 0.3638 - val_accuracy: 0.8855\n",
            "Epoch 366/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9210 - val_loss: 0.3931 - val_accuracy: 0.8790\n",
            "Epoch 367/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9305 - val_loss: 0.3697 - val_accuracy: 0.8837\n",
            "Epoch 368/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.9291 - val_loss: 0.3705 - val_accuracy: 0.8763\n",
            "Epoch 369/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9203 - val_loss: 0.3127 - val_accuracy: 0.8873\n",
            "Epoch 370/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9240 - val_loss: 0.3846 - val_accuracy: 0.8837\n",
            "Epoch 371/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1996 - accuracy: 0.9215 - val_loss: 0.3904 - val_accuracy: 0.8717\n",
            "Epoch 372/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9287 - val_loss: 0.4029 - val_accuracy: 0.8744\n",
            "Epoch 373/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.9289 - val_loss: 0.4086 - val_accuracy: 0.8772\n",
            "Epoch 374/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.9317 - val_loss: 0.4313 - val_accuracy: 0.8744\n",
            "Epoch 375/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1931 - accuracy: 0.9287 - val_loss: 0.4060 - val_accuracy: 0.8615\n",
            "Epoch 376/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2284 - accuracy: 0.9132 - val_loss: 0.3482 - val_accuracy: 0.8772\n",
            "Epoch 377/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.2039 - accuracy: 0.9231 - val_loss: 0.4091 - val_accuracy: 0.8707\n",
            "Epoch 378/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1838 - accuracy: 0.9321 - val_loss: 0.3754 - val_accuracy: 0.8772\n",
            "Epoch 379/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1950 - accuracy: 0.9240 - val_loss: 0.3497 - val_accuracy: 0.8818\n",
            "Epoch 380/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9261 - val_loss: 0.3933 - val_accuracy: 0.8781\n",
            "Epoch 381/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1904 - accuracy: 0.9270 - val_loss: 0.4092 - val_accuracy: 0.8707\n",
            "Epoch 382/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 0.9185 - val_loss: 0.3739 - val_accuracy: 0.8772\n",
            "Epoch 383/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.9282 - val_loss: 0.4150 - val_accuracy: 0.8837\n",
            "Epoch 384/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1904 - accuracy: 0.9289 - val_loss: 0.4010 - val_accuracy: 0.8790\n",
            "Epoch 385/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9353 - val_loss: 0.4304 - val_accuracy: 0.8726\n",
            "Epoch 386/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9270 - val_loss: 0.4423 - val_accuracy: 0.8717\n",
            "Epoch 387/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9275 - val_loss: 0.3868 - val_accuracy: 0.8726\n",
            "Epoch 388/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.9360 - val_loss: 0.3513 - val_accuracy: 0.8763\n",
            "Epoch 389/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.9307 - val_loss: 0.3935 - val_accuracy: 0.8689\n",
            "Epoch 390/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1785 - accuracy: 0.9340 - val_loss: 0.3931 - val_accuracy: 0.8726\n",
            "Epoch 391/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.9317 - val_loss: 0.3759 - val_accuracy: 0.8744\n",
            "Epoch 392/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.9312 - val_loss: 0.3556 - val_accuracy: 0.8818\n",
            "Epoch 393/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9280 - val_loss: 0.3393 - val_accuracy: 0.8790\n",
            "Epoch 394/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9254 - val_loss: 0.4013 - val_accuracy: 0.8744\n",
            "Epoch 395/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9259 - val_loss: 0.4089 - val_accuracy: 0.8790\n",
            "Epoch 396/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1897 - accuracy: 0.9261 - val_loss: 0.3912 - val_accuracy: 0.8790\n",
            "Epoch 397/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9296 - val_loss: 0.3745 - val_accuracy: 0.8790\n",
            "Epoch 398/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9351 - val_loss: 0.3885 - val_accuracy: 0.8790\n",
            "Epoch 399/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9307 - val_loss: 0.4050 - val_accuracy: 0.8744\n",
            "Epoch 400/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9328 - val_loss: 0.4262 - val_accuracy: 0.8744\n",
            "Epoch 401/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1663 - accuracy: 0.9333 - val_loss: 0.4115 - val_accuracy: 0.8763\n",
            "Epoch 402/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9323 - val_loss: 0.4056 - val_accuracy: 0.8818\n",
            "Epoch 403/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1785 - accuracy: 0.9317 - val_loss: 0.4039 - val_accuracy: 0.8717\n",
            "Epoch 404/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.9305 - val_loss: 0.4120 - val_accuracy: 0.8680\n",
            "Epoch 405/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.9270 - val_loss: 0.3972 - val_accuracy: 0.8707\n",
            "Epoch 406/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.9270 - val_loss: 0.3936 - val_accuracy: 0.8763\n",
            "Epoch 407/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.9259 - val_loss: 0.3213 - val_accuracy: 0.8800\n",
            "Epoch 408/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9317 - val_loss: 0.3698 - val_accuracy: 0.8800\n",
            "Epoch 409/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.9310 - val_loss: 0.3801 - val_accuracy: 0.8818\n",
            "Epoch 410/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.9312 - val_loss: 0.3666 - val_accuracy: 0.8744\n",
            "Epoch 411/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1717 - accuracy: 0.9347 - val_loss: 0.3795 - val_accuracy: 0.8809\n",
            "Epoch 412/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9287 - val_loss: 0.3828 - val_accuracy: 0.8837\n",
            "Epoch 413/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9326 - val_loss: 0.3693 - val_accuracy: 0.8864\n",
            "Epoch 414/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1921 - accuracy: 0.9270 - val_loss: 0.3664 - val_accuracy: 0.8827\n",
            "Epoch 415/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9395 - val_loss: 0.4100 - val_accuracy: 0.8864\n",
            "Epoch 416/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.9328 - val_loss: 0.3787 - val_accuracy: 0.8800\n",
            "Epoch 417/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.9213 - val_loss: 0.3627 - val_accuracy: 0.8818\n",
            "Epoch 418/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9326 - val_loss: 0.4177 - val_accuracy: 0.8698\n",
            "Epoch 419/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9312 - val_loss: 0.3652 - val_accuracy: 0.8735\n",
            "Epoch 420/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.9300 - val_loss: 0.3757 - val_accuracy: 0.8818\n",
            "Epoch 421/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9289 - val_loss: 0.3864 - val_accuracy: 0.8753\n",
            "Epoch 422/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.9289 - val_loss: 0.3745 - val_accuracy: 0.8800\n",
            "Epoch 423/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9300 - val_loss: 0.4202 - val_accuracy: 0.8753\n",
            "Epoch 424/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9305 - val_loss: 0.3668 - val_accuracy: 0.8772\n",
            "Epoch 425/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9381 - val_loss: 0.3666 - val_accuracy: 0.8744\n",
            "Epoch 426/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.9270 - val_loss: 0.3766 - val_accuracy: 0.8717\n",
            "Epoch 427/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1618 - accuracy: 0.9330 - val_loss: 0.3987 - val_accuracy: 0.8744\n",
            "Epoch 428/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1868 - accuracy: 0.9275 - val_loss: 0.3755 - val_accuracy: 0.8800\n",
            "Epoch 429/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9307 - val_loss: 0.3950 - val_accuracy: 0.8753\n",
            "Epoch 430/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9344 - val_loss: 0.3710 - val_accuracy: 0.8772\n",
            "Epoch 431/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1686 - accuracy: 0.9390 - val_loss: 0.3605 - val_accuracy: 0.8753\n",
            "Epoch 432/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1689 - accuracy: 0.9335 - val_loss: 0.3954 - val_accuracy: 0.8753\n",
            "Epoch 433/500\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.1837 - accuracy: 0.9289 - val_loss: 0.3568 - val_accuracy: 0.8855\n",
            "Epoch 434/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9240 - val_loss: 0.3549 - val_accuracy: 0.8818\n",
            "Epoch 435/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1734 - accuracy: 0.9305 - val_loss: 0.3590 - val_accuracy: 0.8790\n",
            "Epoch 436/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9289 - val_loss: 0.3882 - val_accuracy: 0.8790\n",
            "Epoch 437/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9388 - val_loss: 0.4227 - val_accuracy: 0.8800\n",
            "Epoch 438/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1758 - accuracy: 0.9321 - val_loss: 0.4055 - val_accuracy: 0.8717\n",
            "Epoch 439/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9337 - val_loss: 0.3698 - val_accuracy: 0.8837\n",
            "Epoch 440/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1792 - accuracy: 0.9379 - val_loss: 0.3509 - val_accuracy: 0.8901\n",
            "Epoch 441/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1882 - accuracy: 0.9263 - val_loss: 0.4017 - val_accuracy: 0.8818\n",
            "Epoch 442/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.9333 - val_loss: 0.3925 - val_accuracy: 0.8800\n",
            "Epoch 443/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.9296 - val_loss: 0.3621 - val_accuracy: 0.8800\n",
            "Epoch 444/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1773 - accuracy: 0.9312 - val_loss: 0.3641 - val_accuracy: 0.8781\n",
            "Epoch 445/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9335 - val_loss: 0.3662 - val_accuracy: 0.8800\n",
            "Epoch 446/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1746 - accuracy: 0.9353 - val_loss: 0.3681 - val_accuracy: 0.8818\n",
            "Epoch 447/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1748 - accuracy: 0.9342 - val_loss: 0.3625 - val_accuracy: 0.8717\n",
            "Epoch 448/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9333 - val_loss: 0.3924 - val_accuracy: 0.8735\n",
            "Epoch 449/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9363 - val_loss: 0.3638 - val_accuracy: 0.8800\n",
            "Epoch 450/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1657 - accuracy: 0.9379 - val_loss: 0.3673 - val_accuracy: 0.8790\n",
            "Epoch 451/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9303 - val_loss: 0.3398 - val_accuracy: 0.8790\n",
            "Epoch 452/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1502 - accuracy: 0.9388 - val_loss: 0.4073 - val_accuracy: 0.8800\n",
            "Epoch 453/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9374 - val_loss: 0.3627 - val_accuracy: 0.8818\n",
            "Epoch 454/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9420 - val_loss: 0.3787 - val_accuracy: 0.8735\n",
            "Epoch 455/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1732 - accuracy: 0.9326 - val_loss: 0.3688 - val_accuracy: 0.8726\n",
            "Epoch 456/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9367 - val_loss: 0.3993 - val_accuracy: 0.8753\n",
            "Epoch 457/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1730 - accuracy: 0.9340 - val_loss: 0.3795 - val_accuracy: 0.8753\n",
            "Epoch 458/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1830 - accuracy: 0.9323 - val_loss: 0.3908 - val_accuracy: 0.8772\n",
            "Epoch 459/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1572 - accuracy: 0.9386 - val_loss: 0.4033 - val_accuracy: 0.8735\n",
            "Epoch 460/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.9333 - val_loss: 0.4143 - val_accuracy: 0.8781\n",
            "Epoch 461/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9353 - val_loss: 0.3870 - val_accuracy: 0.8772\n",
            "Epoch 462/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.9402 - val_loss: 0.4290 - val_accuracy: 0.8680\n",
            "Epoch 463/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1878 - accuracy: 0.9312 - val_loss: 0.4068 - val_accuracy: 0.8763\n",
            "Epoch 464/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9363 - val_loss: 0.3937 - val_accuracy: 0.8717\n",
            "Epoch 465/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9379 - val_loss: 0.3904 - val_accuracy: 0.8763\n",
            "Epoch 466/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1591 - accuracy: 0.9363 - val_loss: 0.3925 - val_accuracy: 0.8707\n",
            "Epoch 467/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1657 - accuracy: 0.9328 - val_loss: 0.4134 - val_accuracy: 0.8689\n",
            "Epoch 468/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1971 - accuracy: 0.9344 - val_loss: 0.3653 - val_accuracy: 0.8717\n",
            "Epoch 469/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1877 - accuracy: 0.9314 - val_loss: 0.3728 - val_accuracy: 0.8661\n",
            "Epoch 470/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1774 - accuracy: 0.9342 - val_loss: 0.3709 - val_accuracy: 0.8661\n",
            "Epoch 471/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1668 - accuracy: 0.9400 - val_loss: 0.3898 - val_accuracy: 0.8753\n",
            "Epoch 472/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9344 - val_loss: 0.3956 - val_accuracy: 0.8827\n",
            "Epoch 473/500\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.1781 - accuracy: 0.9335 - val_loss: 0.3745 - val_accuracy: 0.8790\n",
            "Epoch 474/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 0.9372 - val_loss: 0.3790 - val_accuracy: 0.8763\n",
            "Epoch 475/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9367 - val_loss: 0.3335 - val_accuracy: 0.8846\n",
            "Epoch 476/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1559 - accuracy: 0.9407 - val_loss: 0.4167 - val_accuracy: 0.8809\n",
            "Epoch 477/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.9342 - val_loss: 0.3977 - val_accuracy: 0.8744\n",
            "Epoch 478/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.9363 - val_loss: 0.3586 - val_accuracy: 0.8818\n",
            "Epoch 479/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9395 - val_loss: 0.4242 - val_accuracy: 0.8680\n",
            "Epoch 480/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1585 - accuracy: 0.9390 - val_loss: 0.4302 - val_accuracy: 0.8698\n",
            "Epoch 481/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9400 - val_loss: 0.3994 - val_accuracy: 0.8753\n",
            "Epoch 482/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.9340 - val_loss: 0.3845 - val_accuracy: 0.8781\n",
            "Epoch 483/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9397 - val_loss: 0.3676 - val_accuracy: 0.8800\n",
            "Epoch 484/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9360 - val_loss: 0.3643 - val_accuracy: 0.8781\n",
            "Epoch 485/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.9342 - val_loss: 0.3703 - val_accuracy: 0.8873\n",
            "Epoch 486/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9395 - val_loss: 0.3896 - val_accuracy: 0.8772\n",
            "Epoch 487/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1534 - accuracy: 0.9393 - val_loss: 0.3654 - val_accuracy: 0.8827\n",
            "Epoch 488/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1675 - accuracy: 0.9377 - val_loss: 0.3514 - val_accuracy: 0.8753\n",
            "Epoch 489/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.9363 - val_loss: 0.3841 - val_accuracy: 0.8837\n",
            "Epoch 490/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9349 - val_loss: 0.3658 - val_accuracy: 0.8809\n",
            "Epoch 491/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1781 - accuracy: 0.9317 - val_loss: 0.3128 - val_accuracy: 0.8744\n",
            "Epoch 492/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.9381 - val_loss: 0.3748 - val_accuracy: 0.8726\n",
            "Epoch 493/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1754 - accuracy: 0.9342 - val_loss: 0.3723 - val_accuracy: 0.8818\n",
            "Epoch 494/500\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.1659 - accuracy: 0.9365 - val_loss: 0.4264 - val_accuracy: 0.8790\n",
            "Epoch 495/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1552 - accuracy: 0.9372 - val_loss: 0.4329 - val_accuracy: 0.8781\n",
            "Epoch 496/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1572 - accuracy: 0.9439 - val_loss: 0.4128 - val_accuracy: 0.8763\n",
            "Epoch 497/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1767 - accuracy: 0.9291 - val_loss: 0.4052 - val_accuracy: 0.8744\n",
            "Epoch 498/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9381 - val_loss: 0.4020 - val_accuracy: 0.8744\n",
            "Epoch 499/500\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.9344 - val_loss: 0.3991 - val_accuracy: 0.8763\n",
            "Epoch 500/500\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.1667 - accuracy: 0.9312 - val_loss: 0.3976 - val_accuracy: 0.8772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mzz_B518edS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff6f2ea-651c-424b-f185-3ddeb5506d5f"
      },
      "source": [
        "#plot training accuracy\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hUVfrA8e+bnhBIp4YSmvReRRFUFFHELioqrorr6trdRddVf+qu3bXXFTtiV1YRBGkWelF6FUhCSQgkJCF9zu+PcyczkwwQlCGQeT/Pkycz996ZOXcI5z3lveeKMQallFLBK6S2C6CUUqp2aSBQSqkgp4FAKaWCnAYCpZQKchoIlFIqyGkgUEqpIKeBQAUVEXlbRB6p4bFbROT0QJdJqdqmgUAppYKcBgKljkMiElbbZVB1hwYCdcxxhmTuFpFfRaRQRN4UkUYi8q2I5IvIDBFJ8Dr+XBFZJSK5IjJbRDp67espIkud130ERFX5rHNEZLnz2p9FpFsNy3i2iCwTkX0iki4iD1bZf5LzfrnO/rHO9mgReVpEtopInoj86GwbIiIZfr6H053HD4rIpyLyvojsA8aKSD8Rmed8xg4ReVFEIrxe31lEpovIHhHZJSL3ikhjEdkvIklex/USkWwRCa/Juau6RwOBOlZdCAwD2gMjgW+Be4EU7N/tLQAi0h74ELjN2TcF+J+IRDiV4pfAe0Ai8Inzvjiv7QlMAG4AkoDXgMkiElmD8hUCVwHxwNnAjSJynvO+LZ3yvuCUqQew3HndU0Bv4ESnTH8DXDX8TkYBnzqf+QFQAdwOJAMDgdOAvzhlqA/MAKYCTYG2wPfGmJ3AbOASr/e9EphkjCmrYTlUHaOBQB2rXjDG7DLGZAI/AAuMMcuMMcXAF0BP57hLgW+MMdOdiuwpIBpb0Q4AwoFnjTFlxphPgUVenzEOeM0Ys8AYU2GMeQcocV53UMaY2caYFcYYlzHmV2wwOsXZfTkwwxjzofO5OcaY5SISAvwJuNUYk+l85s/GmJIafifzjDFfOp9ZZIxZYoyZb4wpN8ZswQYydxnOAXYaY542xhQbY/KNMQucfe8AYwBEJBS4DBssVZDSQKCOVbu8Hhf5eR7rPG4KbHXvMMa4gHSgmbMv0/iurLjV63FL4E5naCVXRHKB5s7rDkpE+ovILGdIJQ/4M7ZljvMem/y8LBk7NOVvX02kVylDexH5WkR2OsNF/65BGQC+AjqJSBq215VnjFn4O8uk6gANBOp4tx1boQMgIoKtBDOBHUAzZ5tbC6/H6cC/jDHxXj8xxpgPa/C5E4HJQHNjTBzwKuD+nHSgjZ/X7AaKD7CvEIjxOo9Q7LCSt6pLBb8CrAXaGWMaYIfOvMvQ2l/BnV7Vx9hewZVobyDoaSBQx7uPgbNF5DRnsvNO7PDOz8A8oBy4RUTCReQCoJ/Xa98A/uy07kVE6jmTwPVr8Ln1gT3GmGIR6YcdDnL7ADhdRC4RkTARSRKRHk5vZQLwjIg0FZFQERnozEmsB6Kczw8H7gMONVdRH9gHFIhIB+BGr31fA01E5DYRiRSR+iLS32v/u8BY4Fw0EAQ9DQTquGaMWYdt2b6AbXGPBEYaY0qNMaXABdgKbw92PuFzr9cuBq4HXgT2AhudY2viL8BDIpIP3I8NSO733QaMwAalPdiJ4u7O7ruAFdi5ij3A40CIMSbPec//YnszhYBPFpEfd2EDUD42qH3kVYZ87LDPSGAnsAEY6rX/J+wk9VJjjPdwmQpCojemUSo4ichMYKIx5r+1XRZVuzQQKBWERKQvMB07x5Ff2+VRtUuHhpQKMiLyDvYag9s0CCjQHoFSSgU97REopVSQO+4WrkpOTjatWrWq7WIopdRxZcmSJbuNMVWvTQGOw0DQqlUrFi9eXNvFUEqp44qIHDBNWIeGlFIqyGkgUEqpIKeBQCmlgtxxN0fgT1lZGRkZGRQXF9d2UQIqKiqK1NRUwsP1/iFKqSOnTgSCjIwM6tevT6tWrfBdaLLuMMaQk5NDRkYGaWlptV0cpVQdUieGhoqLi0lKSqqzQQBAREhKSqrzvR6l1NFXJwIBUKeDgFswnKNS6uirM4FAKaXqmoy9+/lu1c6Af44GgiMgNzeXl19++bBfN2LECHJzcwNQIqVUIOXtL+PqCQvZmOW7Zt/ewtJqx7pcBpfLrum2PbeI9D37ATvv595+IMOf/YFx7y2hvMJ1hErunwaCI+BAgaC8vPygr5syZQrx8fGBKpZSQe26dxbxwYLqF9OuzMyrrIz3FZdx66RlZOzdX7nv5dkbydpXzLac/dVem7u/lIW/7aH7Q98xZ302b/64BZfL8PrcTbzz8xZ6PjydnzftZtm2vZWvGfXST1w1YSGl5S5OfGwmJz8xi7FvLaTDP6fS+t4pnPzETJ7+bh2Dn5jFhwu3ATBvUw67C0ooKLF1yAcLth0yaPwRdSJrqLaNHz+eTZs20aNHD8LDw4mKiiIhIYG1a9eyfv16zjvvPNLT0ykuLubWW29l3LhxgGe5jIKCAs466yxOOukkfv75Z5o1a8ZXX31FdHR0LZ+ZUkeXy2X4fm0W7RrGMmlROned0Z6w0BCf/cvS99K7ZaLf1xtjyN1fRmR4CDPWZDFjTRZX9G/ps/+cF34kRGDzo2fz8aJ0vlq+ndAQ4ZlLejD2rUXsLijhianrANjy2NkAzFqXRXK9SEa++KPP5324cBv7isr4ZsWOym2Xv7EAgAdHduKc7k1ZkZkHQPv7vq08Zva67MrH6XuKeGHmRgDu+XwFq7bn8f78bT6f88DkVcTHhDOqR7MafpOHp84Fgv/73ypWb993RN+zU9MGPDCy8wH3P/bYY6xcuZLly5cze/Zszj77bFauXFmZ5jlhwgQSExMpKiqib9++XHjhhSQlJfm8x4YNG/jwww954403uOSSS/jss88YM2bMET0PpfzZW1hKXlEZrZLrHZH325xdwJz12Vw1sBWhIf4THCpchtAQoaS8gj2FpTSJs42ejxenM/7zFZXHvTpnEx9eP4CBbez/l/fmb+WByasY0DqRjk0a8KdBaTRPjKl8z9fmbuKJqeu4rF+LyveYtTaLni3iiY+JYHuezbpzGZj8y3a+XJ4JwJKtezHGUFBS5lPOJ6etpXlCjE+ZqvIOAt4e/N9qHvzfap9tDetHkpVfQnJsBG9e3ZdRL/1Uue+xC7ryS4ZvEOjaLK4ykLw4cyMjuzUl5ADf6R9R5wLBsaBfv34+uf7PP/88X3zxBQDp6els2LChWiBIS0ujR48eAPTu3ZstW7YctfKq4DbyxR/J2FtU2fr1Z2VmHm0bxhIVHkpxWQXzNudwSruUapVSabmLK99cSGZuEfnF5dxyWjuKyyp4dsYGLuqdStuGsewpLOW8l35iZPcmZOwt4qvl21n78HCiwkNZnl59zuyyN+bzxEXduKRPc+auty3p+Zv3MH/zHj6Yv43bh7VnWKeGjHjuR0qdsXT3EAvANW8vomlcFP+6oCtfLM2s3H7Lh8sA6NkinmXbcvnPjA0Ul/mOxb80axMAJ7ZJIjk2knqRoXy4MB2AO4a155np64kOD6W4vIIRXZpUBoUv/nIi57/8MwCNG0QxdlArcgpKuHdERz5enM7pHRuRWC+C+87uyJATGvJrRi4juzfl0r7N+WjRNlwGnriwG2d2bkz3h74DYENWAVNX7WRE1yaH+ic9bHUuEBys5X601KvnaVnNnj2bGTNmMG/ePGJiYhgyZIjfawEiIyMrH4eGhlJUVHRUyqrqvrz9ZWTmFrFk6x7KXYZrBnkaKYUl5WTstX9rOQUlZOWXUFbh4p7PV7C7oIRd+0r48ylteHXOJsae2Ip/ntOJ//6wmae+W0+z+GgGt09h/FkdAIgMC+EfX6wkM9e+30eL0hnRtQk3T1zK2p35vDpnEwvuPY2TH59FaYWrspIF+GxpBoIwaVG6T9lvHtqWGWt28eiUNfRpmcBPm3YTGxnGjUPacGGvVO77cgWPT13L63M3UVrh4tQODZm5Nqvad1BYWsE1by2qtr1ZfDSvjenNwMdm8vz3GwC45bR29GoRz1jn+LYNY3n5il7Ex0RgjGFrzn4u79+Cc7o15aahbTHGUFZhiI4I5U9b95C1r4SeLRLY8tjZTF25gx7NE2gcF1X5mZf29fRWrju5deVnuP3491PZnltEn1ae4a+WSTH0aZno8z5HUp0LBLWhfv365Of7v+NfXl4eCQkJxMTEsHbtWubPn3+US6eOZeUVLkrKXdSLPPh/xb2FpYSGCg2iPMuLZOUXUz8ynOiIUDZnFxAXHU5CTIRPK31/aXlli9LtygEtCQsNoaCknO7/59n36pxNvPHDb9U++9U5tsJ+++ctTFy4jdJy22rOzC3iw4XbfFrfAIPbpzCqe1Pu/OQX/vLBEtbvKqjc99S0dZRWuBjeuTFTvdIi//HFysrHZ3RqxHerd/HqmF4M79KEs7o25twXf+KM/8ylwhi+ueNk2qTYivOxC7sx9KnZ7N1fRuvkekwY25dFW/bwwFerWL3DM0T89V9P4udNu2mZVI+46HDOeu4HAP5+VgcaNoiiRWIMv+0uZFDbJG4Y3JpCZ5L2rC6NefHyXpVDXCLCxOsHVL6v3S6EhdrnVecuhnc5/NZ70/homsZ75gdn3HEKKbGRxMUEbmkZDQRHQFJSEoMGDaJLly5ER0fTqFGjyn3Dhw/n1VdfpWPHjpxwwgkMGDDgIO+kgs1LszYxceFWfvr7qT6TolX1fHg6ifUimH77YD5anM7YE1vR71/fM+SEFN6+ph+nPj2n8thhnRpx74iOpCXXY8HmPdXe68eNu8nOL+HxqWup8MpEcQeBqPAQUupHMrB1Eut2FfCL13CNOwgczDvX9CU7vwTAJwgAfLIkA4C7zmzvEwjcWifX47Ure1NWYYgIs99H56ZxPHp+V37NzGVYp8aVQQAgOTaSBfeextz12ZVzBX1bJfLNLSdhDLS+dwoAzRNjuDTR0xLf9O8RPvMX156Uxn1fruThUV2oFxlGvcgwJt88iBMa1z/gPMfR4t1bCBQNBEfIxIkT/W6PjIzk22+/9bvPPQ+QnJzMypWeFtFdd911xMunaocxBmM44ATf4q172LWvhOXpuT5DAQDv/LyFnzftpmF9Oxywp7C0stXuzmqZvS6b0a/P83nd9NW7mL56F12aNWBlpm/ihAiVQx4A8THh5O73TJD+6/wuXNKnOeFOUHr6u3U+geCeszqQW1RG/7REHvt2LWMGtOS0jg3ZnlvM3z/7lYt7pyIiNGwQRcukGLY6KZhx0eHkFdnPOadbk8rJYYBnL+1BXHQ417y9iDKXCxEhIsz3+7qkb3Mu6dvc73cYExFWreUtIojYoZ89fnL7q1buYwa05KwujUmK9QzRdksNntRuDQRKHWF5+8uIiwnH5TK8NGsjr/+wmfn3nMZnSzNYvi2XdbvyOaltMhf2TmXNDjukOHtdNjPWZPHDhmzeGtuXJ6at41On9eztzR+rD93M99Pqb5kUUy0IABivVPS3xvalZ4t47v9qFVcObMnsdVlc1Du1MggAtE6x813Pje5Bz+YJtEiKqdw35ISGlY+bxEUz445TfD7r7K5NWJGZxzOX9CAiLIS567PpnhpPakI0ISFC/agw8ovL6ZeWSFS4HVu5uLf/yv73+v7OU3zO+WC8g0CwEVPTb+kY0adPH1P1VpVr1qyhY8eOtVSioyuYzvVYlJVfXNlCLy138fLsjZzTrWll931zdkHlME14qFBWYf9/dWzSgDU7Di+t+ZT2KTw0qjOnPDnbZ/uXNw0iLbket01axiyvfHS3H/8+lGbx0cxel83OfcU0jY9m/uYcBrROYmVmHk9OW8dXNw2ie/NDt3gLSsp5+6ffuH5wayLdA+FHyBn/mcPOvGJ+eeAMRITc/aU0iAoPSHqkAhFZYozp42+f9ghUUFu2bS9bcgppm1KfrqlxldvLK1x8vjSTs7s1qZzI/Wp5JrdOWs4zl3RncPsUXpy5kbd/3sKCzXvom5ZI5t4iGsd5WpVlFYawEKHcZVizYx/dUuP4NSOvWhnO69GUL5dvJ7FeBCe1TWbyL9t5bnSPyouHXrq8FzdNXApArxbx9HAq8JHdmzJrXTYX9kolMjyEiQvspG3jBlGICEM7eFrsp7S39ywf3C6ZcYNb+7T6DyY2MoybT21X4+/zcHRpFkdacr3KxRTjYyIC8jnq0DQQqDrHGFNtpdbisgqmrNjBM9PXM2ZAS8ae2Iqi0gqufHMhBSXlxEWHs+De03h59iaWbN3DgLQknp6+nq17Crn7zA68MXcz/5qyBoAHvlpFfoln+ZB5m3OYtzkHgNSEaPq2SmD8WR1ZtGUPY09sxfXvLuaHDbs5qW0yNwxuQ0RYCJ2aNiB9z37CQoQWiTHERoUxZkBLTmhUn3+e04mU+p6Acna3JpzU7gz+8sESHhrVpXL76Z0accew9owb3Jqo8FC+XJbJ/tKKg046iwjhocdGi/vpi7vXeNhGBVZAh4ZEZDjwHBAK/NcY81iV/S2BCUAKsAcYY4ypPjDqRYeGgudcwa6+WFBSTofGDdiWs5/midEHXI57f2k5V725kMVb93LNoFaV15RszSnkto+Ws2ybZ9JzVI+mfLV8O2Ar2m9+3cHovs2r5bGDTYf8cUM2TeKiaZ1Sjx827K7cd2mf5qzblc+aHfsocTJqbju9Hbed3r7ymCemruXl2Zv41/ldfJY7ONKy80vILy6jdUrgs0zU8adWhoZEJBR4CRgGZACLRGSyMcb7muungHeNMe+IyKnAo8CVgSqTOv6c9PgswKYJ7i4o4a+ntuWOYbaS3VdUzqdLM7hqYEu25xZx1ye/sHirXezrrZ+20LtlArv2lfDw176X+SfHRlYGgRaJMTw/uic5BSVMWpROcmwEuwtslsnVA1vyzrytzF2fzUW9U/nnOZ1YsDnHJxAMbJPEYxd2JWNvESc/Ycvaq0WCz+fdNLQtEWEhXNgrNQDfkEdK/UifnoRSNRXIoaF+wEZjzGYAEZkEjAK8/1d2Au5wHs8CvgxgeY4ZsbGxFBQUHPrAIPBLei6LtuxhRNcmRISFEB0eSrnLsD23yGfYYHeBzUt/YeZGPl6czq59JZX7nv9+Q2VqorebJy6rtm3C2D5szy3mvi9tuu5HNwwgNER45Yre/LRpN2d2bszKzDwaRIcjwDvz7OqVT13cHbDrTgGc2qEh15/cmgGtExERUhOi6dC4PrGRYfRL800DrRcZ5tNDUOpYE8hA0Azw7mdnAP2rHPMLcAF2+Oh8oL6IJBljcrwPEpFxwDiAFi1aoI59z3y3jpPbp9DXKzd+T2EpWfnFdGhsK9OXZm3kyWk2H/6Rb9Yc8j0jw0JwGeMTBIDKIJCaEM0TF3Xj14w8Hvt2rc8xfVom8I+zO9KzRQJZ+zyBoHEDmwGUUC+Cc7o1BaCn06I3xnBS22RGdvfkqKcmxDBhbB/6tEr0ucpXRJh62+AafDNKHXtqe7L4LuBFERkLzAUygYqqBxljXgdeBztHcDQLWBPjx4+nefPm3HTTTQA8+OCDhIWFMWvWLPbu3UtZWRmPPPIIo0aNquWSBsbewlJ25BWTX1xG/9ZJ5BWV8fzMjTw/cyNf//UkujSLI29/GSc/PpPC0go+vmEgXy3P5IMF26q911+GtCG3qIz84nL+94sdvvn4hoGk1I8kISac3QWlxESE8u3KnZVDPgNbJ/Hc6B4kxUYSGiLERNg/6w+vH0Dvlgm4jKnMUwdo2CCK5NhIoPqksjcR4f3rqrZd4NQOjfwcrdTxK2CTxSIyEHjQGHOm8/weAGPMowc4PhZYa4w56EDqISeLvx0POw+8ZOzv0rgrnPXYAXcvW7aM2267jTlzbP54p06dmDZtGnFxcTRo0IDdu3czYMAANmzYgIj8oaGh2pgs3rK7kCFPzeaD6/rTrlEsu/NLK4dIfttdyNCnZlceu/bh4azIzOPiVz1Xu2557Gy+XbGDGz9YetDPmXhdf05smwxAUWkFHe+fWvl6f/YUlhIbGVa5FIG3kvKKg+a97y8tx2VseqRSwaC2riNYBLQTkTRsS380cHmVgiUDe4wxLuAebAbRcadnz55kZWWxfft2srOzSUhIoHHjxtx+++3MnTuXkJAQMjMz2bVrF40bN67t4tbYC99vYMrKnUQ46YafLskgM7eIhb/t4eUrerF+Vz7Pztjg85oO/5xa7X1Wb99XGQQu6NmMz5dl+uw/q0tjOjZpQP/WnqW5oyNCeeri7qQdZI38xHoHzjs/1MVP7l6DCgCXC0L8pLAaY9e4OB58eDnsy4Qb5hz62DogYP8bjDHlInIzMA2bPjrBGLNKRB4CFhtjJgNDgEdFxGCHhm76wx98kJZ7IF188cV8+umn7Ny5k0svvZQPPviA7OxslixZQnh4OK1atfK7/PSxpqzCRYXLsDOvmKenr/fZ94VXBX7rpGWUVRif9WSqGtw+hbnrsxnxvF3p8aahbejYpEG1QDCye1O/a6xf1DuwWTYqAAqy4MW+cPqD0OcaKCmAHb9AiwHwZBvocy2c9s/AlqEoF6bdC+Ul0OsqaH3KoV+TvggSW0O9JBvI1n1jt5eXQtgRvNDNXzDcvhxePwWunwXNeh25zzoMAW0WGWOmAFOqbLvf6/GnwKeBLMPRcumll3L99deze/du5syZw8cff0zDhg0JDw9n1qxZbN1a/d6px4L0Pft544fN3H56exLqRXDXJ78wdeXOysyXh0d1Zl9xeeWkrpt76YT3r+3P1pz9jHlzAa+O6c2ZnRuRU1jKjtxi0vfur7yRCMDdZ3bAGEObW2NplhDNgs176J4aR8MGgVljvc47FlvYKz6B4lz4+jboOQb+dyus/BTiW0DRXvjhKTjlbxBWJc3VGDuku/wDOPPfEBLqu68m52mM/fn+/+z7AKyfBvekH/z1FWXw5ulQLwXu3gi7vIaWs9dAk+7Vezkul33PmpZr00x77Hvnw18WQMMOnv2rv7K/v/sntD0V6jeBHpcfuGcVANo/PkI6d+5Mfn4+zZo1o0mTJlxxxRWMHDmSrl270qdPHzp06HDoN6kFf//sV37elMOCzXs4uV1yZX79Dxt206Fxfa4c2AqwSxS8/fMWtuYUsmiLzdWfMLYPzRNjaJ4Yw4J7T6ORU6Enx0aSHBtJucuzZPHtTvqkiNCxiZ1fGNZJJ11/t2/H20r3b5sOfaw370p123xo1AUiD3EBWkU5bJ4FUXHQoCnEefXUvn8Img+ANV9B/xth/iuefbvXw6bv7ePcbb7bG3e1j10uePUk22r/7QdbCZ9wFrQe4tn/fA/oOBJOux8qSiE0wgaSpzvY7SOetL2OqffCtnlgvJbKLs2HSVfAzl8hpQMMewgadfKcf0oHKNhlnxdmw5r/webZntf/8DR0vxy+ugnOeARiG0Jcc/hkLDTuAhe8fujvPH0hvH+B5/nLTgLC/XuhZB+s/do+3/qj/QF7nl/fDjfM9XxXAaSB4AhascLTkkhOTmbevHl+jzsWriH4YMFWPluSwdJtudSPDGPdrnzW7cqnfaNYujSN4/NlmVzv3D0J7Low7lz6qycsZM76bJ+bcDTy06rvlhrPrae144oBLSoXaquTjLEVZdopvi1ZCEyrbucKWOBUuOUl1VvXB/rs4jx4rAWMfB5OGAETzoQO59if/O1w8p2+r3cnknxzByx9xz6u1xDuduaFSgpsRen26ydQUQJnPQnf3g0Zi6HEzw2bMpeAq8IGli0/QNYq+xPipOOu+MQGAmMgey3kboV5L9ofgNBIuPhtyN8BC1+H0//PtrT3e2Wd/+03+/kTL/YM8+Slw8bpcPXXkNzenn9MMpx4s+d1H3ndJ7znGFj2vqfFPvtRWxa3rFVw/mt2W/pC2PKj7e3EVRnSTF9Q/TsA2JcBEy+1gbGq/91qf793AYz5FBp2hmXv2n+r2IbVj/+DNBDUcWt37mP4sz9U3mQjIjSEq99a5DNkM+32wXy/Nov+aYm0TYmlzOXiigEtqt1tye35y3qyZXchcdEHv2NSaIhw+7AguJAqfYGtiE78q201uhXvg+e6Qe9r4PQHPNtrOtQx/xXb+ux4ju97vn+R53lJvv9AMPtxWPI2XDcD4uzidezdYn//7xb4+QX7eO3XnhbpSXfYgLZ1HtRvDIW7YdF/Yb/nSmoKs2Dmv6DPn2DSZb6fWVFih1d6j4Xv/gGrvwRXOQy4Cea/5DnOXclV5XIuCtw0C5a+C9/+3Vas3obeZytk789e8YkNAiff6QlMMYnQ/gwYco893ts750CXC+3j/bthxoP28XUz4fsH4be59vmol6DHGFv2rLWQ4wTAof+wvYhF/4UvboBfP/K8tztghkXDXetssDtQIPh2PGSttv/GedWXNgEgNBzePc9+r7vX2eA76Bb/x/4BGgjqqNJyF/+esoZ9zsVWF706D2MMD4/qUhkEPr5hIGUVLprGR3PlAM8aOJEhoQcMAmBvMlKTJYyPOXs2AwKJaYc81K8dv0JsI9si867I3RXs0vd8A0H2Wjsu/uMz0LQHdBhpW8PvnQdNe8JpD0Dzvv4/qyQfpo63jx/ItZ+Xv8uOfxfshL7X2YqoZB/US/Z97dvn2JY2wOSbYcznNvgUeN3LN8c32wuAhW/Ylrw/o16C8mL46TmY+4T9cRv7DcS3hOe6Q9pgO7ma2teOiwN0vxT632Bb8B+NsUMw3lI62O8K7PDSgldg8l/t8xkPQlwL5xwNnHK3HeZZMxnCY6Bsvy13Qpqt9H94Gk70qiiTvRoiLU60QXPzLFj5md2W2tf2qpr3g9TecP7r8IzXMG7Lgfbnq5vsd9a8vw1OmUvs9+8dBLyVF8GyD6DVIDsZ3OUi6HEZRNSHCWfYY9Z9Y3s+V31le287lsH8V20QW/AqtD0dhj8Gb50FORvh3BdtLyUA6kwg8LfiZF3jfc3Hup12GEdEMMbwz69Wsn5XAV2axrExu8CnxQ+eWwyO/9wOX31wXf9qSyHUec/3tL8fzLOVd3kJpJxw6Nft+c0OZbx2MkgoRDWw2S+FWXDqP+1+sJOkxfvsfoDdXpXtx1fBsIft5F2r4a0AACAASURBVGVpga2oV3xSPRBsXwaxjSHD60brr5wIfa+Fb5yhm7AoaHOqrYg+vdZWJFENYPIttnXvPUSyaSY80RrCo+0EpD+n3gczH/EfBFoOghYD7Th5SAj0vBIeT4OyQs8xKR1tts2Vn9vMG4Bul8LWn+zjpLYQUQ8SWkLTXrbyLsr1TMqm9vUEgpPvhA3T7JyCy1nh9cL/+mbTXDTBnuPerbZS3bUCBvzFtp7vy/IMMYFv0P+Tc6fAjd/bgHThm9BhhO/5NnCy17qN9t2edoodJhp6r33etJf93vZuhZHP2cC0cYY9xm3aPZ7HjbvYih3grg3wlLO0d8dz7e+QEGjWGy58A0oL7ffT7RJIbge3/mqDcEzg/r/WiUAQFRVFTk4OSUlJdTIYuFyGwpJy0ndkQWg4U1fu5M/vL+HOYe1J37uf79dkkePcjm/hb9XvVuX2+IVd+ftn9j9f+0b1j0rZ/crLtJVmo862lVqwy3dCbMWn9g//97Z+/A29eF84+e9mtjIGuHuzrcT8+W2unQh9vofX+1R4sl/ADmF42zjdTnqedHv1Vvey922ruN8NdjzZezigvBQ+vcZW5EltfSuirNWeIADQuBtEOsFm+1KY8zic+S/PsITbea/AlzdC0R4oAuY+6btfQmH0B7aXM/MR/Lr6a995hrBIiKzvGwjcFVTrIZ5tXS+2Q1Bgg4Db6ImAsUNpYL/f4Y/ZVnxJPsSmwM2L7YTvG0NBQqBFlau7Q8Pt0FVsIztfUFFie0ju8nlr2Ak6n2//PdzangZ/33LguZX7siGkStXY7RI7Ph/h3KFNBAZ7Bc7O59seG8CIp+w5f3mjZ38jr79v7zF+dyDwFlEPLnjN63mM53MDpE4EgtTUVDIyMsjOrn63prpgd0EJRWUVbM0t44UFe9lXYlv3VfP8wf59GgOvXNGLN37YzNJtuVzQqxmPXtCVyLBQJi7Yxi8ZeSTH/sHcaFeFrczSBnsq3bIi2w1uOdBz3G9zbXc6LNK2YL+5E5r0sBXinevg9aF20uz+PZ6J1s+utb8PJxD8NheWT7RlWP2l/Y9/+oOQvQ7mPGFTGN1KvSbrn2xtW2KXvGsn+cpL4Kn20GYorPrC7quJ1L62Ff/pn+zz/J12Aja+pWeCcbeTgpuYZretmwITR8OgW+Gt4Z73ytloW8WhkTD8354gcOWXNhCldPQdXpn3om1le7t9tZ0bcFdGI5/3VMz3OUNEEmIr1UKvOYDLP7ZZM4mtbeaKv4nu5v1sC/jKL2wGj7/GV0SMTZMszvXdHupUOaFOq73/DTZrKcVrCCckFAi1Y/YHa9iJwJ9/sI2JpDb+jwlzJpb9bT+QA103cKjKuO91dgir8wX2e2s/HJ7pZOc+mvbwPfbKL+3cQGzKwd/zKKkTgSA8PJy0tN857nsM25RdwE0fLGXtTj+ZF14u7p1KdkEJs9dlc89ZHYiLDuesrk14YeZGAE5sk1x5pe1HNwykoKT8j/ecVn9pK72zn/a0xqbfb7M4Wp4EF71pU/omXmK70IPvhu8ftsftWG5/71plg4D7tY272QuPqiopgF0r/e/L3WZbkhNH+7ZSf/wPnDIeXup34HNo2Ami4m15/tMZIuPsmG5xrg0CYMeCKwlQZUmWP31nP2vovbaFv/Ybm8O+3hmGGHw3zH3KVpgVzmJ5ia3t+YDnOLfbV8GbZ9igUr+J/W7bnm6/y9ZDPBWj+73AjoMv9roov984zwTxuDm2wk9u7wkEVSvBmCR74VXXSyDtZGh/5oG/M7DzBd0utcNTB9PwICnTYc7N60MPknBQkwu5Uk6o2fDe0RAaBl29JvJjEu2Esaui+rBOm6FHt2yHUCcCQV00ffUurn93sd993jdVmXHHYNo2rE9OQQkTfvqNawalVd6GsElcFKt37KO/11xAVHiozwJsv1uxc//dpe95AsF2p4Lf+qNthefvsM/XTbUVdrlzZXVEfZvf/Y5XNow7NXDoP3w/o6zIZohkLoGbl9hx+fXT7Jhxm1PhWa8u99hvoKwYPnAyQt48/eDnMOAv0OtKO2Qy8xEoybOt9ANp2Mm2CvduhUvfs6mHyW3h8kl2f5Nu0OFsm4tess9Ocva/EQb/zbbYH3cm5BNbe74/8ASD0x+0vZK2p9khpyjn1pkJreyPN/fQEMCN8+BhZ3hr+OO2le3m3RId8ZTn38CbCJz7woHPu6qoBr6ZTL/H8H/bivNQweR4FxV36GOOARoIjkF7C0v526e/+Gz79/ldadswlo5N7Jr363bms3ZnPm2cu1ElxUZy95lOC8wZI3/iom4s3ZZL81m3QM4mGDfrjxUsf6fNyhj2sKfLv2M5FGTDrEfsfrfFb3oeZy62P24jnrAV4/KJdtJt0RuefbP+5Xmcu9W2jt3DHu+O8vQgGnbynYxtd6ad2BTxZJ7sXGGvCt3hfJcXTbAtc3eueKxzQVvaEOARW7Gf+g87Ges9vuvWpJutMI05eGv14rdt7vnI5zwt+LAIuOJTO0+QkGZ7SSGhdkjLnfroHseu5wwXRB5kHsc7EISG2Xz2L26wPZoD9fb6XX/g9zvaElrZ4Th1TNBAUMv2Fdsll5vFR1NW4eKTxRm8O28L+4o998R9+LwuXN7fuQ9D+iKIb8FXrT4n+6K7qg/xLJ4Asx6FmxaQFJvIsHYN4ONP7L4dv0B0gr3cv6a8J15n/dtOSDbr4zuuPOMBzyX9YD+jyF59TPMBkD7fPj5hhB1CcU8Mu3sShVm24oxrYVMNE1rZdL1XT6ryZWXYtMuoeJsCOHezHTa5+G2IiPWU05350ek8uOgtmyrYaZSn8r4vy07KthtmnzftaSc3+93gyeJJameHb76+zfP5TboffCjDre1p9qeqdsM8n5lygi33hhmw+gs4+S7PcfVqMG5cNRB1H22vsvWemFWqhjQQHGWFJeW8+eNvXNwnlcy9RVw1YSH7Syt48fKefL40k5lr7UTerae1o2eLeNqkxNI80ZmkKtpbOdwRCaQmt4TU230/4KfnbcX68wv2Iib3WDfAa4Mh+QS4acGBW42fXmsvsrnqK9i9Ed44FQb82WamxDtDG1/fZlvpUfFOy94JAo272eyVloNsHvS0e23e9/vOUM1lH/r/zFEv2eyJLhfacrnz8sEOZ3S/zGa+TL/fpmsun2gDgXHZOYqqLWdxJjhjEu2kXbeLffeHRXouKALbor7wv77HNO9rA9bXt0FiG7u8gbsSP5LanW5/vLkDQUX1u65Vc4JX+qMGAfU7aSA4iqav3sXughKemb6eZ6pk/Lhvq3jLqW3p0SKeoSc0rN7az61y9WFRlYwMl8tWmGCHYoyBeS/7HrN7HWz92Q4heHMvk+DOrpn/iu1ZlOTZIACe7Bf3UE15iZ3A/W2u7SVc/73n/QbeZCcgI5x1bKrmZXuLrO87yZbQyuZ4tx7qSe2MjPVkf7gnQht1qT52DpDqTBB3+oM3AgqPstkdye2qLxsQSO4LxCpKD37cfVnV0xyV+h30r+goWZmZ5zP5GxcdTn5xGf+5tAfNE2O44OWfAXuHrijvtfJzt9mWYWLr6lcxLnnbji1HNbDj81/eaNeTAVs5P9vVZrKknQK/ea2rnrXaDnNMv99eULPuW5va6c19Vas/EbE2BbO8yA6//DYXel9d/Th3S/0fO+3Y/OHwDgxVxTjBIfoAVze36A/3bj8yLeTayO6o7BEcIhAcLAVSqcNwdNY4Vaze4ckSuazBSpacs5Ol/xzGqB7N6Nk8nrioMD6Mf5Wod6qk7j3bFV7oZcfm3Zk1f11qL24pzrXrtuSm26wX9yX97uGCvHSbEtlvnOf9QiNgyl3waDM7ofv+hZ4g0OdamxvuNvI5+zneBt0G452VJKPi7Zoz92Ta1v+BhEdXX4ztj0hx7tDW44oDH3M8D5NE23smV16lq1SAaY8gwIrLKogKD2Vlpm2pt5MMHi39N/wP4ntdCdilmReMTSTq7bn2Xm7F+2w2TpRXi9d7ed+ENDjjYTvhuepz++NdKfb5k02DvPprOxG6b7tnn3crs1EXTz77kHtgyHibAeTWtGeVPHqg83m2Ur/6aztcInLoZYyPtJT2R67Ffyxq0BRGf+h7YZ5SAaSB4AjLzi/h9o+Wc/uw9jw6ZQ2LtzrZMxjOTczk+f1eqynmZUB8cwCi8r2Wt/3katu6T/W6GMq9FgvYCdDE1tDqZM/iYt5ZO+2GwT0ZnqEZ95hzSJhn/Rawa6l/NMYuxtbKydCJTbHLCKfPtyma7uyfbpfatMomTl562smH/+UcSXU1CLhVXQNHqQDSQHCEfbIknR837ubHjbsRgYGtk5i3OYe/hE7mb/urjPEvegMG3mzXHvFed8Y9xJOx0GbAuJfXHXSb71WfXS/yBAK3651rBbwzaaITbI+h11Ww6E1Y8bEd3omKg3Gz7QVQLU70HN9/nP0BGOIssnXOswFf70QpVTvEe0XL40GfPn3M4sX+r7itTTNW7yKnsIQ1O/J5++ctALRMimF2759gybuUhNcnKndj9Rd2vcSuOPj17bDiM5ul461BM7htpZ2cjWrgu88Yu7yCqYCp99iJ2xOGc1AV5XZ4SCt1pYKKiCwxxvTxt08niw+Ty2X4cvLnFE/7P1upYpd4vu7dxfz9sxXM2+RZArhRgyhk7pNI4a7qQcC9lMLOX+04/OIJdginrZOr3trJVomItUNBVYMA2PH5qAa2xX/+q4cOAmBz5jUIKKW86NDQYfpp024iF71MVOgicohmTsrlTFroGda5a8+DpDaN57usOFrV7w87vF48/DE7ObvsfbsyYWG2XaTtDWe9lZYn2ht3r5sCDTvCa7N8b42nlFIBoIHgMOUXl+NyOlLLfvqWO0o7MyniYTaHNSaHOIaFLoE90DEMWP+574tPGAEDbrQBIbI+bGnl2ffXpXZZ2rAIu1yAMfaWf+3OOFqnppQKUhoIDtP23CI6Ytez7yPr2BJl7/o0IGTNwV/Ydpi9QxN4JnLdF0adel/19dRFbB6/UkoFmAaCmlj2PiAUN+rJu/N28ZrY+wPES6H/49sOs3eqAntB1tnP+F9IrMtFNp2z26WBKbdSStWABoJD2fGLXQkTiAK2FX9AfGQB+0w0DaTI/2uG3GNXn5w6HkY8CfUb+T8uNCxgN6NWSqma0qyhg9m/x67Y6aUJe0iSfBa6DnL3pfjm0P/PNle/QdMAF1Ippf4Y7REcjHv5BS+TUj8jYncZy1ztOD10mWfHTYvsmjqZSzw3pz5O7k6klApuAe0RiMhwEVknIhtFpNpyliLSQkRmicgyEflVRI6t6+p3rap8OLzkMQBa7p4NQHhUlSUO4lvYnkDn845W6ZRS6ogIWI9AREKBl4BhQAawSEQmG2NWex12H/CxMeYVEekETAFaBapMh23nCpBQXkr4G2u3tyDLxNNQ7D0ArvvTOCq2dSB072ZofYpdu14ppY5DgRwa6gdsNMZsBhCRScAowDsQGMB9yWwcsJ1jyZYfyG46lCc3daVNSj0qypLt0s9jPie2WSdo1qm2S6iUUn9YIIeGmgHet9TKcLZ5exAYIyIZ2N7AX/29kYiME5HFIrI4Ozvb3yFH3sxHIHcb8+lKbGQY024bTJOxb9sLvFro8sBKqbqjtrOGLgPeNsakAiOA90SkWpmMMa8bY/oYY/qkpNTgxt5/VHkp5qfnAXg9qyNdm8URFhpi72F7xSe6Vo9Sqk4JZCDIBJp7PU91tnm7FvgYwBgzD5uqnxzAMtXMzhVIRQk3lt7KivxYujXX7B+lVN0VyECwCGgnImkiEgGMBiZXOWYbcBqAiHTEBoKjNPZzAHt+o8zpDSx1taN+ZBh/GpRWq0VSSqlACthksTGmXERuBqYBocAEY8wqEXkIWGyMmQzcCbwhIrdjJ47Hmtq8QcKu1fDaYMJdZUwsP5Xyeo2Ze8cpJNQ7zBuvK6XUcSSgF5QZY6ZgJ4G9t93v9Xg1MCiQZTgsmYvBVca8/i9z75w4pv91gAYBpVSdV9uTxceWgl0A3LowjrCQEFok6aSwUqru00DgtuB1mPkIeSaGrCKhbcNYIsNCa7tUSikVcBoI3L69G4AwKgAQkdosjVJKHTUaCABcrsqH9aQEgMv7t6it0iil1FGlq48CbF/q83Ttw8OJDNMYqZQKDhoIAH6ZRHlIJNPLupF60hV0Dde5AaVU8NBmL1C2YSazyzpxY9ntJPYfXdvFUUqpoyo4A0FFmb0FpTFQvI/w3E0sd7VFBJrG6XLSSqngEpxDQ79+VHkfYrDZQStMa3554AzNFlJKBZ3g7BHs3uD1xK5okRHTiQZR4bVTHqWUqkXBGQjyd0BIGCS0AiBPGtCwYePaLZNSStWS4Bwa2rcdmvWGa7/D/PoJYz7fT/eG9Q79OqWUqoOCs0ewbzs0aApATutzWVGcTJuU2FoulFJK1Y7gCwTG2KGh+jYQPDtjPQCtNRAopYJU8AWC0kIo2w+xDckvLuP9+dsA6Ny0QS0XTCmlakfwBYKivfZ3dAK/ZuQBMGFsH5JjI2uxUEopVXuCLxAU59rf0fEsT7ePe7dIrMUCKaVU7Qq+QFDkBIKoeNbtzCc1IZq4GL1+QCkVvIIwEHiGhrbmFJKWrGmjSqngFnyBwGtoaEvOflrq7SiVUkEu+AKBMzT0a46QV1RGqyTtESilgluNAoGIfC4iZ4vI8R84inNBQvnb5M0AdGyiaaNKqeBW04r9ZeByYIOIPCYiJwSwTIFVtBei48kvqaB/WiKD2ibXdomUUqpW1SgQGGNmGGOuAHoBW4AZIvKziFwjIsdXys3+HIhOJK+ojE56EZlSStV8jkBEkoCxwHXAMuA5bGCYHpCSBUpBNqZeCgUl5cRFH18xTCmlAqFGq4+KyBfACcB7wEhjzA5n10cisjhQhQuIwixKkzsDEK+BQCmlarwM9fPGmFn+dhhj+hzB8gReQRbFzQYD6IVkSilFzYeGOolIvPuJiCSIyF8O9SIRGS4i60Rko4iM97P/PyKy3PlZLyK5h1H2w1dWDCX7KAyzS0ro0JBSStU8EFxvjKmspI0xe4HrD/YCEQkFXgLOAjoBl4lIJ+9jjDG3G2N6GGN6AC8Anx9O4Q9bYRYA+zQQKKVUpZoGglDxuqu7U8lHHOI1/YCNxpjNxphSYBIw6iDHXwZ8WMPy/D47fgEgN8R2bjQQKKVUzQPBVOzE8Gkichq2wp56iNc0A9K9nmc426oRkZZAGjDzAPvHichiEVmcnZ1dwyL7MecJSEjjt3o9AIiLPlQsU0qpuq+mgeDvwCzgRufne+BvR7Aco4FPjTEV/nYaY143xvQxxvRJSUn5/Z9Ssg9S+5JTZgOA9giUUqqGWUPGGBfwivNTU5lAc6/nqc42f0YDNx3Ge/8+LheEhJK7v4zo8FAiwo7/FTOUUuqPqul1BO2AR7GTvlHu7caY1gd52SKgnYikYQPAaOwyFVXfuwOQAMyrebF/J1MBEkpeUZn2BpRSylHTJvFb2N5AOTAUeBd4/2AvMMaUAzcD04A1wMfGmFUi8pCInOt16GhgkjHGHG7hD5urAkJCyCsqI16vIVBKKaDmF5RFG2O+FxExxmwFHhSRJcD9B3uRMWYKMKXKtvurPH/wMMr7x3j1CBpoj0AppYCaB4ISZwnqDSJyM3aoJzZwxQoQVwWE2EDQPFFvSKOUUlDzoaFbgRjgFqA3MAa4OlCFChjj0jkCpZSq4pA9AufisUuNMXcBBcA1AS9VoBhXZY9AA4FSSlmH7BE4uf0nHYWyBJ6rggqE/aUVGgiUUspR0zmCZSIyGfgEKHRvNMYEdm2gI81UUOJcsqZZQ0opZdU0EEQBOcCpXtsMgV4k7khzVVBSbpdM0h6BUkpZNb2y+PidF/BmKih2egQaCJRSyqrplcVvYXsAPowxfzriJQoUY8C4KK6wp6GBQCmlrJoODX3t9TgKOB/YfuSLE0DGBUBR5RyBrjyqlFJQ86Ghz7yfi8iHwI8BKVGguGwEKCqzT7VHoJRS1u9dfrMd0PBIFiTgnBWui8rt0wZRNe0MKaVU3VbTOYJ8fOcIdmLvUXD8cHkCQf3IMMJCdQlqpZSCmg8N1Q90QQLOmSPYX2Z0wTmllPJSo2axiJwvInFez+NF5LzAFSsAnKGh/eV6MZlSSnmr6fjIA8aYPPcTY0wu8EBgihQgLtsjKCxz6USxUkp5qWkg8Hfc8TXb6u4RlGnGkFJKeatpIFgsIs+ISBvn5xlgSSALdsQ5k8UFpUaHhpRSyktNA8FfgVLgI2ASUMzRuNn8keT0CApLdbJYKaW81TRrqBAYH+CyBJbTIyg1EB+tVxUrpZRbTbOGpotIvNfzBBGZFrhiBYDTI6gwITpHoJRSXmo6NJTsZAoBYIzZy/F2ZbGTNVSBBgKllPJW00DgEpEW7ici0go/q5Ee05wegYsQGkQfXwlPSikVSDWtEf8B/CgicwABTgbGBaxUgeDMEVQQQmRYaC0XRimljh01nSyeKiJ9sJX/MuBLoCiQBTvijCcQhIdKLRdGKaWOHTVddO464FYgFVgODADm4XvrymObs9aQixDCdcE5pZSqVNMa8VagL7DVGDMU6AnkHvwlxxiXd49AA4FSSrnVtEYsNsYUA4hIpDFmLXBC4IoVAJU9AtGhIaWU8lLTQJDhXEfwJTBdRL4Cth7qRSIyXETWichGEfF7QZqIXCIiq0VklYhMrHnRD5PLkzWkPQKllPKo6WTx+c7DB0VkFhAHTD3Ya0QkFHgJGAZkAItEZLIxZrXXMe2Ae4BBxpi9IhK4axO8JosjwjQQKKWU22En1Btj5tTw0H7ARmPMZgARmQSMAlZ7HXM98JJzgRrGmKzDLU+NaY9AKaX8CmSN2AxI93qe4Wzz1h5oLyI/ich8ERkesNJ4LTERpnMESilVqbYvsQ0D2gFDsKmpc0Wkq/dyFgAiMg7nArYWLVpUfY+a8coaitAegVJKVQpkjZgJNPd6nups85YBTDbGlBljfgPWYwODD2PM68aYPsaYPikpKb+vNHodgVJK+RXIGnER0E5E0kQkAhgNTK5yzJfY3gAikowdKtockNI4PQIjIYSG6NCQUkq5BSwQGGPKgZuBacAa4GNjzCoReUhEznUOmwbkiMhqYBZwtzEmJzAFsoGAkNoeDVNKqWNLQGtFY8wUYEqVbfd7PTbAHc5PYDk9gtBQXXBOKaW8Bc9guTNHICEaCJRSylsQBQLtESillD/BEwicO5SF6ByBUkr5CJ5A4PQIQrRHoJRSPoInELg0ECillD/BEwgqewQ6NKSUUt6CJxC4NBAopZQ/wRMInB5BmA4NKaWUj+AJBC73dQTaI1BKKW/BEwjc1xGEaSBQSilvwRMInDmCsDAdGlJKKW/BEwgqrywOr+WCKKXUsSV4AkFiG+aGDkQ0ECillI/gGTDveA73RdajV2hEbZdEKaWOKcHTIwAqXIYQvSmNUkr5CKpAYIwhVDQQKKWUt6AKBBXGEKKBQCmlfARXIHChQ0NKKVVFUAUClzGEBtUZK6XUoQVVtVjh0jkCpZSqKqgCgUuzhpRSqprgCgQ6WayUUtUEVSCoMIZQ7REopZSPoAoELhfaI1BKqSqCKhBUaNaQUkpVE1TVomYNKaVUdUETCIwxgF5QppRSVQVNIKhwOYFAewRKKeUjoIFARIaLyDoR2Sgi4/3sHysi2SKy3Pm5LlBlqXB6BJo1pJRSvgJ2PwIRCQVeAoYBGcAiEZlsjFld5dCPjDE3B6ocbs6967VHoJRSVQSyR9AP2GiM2WyMKQUmAaMC+HkH5ekR1FYJlFLq2BTIarEZkO71PMPZVtWFIvKriHwqIs39vZGIjBORxSKyODs7+3cVxmV0jkAppfyp7fbx/4BWxphuwHTgHX8HGWNeN8b0Mcb0SUlJ+V0f5NLJYqWU8iuQgSAT8G7hpzrbKhljcowxJc7T/wK9A1UYd9aQThYrpZSvQAaCRUA7EUkTkQhgNDDZ+wARaeL19FxgTaAKU6HXESillF8ByxoyxpSLyM3ANCAUmGCMWSUiDwGLjTGTgVtE5FygHNgDjA1UedxZQ3plsVJK+QpYIAAwxkwBplTZdr/X43uAewJZBjfNGlJKKf+Cplp0TxaL9giUUspH8AQCd49AA4FSSvkImkCgWUNKKeVf0AQCl2YNKaWUX0ETCCo0a0gppfwKmkDg0qwhpZTyK2iqxQrNGlJKKb+CJhBo1pBSSvkXNIFAs4aUUsq/oAkEmjWklFL+BVEgsL91aEgppXwFTSDw3Ly+lguilFLHmKAJBJU3ptFIoJRSPoImEHhWH9VAoJRS3oInEOitKpVSyq+gCQTGPVmsPQKllPIRNIFAJ4uVUsq/4AkERoeGlFLKn6AJBC69slgppfwKmkCgWUNKKeVf8AQCzRpSSim/giYQuLOGtEOglFK+giYQ6OqjSinlX/AEAs0aUkopv4ImEGjWkFJK+Rc0gUCzhpRSyr+gCQSuysliDQRKKeUtoIFARIaLyDoR2Sgi4w9y3IUiYkSkT6DK4tIlJpRSyq+ABQIRCQVeAs4COgGXiUgnP8fVB24FFgSqLKBZQ0opdSCB7BH0AzYaYzYbY0qBScAoP8c9DDwOFAewLHrPYqWUOoBABoJmQLrX8wxnWyUR6QU0N8Z8c7A3EpFxIrJYRBZnZ2f/rsJU9gh0jkAppXzU2mSxiIQAzwB3HupYY8zrxpg+xpg+KSkpv+vzWqfEMqJrY8JCNRAopZS3sAC+dybQ3Ot5qrPNrT7QBZgttpXeGJgsIucaYxYf6cIM69SIYZ0aHem3VUqp414gewSLgHYikiYiEcBoYLJ7pzEmzxiTbIxpZYxpBcwHAhIElFJKHVjAAoExphy4GZgGrAE+NsasEpGHROTcQH2uUkqpwxPIoSGMMVOAKVW23X+AY4cEsixKKaX8C5ori5VSSvmngUApdikn9gAABfRJREFUpYKcBgKllApyGgiUUirIaSBQSqkgJ8Z9M9/jhIhkA1t/58uTgd1HsDjHAz3n4KDnHBz+yDm3NMb4XZrhuAsEf4SILDbGBGyp62ORnnNw0HMODoE6Zx0aUkqpIKeBQCmlglywBYLXa7sAtUDPOTjoOQeHgJxzUM0RKKWUqi7YegRKKaWq0ECglFJBLmgCgYgMF5F1IrJRRMbXdnmOFBGZICJZIrLSa1uiiEwXkQ3O7wRnu4jI88538Ktzq9Djjog0F5FZIrJaRFaJyK3O9jp73iISJSILReQX55z/z9meJiILnHP7yLn3ByIS6Tzf6OxvVZvl/71EJFRElonI187zOn2+ACKyRURWiMhyEVnsbAvo33ZQBAIRCQVeAs4COgGXiUin2i3VEfM2MLzKtvHA98aYdsD3znOw59/O+RkHvHKUyniklQN3GmM6AQOAm5x/z7p83iXAqcaY7kAPYLiIDAAeB/5jjGkL7AWudY6/FtjrbP+Pc9zx6Fbs/Uzc6vr5ug01xvTwumYgsH/bxpg6/wMMBKZ5Pb8HuKe2y3UEz68VsNLr+TqgifO4CbDOefwacJm/447nH+ArYFiwnDcQAywF+mOvMg1ztlf+nWNvCDXQeRzmHCe1XfbDPM9Up9I7FfgakLp8vl7nvQVIrrItoH/bQdEjAJoB6V7PM5xtdVUjY8wO5/FOwH2z5jr3PThDAD2BBdTx83aGSZYDWcB0YBOQa+zdAMH3vCrP2dmfByQd3RL/Yc8CfwNczvMk6vb5uhngOxFZIiLjnG0B/dsO6B3KVO0zxhgRqZM5wiISC3wG3GaM2Scilfvq4nkbYyqAHiISD3wBdKjlIgWMiJwDZBljlojIkNouz1F2kjEmU0QaAtNFZK33zkD8bQdLjyATaO71PNXZVlftEpEmAM7vLGd7nfkeRCQcGwQ+MMZ87myu8+cNYIzJBWZhh0biRcTdoPM+r8pzdvbHATlHuah/xCDgXBHZAkzCDg89R90930rGmEzndxY24PcjwH/bwRIIFgHtnIyDCGA0MLmWyxRIk4GrncdXY8fQ3duvcjINBgB5Xt3N44bYpv+bwBpjzDNeu+rseYtIitMTQESisXMia7AB4SLnsKrn7P4uLgJmGmcQ+XhgjLnHGJNqjGmF/f860xhzBXX0fN1EpJ6I1Hc/Bs4A/r+9e3eNKoqiOPxbNvERUAQrCyHaiBACioUPCNilslAENYVY2tiJ+AL/ASvBlBGDiGIay6QIpJAoGjWKaLQKCDYiRlAkbouzB2ISIcQkVzzrg4GZM3cud8PM7JlzZ9aZYKWf202fGFnFEzA9wBvKvOqFpo9nGeu6DXwAflDmB09T5kaHgbfAELA5txXl11PvgBfAnqaPf4k1H6DMoz4HxvPS8z/XDXQCT7PmCeByjncAY8AkcBdoy/G1eXsy7+9ouoa/qL0beFBDvVnfs7y8bL1XrfRz2xETZmaVq2VqyMzM/sCNwMyscm4EZmaVcyMwM6ucG4GZWeXcCMxWkaTuVpKm2b/CjcDMrHJuBGYLkHQy8//HJfVl4Nu0pGu5HsCwpC25bZekh5kHPzgrK36HpKFcQ+CJpO25+3ZJ9yS9ljSg2SFJZg1wIzCbQ9JO4BiwPyK6gBngBLABeBwRu4AR4Eo+5CZwLiI6Kf/ubI0PANejrCGwj/IPcChpqWcpa2N0UHJ1zBrj9FGz+Q4Bu4FH+WF9HSXk6ydwJ7e5BdyXtBHYFBEjOd4P3M28mK0RMQgQEd8Acn9jETGVt8cp60mMrnxZZgtzIzCbT0B/RJz/bVC6NGe7peazfJ91fQa/Dq1hnhoym28YOJJ58K31YrdRXi+t5MvjwGhEfAY+STqY473ASER8AaYkHc59tElav6pVmC2SP4mYzRERryRdpKwStYaS7HoG+Arszfs+Us4jQIkFvpFv9O+BUzneC/RJupr7OLqKZZgtmtNHzRZJ0nREtDd9HGbLzVNDZmaV8zcCM7PK+RuBmVnl3AjMzCrnRmBmVjk3AjOzyrkRmJlV7hfzO1yUeujd0AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E_kALHn8efk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08476e2c-4d5d-49ab-a713-b802dc1efb49"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFvNdC5xxcc7",
        "outputId": "6db1b36b-49e7-40c6-f390-b5c7d3c5eaca"
      },
      "source": [
        "y_test_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7LQrvY48ehr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cce426-28dd-43dc-a002-5ca9026f575e"
      },
      "source": [
        "#predict on testing dataset\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "K.set_value(model.optimizer.learning_rate, 0.0008)\n",
        "y_pred_s = model.predict(x_test_norm_s)\n",
        "y_pred_s.shape, y_test_s.shape\n",
        "\n",
        "y_test_s = y_test_s.argmax(axis=-1)\n",
        "print('y_test_s', y_test_s)\n",
        "y_pred_s = y_pred_s.argmax(axis=-1)\n",
        "print('y_pred_s',y_pred_s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test_s [2 2 2 2 2 3 1 2 1 2 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 2 1 1 1 1 2 2 2\n",
            " 1 2 3 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 3 3 3 3 3 3 3 2 2 1 1 1 1 1 1 1\n",
            " 1 2 3 2 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 3 3 4 3 2 2 2 2 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 3 3 2 2 3 2 3 3 3 2 2 2 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 2 3 2 2 2 2 3 4 3 4\n",
            " 3 3 2 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 2 2 2 3 3 2 3 3 3 2 2 2\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4]\n",
            "y_pred_s [3 2 2 2 2 3 1 3 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 2 1 1 0 2 2 2 2\n",
            " 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 3 3 3 3 2 3 2 2 2 1 2 1 1 1 1 1\n",
            " 1 2 0 2 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 2 3 4 3 2 2 2 2 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 2 2 3 3 3 2 3 2 3 3 3 2 2 2 1 2 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 2 2 0 1 1 2 2 3 2 2 3 2 3 4 3 4\n",
            " 3 3 2 2 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 2 2 2 3 3 3 3 3 2 2 2 2\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 2 3 3 2 3 3 2 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 3 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8D_jq6a8ejv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb58316-d4d0-45c8-a0bc-319192e19721"
      },
      "source": [
        "#Compute Recall, precision, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_s, y_pred_s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.81       136\n",
            "           1       0.74      0.73      0.73       117\n",
            "           2       0.83      0.93      0.88       122\n",
            "           3       0.93      0.89      0.91       114\n",
            "           4       1.00      0.97      0.99       112\n",
            "\n",
            "    accuracy                           0.86       601\n",
            "   macro avg       0.86      0.86      0.86       601\n",
            "weighted avg       0.86      0.86      0.86       601\n",
            "\n"
          ]
        }
      ]
    }
  ]
}